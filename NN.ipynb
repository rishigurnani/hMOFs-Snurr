{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.read_csv('~/efrc/prep_data/no_cat_v1/data_DONOTOUCH/ml_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "start_str = 'SMILES'\n",
    "end_str = 'valence_pa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)\n",
    "fp_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_space(x, y, n, force_int=False):\n",
    "    step = (y - x) / (n - 1)\n",
    "    if force_int:\n",
    "        return [int(x + step * i) for i in range(n)]\n",
    "    return [x + step * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define default params\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":2, \"n_unit\":10, \"activation\":'relu', \"loss\":'mse', \n",
    "            \"opt\":'adam', \"val_pct\":.2} #patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial grid\n",
    "init_grid = {\"patience\":eq_space(20, 1000, 5, True), \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":[.3, .5, 5]}\n",
    "#patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_used = 'norm_CH4_v/v_1_bar' #column name of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(ml_data, total_frac, start_str, end_str, training_pct, batch_size=1, norm=True):\n",
    "    '''\n",
    "    get normalized training and test data\n",
    "    '''\n",
    "    fp_dat = ml_data.sample(frac=total_frac, random_state=0)\n",
    "    train_dataset = fp_dat.sample(frac=training_pct,random_state=2)\n",
    "    test_dataset = fp_dat.drop(train_dataset.index)\n",
    "    train_label = train_dataset[property_used]\n",
    "    test_label = test_dataset[property_used]\n",
    "    for ind, col in enumerate(ml_data.columns):\n",
    "        if start_str in col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "\n",
    "\n",
    "    features = list(ml_data.columns[start_col:end_col])\n",
    "    other_props = ['norm_Dom._Pore_(ang.)',\n",
    "     'norm_Max._Pore_(ang.)',\n",
    "     'norm_Void_Fraction',\n",
    "     'norm_Surf._Area_(m2/g)',\n",
    "     'norm_Vol._Surf._Area',\n",
    "     'norm_Density',\n",
    "      'norm_valence_pa',\n",
    "       'norm_atomic_rad_pa_(angstroms)',\n",
    "         'norm_affinity_pa_(eV)',\n",
    "           'norm_ionization_potential_pa_(eV)',\n",
    "               'norm_electronegativity_pa']\n",
    "\n",
    "    features = features + other_props\n",
    "\n",
    "    train_fp = train_dataset[features]\n",
    "    test_fp = test_dataset[features]\n",
    "    \n",
    "    if norm:\n",
    "        # Summary of training ( and test)\n",
    "        train_stats = train_fp.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "\n",
    "        test_stats = test_fp.describe()\n",
    "        test_stats = test_stats.transpose()\n",
    "        ######################################\n",
    "        \n",
    "        # Remove features with 0 std\n",
    "        my_set ={}\n",
    "        my_set.update(train_stats['std'][train_stats['std'] == 0])\n",
    "        my_set.update(test_stats['std'][test_stats['std'] == 0])\n",
    "\n",
    "\n",
    "        train_fp1 = train_fp.drop(my_set.keys(), axis=1)\n",
    "\n",
    "        test_fp1 = test_fp.drop(my_set.keys(), axis=1)\n",
    "        ###################################################\n",
    "        \n",
    "        # Normalization (check it this is required. Try without first)\n",
    "        def norm(x):\n",
    "            stats = x.describe()\n",
    "            stats = stats.transpose()\n",
    "            return (x - stats['mean']) / stats['std']\n",
    "\n",
    "        normed_train_fp = norm(train_fp1)\n",
    "        normed_test_fp = norm(test_fp1)\n",
    "        train_fp = normed_train_fp\n",
    "        test_fp = normed_test_fp\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_fp.to_numpy().astype(np.float32), \n",
    "                                                     train_label.to_numpy().astype(np.float32))).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_fp.to_numpy().astype(np.float32), \n",
    "                                                    test_label.to_numpy().astype(np.float32))).batch(batch_size)\n",
    "\n",
    "    return train_data, test_data\n",
    "    #model.fit(train_data, validation_data=train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def buildModel1(train_data, n_layer, n_unit, activation, loss, opt, dropout=False):\n",
    "    \n",
    "    #n_col = train_fp.shape[1]\n",
    "    n_col = train_data.element_spec[0].shape[1]\n",
    "    print(n_col)\n",
    "#     model_guts = [layers.Dense(n_unit, activation=activation, \n",
    "#                                input_shape=[n_col])] + [layers.Dense(n_unit, \n",
    "#                                 activation=activation) for i in range(n_layer - 1)] + [layers.Dense(1, activation='linear')]\n",
    "    model_guts = []\n",
    "    model_guts.append(layers.Dense(n_unit, activation=activation, \n",
    "                                input_shape=[n_col]))\n",
    "    for i in range(n_layer - 1):\n",
    "        model_guts.append(layers.Dense(n_unit, activation=activation))\n",
    "        if dropout:\n",
    "            model_guts.append(layers.Dropout(.3))\n",
    "    \n",
    "    model_guts.append(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model = keras.Sequential(model_guts)\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "        optimizer=opt,\n",
    "        metrics=['mae', 'mse'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, patience, val_pct, train_data):\n",
    "    EPOCHS = 2000\n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                          verbose=1, save_best_only=True, mode='min')\n",
    "    # early_history = model.fit(normed_train_data, train_label.to_numpy(), \n",
    "    #                     epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop,checkpoint_callbacks])\n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    early_history = model.fit(train_data,\n",
    "                        epochs=EPOCHS, validation_data = train_data, verbose=1,\\\n",
    "                              callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct, batch_size=10, norm=True):\n",
    "    '''\n",
    "    This function creates a model and returns its mse\n",
    "    '''\n",
    "    print(\"Patience: \", patience)\n",
    "    print(\"training_pct: \", training_pct)\n",
    "    print(\"n_layer: \", n_layer)\n",
    "    print(\"n_unit: \", n_unit)\n",
    "    print(\"activation: \", activation)\n",
    "    print(\"loss: \", loss)\n",
    "    print(\"opt: \", opt)\n",
    "    print(\"val_pct: \", val_pct)\n",
    "    train_data, test_data = getData(ml_data, total_frac, start_str, \n",
    "                                                                           end_str, \n",
    "                                    training_pct, batch_size=batch_size, norm=norm) #get normalized training and test data\n",
    "    model = buildModel1(train_data, n_layer, n_unit, activation, loss, opt)\n",
    "    \n",
    "    \n",
    "    trainModel(model, patience, val_pct, train_data)\n",
    "    \n",
    "    loss, mae, mse = model.evaluate(test_data, verbose=2)\n",
    "    \n",
    "    return mse, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varyParams(ml_data, default_params, grid, total_frac, start_str, end_str):\n",
    "    par_d = {}\n",
    "    for grid_k,grid_v in zip(grid.keys(), grid.values()):\n",
    "        for val in grid_v:\n",
    "            for def_k,def_v in zip(default_params.keys(), default_params.values()):\n",
    "                exec(def_k+\"=def_v\")\n",
    "                exec(\"global \" + def_k)\n",
    "            \n",
    "            exec(grid_k+\"=val\")\n",
    "            exec(\"global \" + grid_k)\n",
    "            l = list(default_params.keys())\n",
    "            mse = eval('evaluate_model(ml_data, total_frac, start_str, end_str, ' + l[0] + ',' + l[1] + ',' + l[2] + ',' + l[3] + ',' + l[4] + ',' + l[5] + ',' + l[6] + ',' + l[7] + ')')\n",
    "            par_d[val] = mse\n",
    "\n",
    "    #r = eval('[' + l[0] + '_d' + ', ' + l[1] + '_d' + ', ' + l[2] + '_d' + ', ' + l[3] + '_d' + ', ' + l[4] + '_d' + ', ' + l[5] + '_d' + ', ' + l[6] + '_d' + ', ' + l[7] + '_d' + ']')\n",
    "    return par_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fp_dat.sample(frac=training_pct,random_state=2)\n",
    "test_dataset = fp_dat.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_dataset[property_used]\n",
    "test_label = test_dataset[property_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, col in enumerate(ml_data.columns):\n",
    "    if start_str in col:\n",
    "        start_col = ind + 1\n",
    "    elif end_str == col:\n",
    "        end_col = ind\n",
    "\n",
    "\n",
    "features = list(ml_data.columns[start_col:end_col])\n",
    "other_props = ['norm_Dom._Pore_(ang.)',\n",
    " 'norm_Max._Pore_(ang.)',\n",
    " 'norm_Void_Fraction',\n",
    " 'norm_Surf._Area_(m2/g)',\n",
    " 'norm_Vol._Surf._Area',\n",
    " 'norm_Density',\n",
    "  'norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "\n",
    "features = features + other_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = train_dataset[features]\n",
    "test_fp = test_dataset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_fp.to_numpy().astype(np.float32), \n",
    "                                                     train_label.to_numpy().astype(np.float32))).batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices((test_fp.to_numpy().astype(np.float32), \n",
    "                                                    test_label.to_numpy().astype(np.float32))).batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of test and training sets\n",
    "fig,ax = plt.subplots(figsize = (8,5))\n",
    "n_bins=30\n",
    "n, bins, patches = plt.hist(train_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='#FDA65F', alpha=1,label = 'Training set')\n",
    "n, bins, patches = plt.hist(test_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='green', alpha=1, label = 'Test set')\n",
    "plt.xlabel('y_val',fontsize=labelfontsize)\n",
    "plt.ylabel('Count',fontsize=labelfontsize)\n",
    "#ax.set_xlim(2,12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig('%s.png'%property_used,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of training ( and test)\n",
    "train_stats = train_fp.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = test_fp.describe()\n",
    "test_stats = test_stats.transpose()\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats['std'][train_stats['std'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats['std'][test_stats['std'] == 0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set ={}\n",
    "my_set.update(train_stats['std'][train_stats['std'] == 0])\n",
    "my_set.update(test_stats['std'][test_stats['std'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp1 = train_fp.drop(my_set.keys(), axis=1)\n",
    "#test_fp1 = test_fp.drop(train_stats['std'][train_stats['std'] == 0].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp1 = test_fp.drop(my_set.keys(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (check it this is required. Try without first)\n",
    "def norm(x):\n",
    "    stats = x.describe()\n",
    "    stats = stats.transpose()\n",
    "    return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data = norm(train_fp1)\n",
    "normed_test_data = norm(test_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_fp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(100, activation='relu', input_shape=[len(train_fp.keys())]),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "        optimizer='adam',\n",
    "        metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model training\n",
    "EPOCHS = 5000\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                      verbose=1, save_best_only=True, mode='min')\n",
    "# early_history = model.fit(normed_train_data, train_label.to_numpy(), \n",
    "#                     epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop,checkpoint_callbacks])\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_history = model.fit(train_fp.to_numpy(), train_label.to_numpy(), \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "                          callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if run converged\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "#plt.ylim([0, 0.15])\n",
    "plt.ylabel('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of test error and plotting parity\n",
    "\n",
    "#model = tf.keras.models.load_model('model_checkpoint_bandgap.h5')\n",
    "loss, mae, mse = model.evaluate(test_fp.to_numpy(), test_label.to_numpy(), verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} bg\".format(mae))\n",
    "\n",
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_fp.to_numpy(), train_label.to_numpy(), verbose=2)\n",
    "\n",
    "tr_rmse = math.sqrt(tr_mse)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "test_predictions = model.predict(test_fp.to_numpy()).flatten()\n",
    "\n",
    "\n",
    "train_predictions = model.predict(train_fp.to_numpy()).flatten()\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "ax1.scatter(test_label, test_predictions, c='r',s=30)\n",
    "\n",
    "ax1.scatter(train_label, train_predictions, c='b',s=30)\n",
    "\n",
    "ax1.set_xlabel('True normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_ylabel('Predicted normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_xlim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.set_ylim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.legend()\n",
    "plot_x_min, plot_x_max = plt.xlim()\n",
    "plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.15\n",
    "\n",
    "ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "         \"RMSE train=\" + str(\"%.4f\" % tr_rmse), ha='left', fontsize=16)\n",
    "\n",
    "# ax1.text(text_position_x, text_position_y, \"MAE=\" + str(\"%.4f\" % mae) + ' \\n' + \n",
    "#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./%s_test_parity_%s.png'%(property_used, total_frac),dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - test_label\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_space(4,9, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "defaults = [10, .8, 3, 100, 'relu', 'mse', 'adam', .2]\n",
    "patience = defaults[0]\n",
    "training_pct = defaults[1]\n",
    "n_layer = defaults[2]\n",
    "n_unit = defaults[3]\n",
    "activation = defaults[4]\n",
    "loss = defaults[5]\n",
    "opt = defaults[6]\n",
    "val_pct = defaults[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = .1\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":7, \"n_unit\":20, \"activation\":'relu', \"loss\":'mean_absolute_error', \n",
    "            \"opt\":'rmsprop', \"val_pct\":.2}\n",
    "all_grid = {\"patience\":[10], \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":eq_space(.2, .5, 5)}\n",
    "\n",
    "\n",
    "init_grid = {\"val_pct\":eq_space(.2, .5, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#patience_d, training_pct_d, n_layer_d, n_unit_d, activation_d, loss_d, opt_d, val_pct_d \n",
    "r = varyParams(ml_data, defaults, init_grid, total_frac, start_str, end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "training_pct = .9\n",
    "total_frac = 1\n",
    "mse, model = evaluate_model(ml_data, total_frac, start_str, end_str, 10, training_pct, 3, 200, 'relu', \n",
    "                   'mse', 'adam', .2, batch_size=5000, norm=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_data, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}