{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ml_data = pd.read_csv('~/efrc/prep_data/no_cat_v1/data_DONOTOUCH/ml_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>Metal_ID</th>\n",
       "      <th>#_of_Linkers</th>\n",
       "      <th>L0_Smiles</th>\n",
       "      <th>L1_Smiles</th>\n",
       "      <th>L2_Smiles</th>\n",
       "      <th>L3_Smiles</th>\n",
       "      <th>L4_Smiles</th>\n",
       "      <th>L5_Smiles</th>\n",
       "      <th>...</th>\n",
       "      <th>std_CH4_v/v_35_bar</th>\n",
       "      <th>norm_CH4_v/v_65_bar</th>\n",
       "      <th>mean_CH4_v/v_65_bar</th>\n",
       "      <th>std_CH4_v/v_65_bar</th>\n",
       "      <th>norm_CH4_v/v_100_bar</th>\n",
       "      <th>mean_CH4_v/v_100_bar</th>\n",
       "      <th>std_CH4_v/v_100_bar</th>\n",
       "      <th>norm_CH4_v/v_248_bar</th>\n",
       "      <th>mean_CH4_v/v_248_bar</th>\n",
       "      <th>std_CH4_v/v_248_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hypotheticalMOF_32526_i_2_j_11_k_9_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\Br</td>\n",
       "      <td>[O]C(=O)/C=C/C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.404660</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.395204</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.963777</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hypotheticalMOF_32003_i_2_j_11_k_2_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C(/C=C(/C(=O)[O])\\F)\\F</td>\n",
       "      <td>[O]C(=O)/C=C(/C(=C(/C(=O)[O])\\F)/F)\\F</td>\n",
       "      <td>Fc1nccc(c1F)c1ccc(cc1)c1c(F)c(F)nc(c1F)F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.075329</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hypotheticalMOF_1003468_i_4_j_6_k_2_m_10.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>N#Cc1c(C#N)c(ccc1c1cc(C#N)c(c(c1)C#N)C(=O)[O])...</td>\n",
       "      <td>N#Cc1cc(cc(c1C#N)C#N)C(=O)[O]</td>\n",
       "      <td>N#Cc1cc(C#N)c(c(c1C#N)C(=O)[O])C#N</td>\n",
       "      <td>N#Cc1c(c2ccc(cc2)C(=O)[O])c(C#N)c(c(c1C#N)c1cc...</td>\n",
       "      <td>N#Cc1c(C#N)cc(c(c1C#N)C(=O)[O])C#N</td>\n",
       "      <td>N#Cc1cc(ccc1C#Cc1ccc(cc1)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.133157</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hypotheticalMOF_3001711_i_2_j_25_k_25_m_13.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.255085</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hypotheticalMOF_3498_i_0_j_7_k_3_m_0.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.457618</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        filename  Metal_ID  \\\n",
       "0           0      hypotheticalMOF_32526_i_2_j_11_k_9_m_3.cif         2   \n",
       "1           1      hypotheticalMOF_32003_i_2_j_11_k_2_m_1.cif         2   \n",
       "2           2    hypotheticalMOF_1003468_i_4_j_6_k_2_m_10.cif         4   \n",
       "3           3  hypotheticalMOF_3001711_i_2_j_25_k_25_m_13.cif         2   \n",
       "4           4        hypotheticalMOF_3498_i_0_j_7_k_3_m_0.cif         0   \n",
       "\n",
       "   #_of_Linkers                                          L0_Smiles  \\\n",
       "0             2                     [O]C(=O)/C=C/C=C(/C(=O)[O])\\Br   \n",
       "1             3                  [O]C(=O)/C=C(/C=C(/C(=O)[O])\\F)\\F   \n",
       "2             9  N#Cc1c(C#N)c(ccc1c1cc(C#N)c(c(c1)C#N)C(=O)[O])...   \n",
       "3             9                                       CCCOC(=O)[O]   \n",
       "4             3                     [O]C(=O)c1ccc(c2c1CC2)C(=O)[O]   \n",
       "\n",
       "                               L1_Smiles  \\\n",
       "0                  [O]C(=O)/C=C/C(=O)[O]   \n",
       "1  [O]C(=O)/C=C(/C(=C(/C(=O)[O])\\F)/F)\\F   \n",
       "2          N#Cc1cc(cc(c1C#N)C#N)C(=O)[O]   \n",
       "3                           CCCOC(=O)[O]   \n",
       "4         [O]C(=O)c1ccc(c2c1CC2)C(=O)[O]   \n",
       "\n",
       "                                   L2_Smiles  \\\n",
       "0                                        NaN   \n",
       "1   Fc1nccc(c1F)c1ccc(cc1)c1c(F)c(F)nc(c1F)F   \n",
       "2         N#Cc1cc(C#N)c(c(c1C#N)C(=O)[O])C#N   \n",
       "3                               CCCOC(=O)[O]   \n",
       "4  [O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1)C(=O)[O]   \n",
       "\n",
       "                                           L3_Smiles  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  N#Cc1c(c2ccc(cc2)C(=O)[O])c(C#N)c(c(c1C#N)c1cc...   \n",
       "3                                       CCCOC(=O)[O]   \n",
       "4                                                NaN   \n",
       "\n",
       "                            L4_Smiles  \\\n",
       "0                                 NaN   \n",
       "1                                 NaN   \n",
       "2  N#Cc1c(C#N)cc(c(c1C#N)C(=O)[O])C#N   \n",
       "3                        CCCOC(=O)[O]   \n",
       "4                                 NaN   \n",
       "\n",
       "                                    L5_Smiles  ... std_CH4_v/v_35_bar  \\\n",
       "0                                         NaN  ...          38.761663   \n",
       "1                                         NaN  ...          38.761663   \n",
       "2  N#Cc1cc(ccc1C#Cc1ccc(cc1)C(=O)[O])C(=O)[O]  ...          38.761663   \n",
       "3                                CCCOC(=O)[O]  ...          38.761663   \n",
       "4                                         NaN  ...          38.761663   \n",
       "\n",
       "  norm_CH4_v/v_65_bar mean_CH4_v/v_65_bar std_CH4_v/v_65_bar  \\\n",
       "0           -0.404660          179.769656          39.064489   \n",
       "1            0.101287          179.769656          39.064489   \n",
       "2            0.098730          179.769656          39.064489   \n",
       "3                 NaN          179.769656          39.064489   \n",
       "4                 NaN          179.769656          39.064489   \n",
       "\n",
       "  norm_CH4_v/v_100_bar mean_CH4_v/v_100_bar std_CH4_v/v_100_bar  \\\n",
       "0            -0.395204           204.360203           42.586407   \n",
       "1             0.075329           204.360203           42.586407   \n",
       "2             0.133157           204.360203           42.586407   \n",
       "3             0.255085           204.360203           42.586407   \n",
       "4             0.457618           204.360203           42.586407   \n",
       "\n",
       "  norm_CH4_v/v_248_bar mean_CH4_v/v_248_bar std_CH4_v/v_248_bar  \n",
       "0            -0.963777           247.593271           56.068422  \n",
       "1             0.093458           247.593271           56.068422  \n",
       "2             0.172458           247.593271           56.068422  \n",
       "3                  NaN           247.593271           56.068422  \n",
       "4                  NaN           247.593271           56.068422  \n",
       "\n",
       "[5 rows x 469 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "start_str = 'SMILES'\n",
    "end_str = 'valence_pa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>Metal_ID</th>\n",
       "      <th>#_of_Linkers</th>\n",
       "      <th>L0_Smiles</th>\n",
       "      <th>L1_Smiles</th>\n",
       "      <th>L2_Smiles</th>\n",
       "      <th>L3_Smiles</th>\n",
       "      <th>L4_Smiles</th>\n",
       "      <th>L5_Smiles</th>\n",
       "      <th>...</th>\n",
       "      <th>std_CH4_v/v_35_bar</th>\n",
       "      <th>norm_CH4_v/v_65_bar</th>\n",
       "      <th>mean_CH4_v/v_65_bar</th>\n",
       "      <th>std_CH4_v/v_65_bar</th>\n",
       "      <th>norm_CH4_v/v_100_bar</th>\n",
       "      <th>mean_CH4_v/v_100_bar</th>\n",
       "      <th>std_CH4_v/v_100_bar</th>\n",
       "      <th>norm_CH4_v/v_248_bar</th>\n",
       "      <th>mean_CH4_v/v_248_bar</th>\n",
       "      <th>std_CH4_v/v_248_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69176</th>\n",
       "      <td>69223</td>\n",
       "      <td>hypotheticalMOF_3593_i_0_j_7_k_4_m_2.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1Cl)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1Cl)/N=N/c1ccc(c(c1)Cl)C(=O)[O]</td>\n",
       "      <td>Clc1cc(C(=O)[O])c(cc1/N=N/c1c(Cl)cc(c(c1Cl)Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.354921</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21102</th>\n",
       "      <td>21110</td>\n",
       "      <td>hypotheticalMOF_5043233_i_1_j_23_k_9_m_1.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#C[C@@]12CC[C@@](C([C@H]1F)(F)F)([C@H...</td>\n",
       "      <td>[O]C(=O)/C=C(/C(=O)[O])\\F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.955686</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.810894</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.173564</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46293</th>\n",
       "      <td>46320</td>\n",
       "      <td>hypotheticalMOF_27246_i_1_j_18_k_18_m_2.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(Cl)c2c(c1)c1cc(Cl)c3c(c1cc2)cc(c(...</td>\n",
       "      <td>[O]C(=O)c1cc(Cl)c2c([c]1)c1[c]cc3c(c1cc2Cl)[c]...</td>\n",
       "      <td>Clc1cc2c(c3c1c(Cl)c(Cl)n[c]3)cc(c1c2cncc1Cl)Cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.016301</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18090</th>\n",
       "      <td>18096</td>\n",
       "      <td>hypotheticalMOF_5035712_i_1_j_20_k_0_m_5.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.465522</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.270543</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13754</th>\n",
       "      <td>13758</td>\n",
       "      <td>hypotheticalMOF_5031267_i_0_j_29_k_9_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C(=O)[O]</td>\n",
       "      <td>F/C(=C(\\C(=O)[O])/F)/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1c(F)cc(c(c1F)F)n1c(=O)c2c(F)cc3c4c2c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.005855</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.887350</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.592185</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39495</th>\n",
       "      <td>39517</td>\n",
       "      <td>hypotheticalMOF_5055665_i_1_j_28_k_21_m_4.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>C/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>N#C/C=C(/C#N)\\C</td>\n",
       "      <td>CO[C]1N(c2[c]cc(cc2C)C(=O)[O])C(=O)c2c3c1c(C)c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.090932</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.049613</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20167</td>\n",
       "      <td>hypotheticalMOF_35880_i_2_j_16_k_10_m_2.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[N]=CC(=[N])Cl.ClCl</td>\n",
       "      <td>[O]C(=O)/[C]=C/C=C/C(=O)[O].[Cl]</td>\n",
       "      <td>[O]C(=O)c1ccc2c([c]1)c1[c]cc(c(c1cc2Cl)C(=O)[O...</td>\n",
       "      <td>[CH]/[C]=C(\\Cl)/[CH]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.038009</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18876</th>\n",
       "      <td>18882</td>\n",
       "      <td>hypotheticalMOF_5008993_i_0_j_21_k_21_m_2.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>Cl/C(=C(\\C#CC(=O)[O])/Cl)/C#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-1.230404</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.556891</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.536843</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>12780</td>\n",
       "      <td>hypotheticalMOF_12646_i_0_j_16_k_1_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc2c(c1)c1ccc(c(c1c(c2F)F)C(=O)[O])F</td>\n",
       "      <td>Fc1cc(ccc1c1ccc(c(c1)F)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>FOC(=O)c1cc[c]c2c1[c]c(F)c1c2[c]c(c(c1)F)C(=O)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.441928</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65757</th>\n",
       "      <td>65799</td>\n",
       "      <td>hypotheticalMOF_8394_i_0_j_12_k_6_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCc1cc(C#Cc2cc(CC)c(c(c2CC)CC)C(=O)[O])c(c(c1C...</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>CCc1cc(ccc1C#Cc1c(CC)cc(c(c1CC)CC)C(=O)[O])C(=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.555553</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14868</th>\n",
       "      <td>14872</td>\n",
       "      <td>hypotheticalMOF_2000012_i_1_j_19_k_19_m_3.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.126752</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7449</th>\n",
       "      <td>7450</td>\n",
       "      <td>hypotheticalMOF_1000614_i_3_j_7_k_2_m_1.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Fc1cc(cc(c1c1ccc(c(c1)F)C(=O)[O])F)c1cc(F)c(cc...</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1F)/N=N/c1ccc(c(c1F)F)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.292017</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2534</td>\n",
       "      <td>hypotheticalMOF_5010169_i_0_j_22_k_11_m_7.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\N</td>\n",
       "      <td>[O]C(=O)[C@]12[C@@H](N)[C@@H](N)[C@](C([C@H]1N...</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.512362</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.375926</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.365204</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36442</th>\n",
       "      <td>36459</td>\n",
       "      <td>hypotheticalMOF_5019737_i_0_j_26_k_0_m_9.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(O)c(c(c1)O)C(=O)[O]</td>\n",
       "      <td>O/C(=C\\c1[c]cc(c(c1O)O)C(=O)[O])/c1ccc(cc1)C(=...</td>\n",
       "      <td>[O]C(=O)c1c(O)c(O)c(c(c1O)O)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.457002</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62958</th>\n",
       "      <td>63000</td>\n",
       "      <td>hypotheticalMOF_5042527_i_1_j_23_k_0_m_10.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>N#Cc1ncc(nc1)C#N</td>\n",
       "      <td>N#C[C@@H]1[C@@H](C#N)[C@@]2(C#CC(=O)[O])[C@@H]...</td>\n",
       "      <td>N#Cc1cc(C(=O)[O])c(cc1C(=O)[O])C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.571949</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.704145</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.847480</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46722</th>\n",
       "      <td>46749</td>\n",
       "      <td>hypotheticalMOF_1000454_i_3_j_6_k_2_m_8.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.372548</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>2771</td>\n",
       "      <td>hypotheticalMOF_5013931_i_0_j_24_k_3_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CC[C@@H]1[C@@H](CC)c2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCC1(CC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CC[C@@]12[C@@H]3[C@@H]4[C@]2([C@@]2([C@H]1[C@]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.172103</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.304832</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63998</th>\n",
       "      <td>64040</td>\n",
       "      <td>hypotheticalMOF_5074221_i_2_j_26_k_6_m_9.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Oc1cc(C(=O)[O])c(cc1/C=C/c1c(O)cc(cc1O)C(=O)[O])O</td>\n",
       "      <td>OC1=N[CH]C=C([CH]1)C#Cc1ccncc1</td>\n",
       "      <td>Oc1cc(C(=O)[O])c(c(c1C#Cc1c(O)cc(c(c1O)O)C(=O)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.221266</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.162555</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65587</th>\n",
       "      <td>65629</td>\n",
       "      <td>hypotheticalMOF_5049712_i_1_j_26_k_12_m_1.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>F/C(=C\\c1cc(F)c(c(c1)F)C(=O)[O])/c1ccc(cc1)C(=...</td>\n",
       "      <td>F/C(=C\\c1ccc(cc1)C(=O)[O])/c1cc(F)c(c(c1)F)C(=...</td>\n",
       "      <td>Fc1[c]c(/C=[C]/c2cc(F)nc(c2F)F)cc(n1)F.FF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.394484</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59592</th>\n",
       "      <td>59630</td>\n",
       "      <td>hypotheticalMOF_5043805_i_1_j_23_k_18_m_4.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Cc1cn[c]c2c1ccc1c2ccc2c1[c]ncc2.[CH3].[CH3]</td>\n",
       "      <td>[O]C(=O)c1ccc2c(c1)c1c(C)cc3c(c1cc2)[c]c(c(c3C...</td>\n",
       "      <td>[O]C(=O)C#C[C@@]12[C@@H](C)[C@@H](C)[C@@](C([C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.039952</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47742</th>\n",
       "      <td>47769</td>\n",
       "      <td>hypotheticalMOF_5017542_i_0_j_25_k_9_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CC[CH]O/C(=C(\\C([O])[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CC[CH]O/C(=C\\C(=O)[O])/C(=O)O</td>\n",
       "      <td>CCCO[C@]12[C@@]3(C#CC(=O)[O])[C@H]4[C@@]2([C@]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.066911</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60628</th>\n",
       "      <td>60669</td>\n",
       "      <td>hypotheticalMOF_5007885_i_0_j_21_k_13_m_10.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>N#Cc1cc2cc(C(=O)[O])c(c(c2cc1C(=O)[O])C#N)C#N</td>\n",
       "      <td>N#Cc1cc2c(C#N)c(cc(c2cc1C(=O)[O])C#N)C(=O)[O]</td>\n",
       "      <td>N#C/C(=C(\\C#CC(=O)[O])/C#N)/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.072655</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.167162</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.383556</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75522</th>\n",
       "      <td>75574</td>\n",
       "      <td>hypotheticalMOF_8101_i_0_j_12_k_3_m_6.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>CCCc1cc(C(=O)[O])c2c(c1C(=O)[O])C([C@H]2CCC)(C...</td>\n",
       "      <td>CCCC1(CCC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-2.133011</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49203</th>\n",
       "      <td>49230</td>\n",
       "      <td>hypotheticalMOF_5053686_i_1_j_27_k_26_m_13.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>CCCOc1cc(cc(c1c1nnc(nn1)c1c(OCCC)cc(cc1OCCC)C(...</td>\n",
       "      <td>CCCOC1=N[CH]C(=C([CH]1)/[C]=C/c1c(OCCC)cnc(c1[...</td>\n",
       "      <td>CCOCOc1cc(ccc1/C(=[C]/c1[c]cc(cc1)C(=O)[O])/O[...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.481760</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36370</th>\n",
       "      <td>36387</td>\n",
       "      <td>hypotheticalMOF_18994_i_1_j_7_k_0_m_5.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>c1[c]ccnc1.CCc1[c]c(C(=O)[O])c(cc1/N=N/c1c(CC)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.286482</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.445445</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.157183</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64251</th>\n",
       "      <td>64293</td>\n",
       "      <td>hypotheticalMOF_7002430_i_4_j_28_k_3_m_9.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Oc1ccc(c(c1)O)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2(O)O)C(=O)[O]</td>\n",
       "      <td>Oc1cc(O)cc(c1)C(=O)[O]</td>\n",
       "      <td>Oc1cc(O)c(c(c1)C(=O)[O])O</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1O)O)O</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)O)O</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.488360</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.827030</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.037695</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76287</th>\n",
       "      <td>76339</td>\n",
       "      <td>hypotheticalMOF_7002680_i_4_j_28_k_19_m_11.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>COc1cccc(c1)C(=O)[O]</td>\n",
       "      <td>COc1cccc(c1)C(=O)[O]</td>\n",
       "      <td>COc1cc(OC)cc(c1C(=O)[O])OC</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56573</th>\n",
       "      <td>56609</td>\n",
       "      <td>hypotheticalMOF_19577_i_1_j_7_k_6_m_8.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(c2ccccc2)c(c(c1c1ccccc1)c1ccccc1)...</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1c1ccccc1)C(=O)[O]</td>\n",
       "      <td>c1ccc(cc1)c1nccc(c1)C#Cc1ccncc1c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.626222</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31235</th>\n",
       "      <td>31248</td>\n",
       "      <td>hypotheticalMOF_1001308_i_3_j_10_k_9_m_12.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]C(/C=C/C(=O)[O])[O].[CH2]CO/C(=C(\\C(=O)[O])...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.653463</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.707547</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.201540</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42073</th>\n",
       "      <td>42097</td>\n",
       "      <td>hypotheticalMOF_31820_i_2_j_10_k_10_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Br/N=C\\C(=N)Br</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)/C=C/C(=[C]/C(=O)[O])/Br.[Br]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.418621</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41504</th>\n",
       "      <td>41527</td>\n",
       "      <td>hypotheticalMOF_5049272_i_1_j_26_k_5_m_11.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>N#Cc1ccc(c(c1)OC)C#N</td>\n",
       "      <td>COc1cc(C(=O)[O])c(cc1/C=C/c1c([O])c(OC)c(c(c1O...</td>\n",
       "      <td>COc1cc(C#CC(=O)[O])ccc1C#CC(=O)[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.374758</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.643970</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.839435</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52489</th>\n",
       "      <td>52520</td>\n",
       "      <td>hypotheticalMOF_30428_i_2_j_8_k_5_m_8.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#Cc1ccc(cc1c1ccccc1)[C@@]12[C@@H]3[CH...</td>\n",
       "      <td>N#Cc1ccc(c(c1)c1ccccc1)C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68755</th>\n",
       "      <td>68801</td>\n",
       "      <td>hypotheticalMOF_5020442_i_0_j_26_k_6_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCc1cc(C#Cc2ccc(cc2)C(=O)[O])ccc1C(=O)[O]</td>\n",
       "      <td>CCc1c(CC)c(/C=C/c2[c]c(CC)c(c(c2CC)CC)C(=O)[O]...</td>\n",
       "      <td>CCc1c(C#Cc2cc(CC)c(c(c2)CC)C(=O)[O])c(CC)c(c(c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.210698</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.552423</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.024328</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49811</th>\n",
       "      <td>49838</td>\n",
       "      <td>hypotheticalMOF_5045653_i_1_j_24_k_19_m_13.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>CCCO[C@]12[C@H]3[C@H]4[C@@]2([C@H]2[C@@H]1[C@@...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.965705</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>797</td>\n",
       "      <td>hypotheticalMOF_6003141_i_3_j_24_k_21_m_11.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>CO[C@]12[C@@H]3[C@@]4([C@H]1[C@@]1([C@]2([C@]3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.481962</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31921</th>\n",
       "      <td>31934</td>\n",
       "      <td>hypotheticalMOF_3000716_i_1_j_26_k_23_m_8.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>N#CC(C#N)(C#N)C#N</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.143828</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24675</th>\n",
       "      <td>24685</td>\n",
       "      <td>hypotheticalMOF_6001587_i_3_j_21_k_5_m_12.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>CCOc1cc(C#CC(=O)[O])c(cc1C#CC(=O)[O])OCC</td>\n",
       "      <td>CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.535856</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>47981</td>\n",
       "      <td>hypotheticalMOF_15854_i_0_j_18_k_6_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Fc1cc(C(=O)[O])c(cc1C#Cc1c(F)cc(c(c1F)F)C(=O)[...</td>\n",
       "      <td>[O]C(=O)c1cc2c3c(F)cc4c(c3cc(c2cc1F)F)c(F)c(c(...</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)F)C#Cc1cc(F)c(cc1F)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.319036</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2497</td>\n",
       "      <td>hypotheticalMOF_5066677_i_2_j_22_k_7_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Brc1ncc(c(c1)/N=N/c1c(Br)cncc1Br)Br</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.227695</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73135</th>\n",
       "      <td>73185</td>\n",
       "      <td>hypotheticalMOF_31137_i_2_j_10_k_0_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Fc1cnccn1</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.202383</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.490113</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>1872</td>\n",
       "      <td>hypotheticalMOF_5069694_i_2_j_24_k_5_m_4.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#Cc1ccc(cc1)C#CC(=O)[O]</td>\n",
       "      <td>N#Cc1cc(C)c(c(c1C)C)C#N</td>\n",
       "      <td>[O]C(=O)[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@]1(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>7878</td>\n",
       "      <td>hypotheticalMOF_6004262_i_3_j_27_k_2_m_14.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>N=Nc1cc(ccc1c1cc(N=N)c(cc1N=N)c1cc(N=N)c(cc1N=...</td>\n",
       "      <td>N=Nc1cc(C(=O)[O])c(c(c1c1nnc(nn1)c1ccc(c2c1n[n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.394362</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.388359</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.326295</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37619</th>\n",
       "      <td>37638</td>\n",
       "      <td>hypotheticalMOF_5008222_i_0_j_21_k_16_m_12.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCOc1c(O[CH2])c2c(ccc(c2c2c1cc(O[C]C)c(c2)[C][...</td>\n",
       "      <td>CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>CCO/C(=C\\C#[C])/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.784153</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.470121</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70608</th>\n",
       "      <td>70656</td>\n",
       "      <td>hypotheticalMOF_5062990_i_2_j_20_k_16_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>Brc1c(Br)ncc2c1ccc1c2c(Br)ccn1.[O]C(=O)c1cc2c(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67699</th>\n",
       "      <td>67745</td>\n",
       "      <td>hypotheticalMOF_13406_i_0_j_16_k_10_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc2c1ccc1c2c(OCCC)cc(c1C(=O)[O])OCCC)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.365916</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.099079</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38804</th>\n",
       "      <td>38825</td>\n",
       "      <td>hypotheticalMOF_5046420_i_1_j_25_k_2_m_9.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Oc1[c]c(ccc1c1ccc(c(c1)O)C(=O)[O])c1c(O)c(O)c(...</td>\n",
       "      <td>[O]C(=O)C#C[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@...</td>\n",
       "      <td>Oc1ncc(c(c1)c1ccc(c(c1)O)c1cc(O)ncc1O)O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.440158</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.651970</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>6922</td>\n",
       "      <td>hypotheticalMOF_1003717_i_4_j_10_k_0_m_5.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1c(ccc(c1CC)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1cc(C(=O)[O])c(c(c1C(=O)[O])CC)CC</td>\n",
       "      <td>CC/C(=C\\C(=O)[O])/C=C(/C(=O)[O])\\CC</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-1.646670</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18983</th>\n",
       "      <td>18989</td>\n",
       "      <td>hypotheticalMOF_5044313_i_1_j_23_k_23_m_7.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C[C@]12CC[C@]([C@@H](C1)N)([C@@H]([C...</td>\n",
       "      <td>[O]C(=O)C#C[C@]12C[C@@H](N)[C@]([C@H]([C@@H]1N...</td>\n",
       "      <td>N#C[C@@]12C[C@@H](N)[C@@](C(C1)(N)N)([C@H]([C@...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.940483</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32230</th>\n",
       "      <td>32243</td>\n",
       "      <td>hypotheticalMOF_116_i_0_j_0_k_0_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.558683</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.207872</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>17095</td>\n",
       "      <td>hypotheticalMOF_6001973_i_3_j_22_k_0_m_11.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>COc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CO[C@@H]1C[C@@]([CH2])(C(=O)[O])C([C@@H]([C]1C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-2.080005</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-2.395274</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-2.519903</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52620</th>\n",
       "      <td>52651</td>\n",
       "      <td>hypotheticalMOF_5072196_i_2_j_25_k_11_m_8.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N=CC(=N)c1ccccc1</td>\n",
       "      <td>[O]C(=O)/C(=C/C=C/C(=O)[O])/c1ccccc1</td>\n",
       "      <td>[O]C(=O)C#C[C@]12[C@@H]3[C@@H]4[C@H]1[C@@]1([C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.844760</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>39534</td>\n",
       "      <td>hypotheticalMOF_23807_i_1_j_14_k_1_m_10.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N#Cc1cc(C(=O)[O])c(cc1c1ccc(cc1C#N)C(=O)[O])C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.459399</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48600</th>\n",
       "      <td>48627</td>\n",
       "      <td>hypotheticalMOF_5082196_i_2_j_29_k_16_m_9.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>OC1=N[CH]c2c([C]1O)ccc1c2c(O)cc(n1)O</td>\n",
       "      <td>[O]C(=O)c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)C(=O)[O])O</td>\n",
       "      <td>OOc1n(c2[c]cc(c[c]2)C(=O)[O])c(OO)c2c3c1CC=c1c...</td>\n",
       "      <td>[O][C]c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)[C][O])O</td>\n",
       "      <td>[O][C]c1c(O)ccc2c1c(O)cc1c2c[c]c(c1O)O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.562211</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.364967</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55026</th>\n",
       "      <td>55061</td>\n",
       "      <td>hypotheticalMOF_6001255_i_3_j_20_k_14_m_5.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>CCc1cccc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.105969</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>42017</td>\n",
       "      <td>hypotheticalMOF_5075328_i_2_j_26_k_22_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>F/C(=C\\c1cc(F)c(c(c1F)F)C(=O)[O])/c1cc(F)c(cc1...</td>\n",
       "      <td>F[C@@H]1CN2[C@@H](CN1[C@H]([C@H]2F)F)F</td>\n",
       "      <td>[O]C(=O)[C@@]12CC[C@@](C(C1)(F)F)(C(C2(F)F)(F)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.346433</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>21251</td>\n",
       "      <td>hypotheticalMOF_32735_i_2_j_12_k_0_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(F)c(cc1F)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>Fc1ncc(nc1)F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-3.095739</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-3.379274</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-3.107787</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>45917</td>\n",
       "      <td>hypotheticalMOF_18966_i_1_j_7_k_0_m_2.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)Cl)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1Cl)/N=N/c1c(Cl)cc(c(c1Cl)Cl)C...</td>\n",
       "      <td>Clc1cnccn1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.559909</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>42638</td>\n",
       "      <td>hypotheticalMOF_5064665_i_2_j_21_k_10_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)/[C]=C/C(=C/C(=O)[O])/Br.[Br]</td>\n",
       "      <td>Br/N=C(\\C(=N)Br)/Br</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.128717</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.232652</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>43592</td>\n",
       "      <td>hypotheticalMOF_5081795_i_2_j_29_k_10_m_2.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C(=C/C(=O)[O])/Cl.ClO[C]1N(c2[c]c...</td>\n",
       "      <td>N=CC(=[N])Cl.[Cl]</td>\n",
       "      <td>[O][C]C=[CH]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.564215</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68268</th>\n",
       "      <td>68314</td>\n",
       "      <td>hypotheticalMOF_37209_i_2_j_18_k_2_m_4.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Cc1c(c2ccncc2)c(C)c(c(c1C)c1c(C)cnc(c1C)C)C</td>\n",
       "      <td>[O]C(=O)c1cc(C)c2c(c1)c1cc(C)c3c(c1c(c2)C)cc(c...</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1C)C)c1[c]cc([c]c1)c1cc(C)c(c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.535577</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.563105</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.274043</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76382 rows Ã— 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        filename  Metal_ID  \\\n",
       "69176       69223        hypotheticalMOF_3593_i_0_j_7_k_4_m_2.cif         0   \n",
       "21102       21110    hypotheticalMOF_5043233_i_1_j_23_k_9_m_1.cif         1   \n",
       "46293       46320     hypotheticalMOF_27246_i_1_j_18_k_18_m_2.cif         1   \n",
       "18090       18096    hypotheticalMOF_5035712_i_1_j_20_k_0_m_5.cif         1   \n",
       "13754       13758    hypotheticalMOF_5031267_i_0_j_29_k_9_m_1.cif         0   \n",
       "39495       39517   hypotheticalMOF_5055665_i_1_j_28_k_21_m_4.cif         1   \n",
       "20161       20167     hypotheticalMOF_35880_i_2_j_16_k_10_m_2.cif         2   \n",
       "18876       18882   hypotheticalMOF_5008993_i_0_j_21_k_21_m_2.cif         0   \n",
       "12776       12780      hypotheticalMOF_12646_i_0_j_16_k_1_m_1.cif         0   \n",
       "65757       65799       hypotheticalMOF_8394_i_0_j_12_k_6_m_5.cif         0   \n",
       "14868       14872   hypotheticalMOF_2000012_i_1_j_19_k_19_m_3.cif         1   \n",
       "7449         7450     hypotheticalMOF_1000614_i_3_j_7_k_2_m_1.cif         3   \n",
       "2533         2534   hypotheticalMOF_5010169_i_0_j_22_k_11_m_7.cif         0   \n",
       "36442       36459    hypotheticalMOF_5019737_i_0_j_26_k_0_m_9.cif         0   \n",
       "62958       63000   hypotheticalMOF_5042527_i_1_j_23_k_0_m_10.cif         1   \n",
       "46722       46749     hypotheticalMOF_1000454_i_3_j_6_k_2_m_8.cif         3   \n",
       "2770         2771    hypotheticalMOF_5013931_i_0_j_24_k_3_m_5.cif         0   \n",
       "63998       64040    hypotheticalMOF_5074221_i_2_j_26_k_6_m_9.cif         2   \n",
       "65587       65629   hypotheticalMOF_5049712_i_1_j_26_k_12_m_1.cif         1   \n",
       "59592       59630   hypotheticalMOF_5043805_i_1_j_23_k_18_m_4.cif         1   \n",
       "47742       47769   hypotheticalMOF_5017542_i_0_j_25_k_9_m_13.cif         0   \n",
       "60628       60669  hypotheticalMOF_5007885_i_0_j_21_k_13_m_10.cif         0   \n",
       "75522       75574       hypotheticalMOF_8101_i_0_j_12_k_3_m_6.cif         0   \n",
       "49203       49230  hypotheticalMOF_5053686_i_1_j_27_k_26_m_13.cif         1   \n",
       "36370       36387       hypotheticalMOF_18994_i_1_j_7_k_0_m_5.cif         1   \n",
       "64251       64293    hypotheticalMOF_7002430_i_4_j_28_k_3_m_9.cif         4   \n",
       "76287       76339  hypotheticalMOF_7002680_i_4_j_28_k_19_m_11.cif         4   \n",
       "56573       56609       hypotheticalMOF_19577_i_1_j_7_k_6_m_8.cif         1   \n",
       "31235       31248   hypotheticalMOF_1001308_i_3_j_10_k_9_m_12.cif         3   \n",
       "42073       42097     hypotheticalMOF_31820_i_2_j_10_k_10_m_3.cif         2   \n",
       "...           ...                                             ...       ...   \n",
       "41504       41527   hypotheticalMOF_5049272_i_1_j_26_k_5_m_11.cif         1   \n",
       "52489       52520       hypotheticalMOF_30428_i_2_j_8_k_5_m_8.cif         2   \n",
       "68755       68801    hypotheticalMOF_5020442_i_0_j_26_k_6_m_5.cif         0   \n",
       "49811       49838  hypotheticalMOF_5045653_i_1_j_24_k_19_m_13.cif         1   \n",
       "797           797  hypotheticalMOF_6003141_i_3_j_24_k_21_m_11.cif         3   \n",
       "31921       31934   hypotheticalMOF_3000716_i_1_j_26_k_23_m_8.cif         1   \n",
       "24675       24685   hypotheticalMOF_6001587_i_3_j_21_k_5_m_12.cif         3   \n",
       "47954       47981      hypotheticalMOF_15854_i_0_j_18_k_6_m_1.cif         0   \n",
       "2496         2497    hypotheticalMOF_5066677_i_2_j_22_k_7_m_3.cif         2   \n",
       "73135       73185      hypotheticalMOF_31137_i_2_j_10_k_0_m_1.cif         2   \n",
       "1871         1872    hypotheticalMOF_5069694_i_2_j_24_k_5_m_4.cif         2   \n",
       "7877         7878   hypotheticalMOF_6004262_i_3_j_27_k_2_m_14.cif         3   \n",
       "37619       37638  hypotheticalMOF_5008222_i_0_j_21_k_16_m_12.cif         0   \n",
       "70608       70656   hypotheticalMOF_5062990_i_2_j_20_k_16_m_3.cif         2   \n",
       "67699       67745    hypotheticalMOF_13406_i_0_j_16_k_10_m_13.cif         0   \n",
       "38804       38825    hypotheticalMOF_5046420_i_1_j_25_k_2_m_9.cif         1   \n",
       "6921         6922    hypotheticalMOF_1003717_i_4_j_10_k_0_m_5.cif         4   \n",
       "18983       18989   hypotheticalMOF_5044313_i_1_j_23_k_23_m_7.cif         1   \n",
       "32230       32243        hypotheticalMOF_116_i_0_j_0_k_0_m_13.cif         0   \n",
       "17089       17095   hypotheticalMOF_6001973_i_3_j_22_k_0_m_11.cif         3   \n",
       "52620       52651   hypotheticalMOF_5072196_i_2_j_25_k_11_m_8.cif         2   \n",
       "39512       39534     hypotheticalMOF_23807_i_1_j_14_k_1_m_10.cif         1   \n",
       "48600       48627   hypotheticalMOF_5082196_i_2_j_29_k_16_m_9.cif         2   \n",
       "55026       55061   hypotheticalMOF_6001255_i_3_j_20_k_14_m_5.cif         3   \n",
       "41993       42017   hypotheticalMOF_5075328_i_2_j_26_k_22_m_1.cif         2   \n",
       "21243       21251      hypotheticalMOF_32735_i_2_j_12_k_0_m_1.cif         2   \n",
       "45891       45917       hypotheticalMOF_18966_i_1_j_7_k_0_m_2.cif         1   \n",
       "42613       42638   hypotheticalMOF_5064665_i_2_j_21_k_10_m_3.cif         2   \n",
       "43567       43592   hypotheticalMOF_5081795_i_2_j_29_k_10_m_2.cif         2   \n",
       "68268       68314      hypotheticalMOF_37209_i_2_j_18_k_2_m_4.cif         2   \n",
       "\n",
       "       #_of_Linkers                                          L0_Smiles  \\\n",
       "69176             3        [O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1Cl)C(=O)[O]   \n",
       "21102             2  [O]C(=O)C#C[C@@]12CC[C@@](C([C@H]1F)(F)F)([C@H...   \n",
       "46293             3  [O]C(=O)c1cc(Cl)c2c(c1)c1cc(Cl)c3c(c1cc2)cc(c(...   \n",
       "18090             1                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "13754             3                              [O]C(=O)/C=C/C(=O)[O]   \n",
       "39495             3                    C/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "20161             4                                [N]=CC(=[N])Cl.ClCl   \n",
       "18876             3                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "12776             3    [O]C(=O)c1ccc2c(c1)c1ccc(c(c1c(c2F)F)C(=O)[O])F   \n",
       "65757             3  CCc1cc(C#Cc2cc(CC)c(c(c2CC)CC)C(=O)[O])c(c(c1C...   \n",
       "14868             8           [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449              2  Fc1cc(cc(c1c1ccc(c(c1)F)C(=O)[O])F)c1cc(F)c(cc...   \n",
       "2533              3                      [O]C(=O)/C=C/C=C(/C(=O)[O])\\N   \n",
       "36442             3                   [O]C(=O)c1cc(O)c(c(c1)O)C(=O)[O]   \n",
       "62958             3                                   N#Cc1ncc(nc1)C#N   \n",
       "46722             2            [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]   \n",
       "2770              3     CC[C@@H]1[C@@H](CC)c2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "63998             3  Oc1cc(C(=O)[O])c(cc1/C=C/c1c(O)cc(cc1O)C(=O)[O])O   \n",
       "65587             3  F/C(=C\\c1cc(F)c(c(c1)F)C(=O)[O])/c1ccc(cc1)C(=...   \n",
       "59592             3        Cc1cn[c]c2c1ccc1c2ccc2c1[c]ncc2.[CH3].[CH3]   \n",
       "47742             3            CC[CH]O/C(=C(\\C([O])[O])/OCCC)/C(=O)[O]   \n",
       "60628             3      N#Cc1cc2cc(C(=O)[O])c(c(c2cc1C(=O)[O])C#N)C#N   \n",
       "75522             3                                   [O]C(=O)C(=O)[O]   \n",
       "49203             3  CCCOc1cc(cc(c1c1nnc(nn1)c1c(OCCC)cc(cc1OCCC)C(...   \n",
       "36370             1  c1[c]ccnc1.CCc1[c]c(C(=O)[O])c(cc1/N=N/c1c(CC)...   \n",
       "64251             8                             Oc1ccc(c(c1)O)C(=O)[O]   \n",
       "76287             8                                   [O]C(=O)c1ccccc1   \n",
       "56573             3  [O]C(=O)c1cc(c2ccccc2)c(c(c1c1ccccc1)c1ccccc1)...   \n",
       "31235             1  [O]C(/C=C/C(=O)[O])[O].[CH2]CO/C(=C(\\C(=O)[O])...   \n",
       "42073             3                                     Br/N=C\\C(=N)Br   \n",
       "...             ...                                                ...   \n",
       "41504             6                               N#Cc1ccc(c(c1)OC)C#N   \n",
       "52489             2  [O]C(=O)C#Cc1ccc(cc1c1ccccc1)[C@@]12[C@@H]3[CH...   \n",
       "68755             3          CCc1cc(C#Cc2ccc(cc2)C(=O)[O])ccc1C(=O)[O]   \n",
       "49811             2                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "797               2                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "31921            10                                   [O]C(=O)c1ccccc1   \n",
       "24675             2           CCOc1cc(C#CC(=O)[O])c(cc1C#CC(=O)[O])OCC   \n",
       "47954             3  Fc1cc(C(=O)[O])c(cc1C#Cc1c(F)cc(c(c1F)F)C(=O)[...   \n",
       "2496              1                Brc1ncc(c(c1)/N=N/c1c(Br)cncc1Br)Br   \n",
       "73135             3                                          Fc1cnccn1   \n",
       "1871              3                   [O]C(=O)C#Cc1ccc(cc1)C#CC(=O)[O]   \n",
       "7877              2  N=Nc1cc(ccc1c1cc(N=N)c(cc1N=N)c1cc(N=N)c(cc1N=...   \n",
       "37619             3  CCOc1c(O[CH2])c2c(ccc(c2c2c1cc(O[C]C)c(c2)[C][...   \n",
       "70608             2                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "67699             3            CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]   \n",
       "38804             3  Oc1[c]c(ccc1c1ccc(c(c1)O)C(=O)[O])c1c(O)c(O)c(...   \n",
       "6921              6                       CCc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "18983             3  [O]C(=O)C#C[C@]12CC[C@]([C@@H](C1)N)([C@@H]([C...   \n",
       "32230             3                         [O]C(=O)c1ccc(cc1)C(=O)[O]   \n",
       "17089             2                       COc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "52620             3                                   N=CC(=N)c1ccccc1   \n",
       "39512             1    N#Cc1cc(C(=O)[O])c(cc1c1ccc(cc1C#N)C(=O)[O])C#N   \n",
       "48600             5               OC1=N[CH]c2c([C]1O)ccc1c2c(O)cc(n1)O   \n",
       "55026             2                 CCc1cccc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "41993             3  F/C(=C\\c1cc(F)c(c(c1F)F)C(=O)[O])/c1cc(F)c(cc1...   \n",
       "21243             3                     [O]C(=O)c1cc(F)c(cc1F)C(=O)[O]   \n",
       "45891             3                     [O]C(=O)c1ccc(c(c1)Cl)C(=O)[O]   \n",
       "42613             3                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "43567             3  [O]C(=O)/C=C/C(=C/C(=O)[O])/Cl.ClO[C]1N(c2[c]c...   \n",
       "68268             3        Cc1c(c2ccncc2)c(C)c(c(c1C)c1c(C)cnc(c1C)C)C   \n",
       "\n",
       "                                               L1_Smiles  \\\n",
       "69176    [O]C(=O)c1ccc(cc1Cl)/N=N/c1ccc(c(c1)Cl)C(=O)[O]   \n",
       "21102                          [O]C(=O)/C=C(/C(=O)[O])\\F   \n",
       "46293  [O]C(=O)c1cc(Cl)c2c([c]1)c1[c]cc3c(c1cc2Cl)[c]...   \n",
       "18090                                                NaN   \n",
       "13754                      F/C(=C(\\C(=O)[O])/F)/C(=O)[O]   \n",
       "39495                                    N#C/C=C(/C#N)\\C   \n",
       "20161                   [O]C(=O)/[C]=C/C=C/C(=O)[O].[Cl]   \n",
       "18876              Cl/C(=C(\\C#CC(=O)[O])/Cl)/C#CC(=O)[O]   \n",
       "12776           Fc1cc(ccc1c1ccc(c(c1)F)C(=O)[O])C(=O)[O]   \n",
       "65757                                   [O]C(=O)C(=O)[O]   \n",
       "14868         [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br   \n",
       "7449      [O]C(=O)c1ccc(cc1F)/N=N/c1ccc(c(c1F)F)C(=O)[O]   \n",
       "2533   [O]C(=O)[C@]12[C@@H](N)[C@@H](N)[C@](C([C@H]1N...   \n",
       "36442  O/C(=C\\c1[c]cc(c(c1O)O)C(=O)[O])/c1ccc(cc1)C(=...   \n",
       "62958  N#C[C@@H]1[C@@H](C#N)[C@@]2(C#CC(=O)[O])[C@@H]...   \n",
       "46722            [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]   \n",
       "2770                CCC1(CC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "63998                     OC1=N[CH]C=C([CH]1)C#Cc1ccncc1   \n",
       "65587  F/C(=C\\c1ccc(cc1)C(=O)[O])/c1cc(F)c(c(c1)F)C(=...   \n",
       "59592  [O]C(=O)c1ccc2c(c1)c1c(C)cc3c(c1cc2)[c]c(c(c3C...   \n",
       "47742                      CC[CH]O/C(=C\\C(=O)[O])/C(=O)O   \n",
       "60628      N#Cc1cc2c(C#N)c(cc(c2cc1C(=O)[O])C#N)C(=O)[O]   \n",
       "75522  CCCc1cc(C(=O)[O])c2c(c1C(=O)[O])C([C@H]2CCC)(C...   \n",
       "49203  CCCOC1=N[CH]C(=C([CH]1)/[C]=C/c1c(OCCC)cnc(c1[...   \n",
       "36370                                                NaN   \n",
       "64251                 [O]C(=O)c1ccc(c2c1CC2(O)O)C(=O)[O]   \n",
       "76287                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "56573    [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1c1ccccc1)C(=O)[O]   \n",
       "31235                                                NaN   \n",
       "42073                          [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "...                                                  ...   \n",
       "41504  COc1cc(C(=O)[O])c(cc1/C=C/c1c([O])c(OC)c(c(c1O...   \n",
       "52489                         N#Cc1ccc(c(c1)c1ccccc1)C#N   \n",
       "68755  CCc1c(CC)c(/C=C/c2[c]c(CC)c(c(c2CC)CC)C(=O)[O]...   \n",
       "49811  CCCO[C@]12[C@H]3[C@H]4[C@@]2([C@H]2[C@@H]1[C@@...   \n",
       "797    CO[C@]12[C@@H]3[C@@]4([C@H]1[C@@]1([C@]2([C@]3...   \n",
       "31921                                   [O]C(=O)c1ccccc1   \n",
       "24675                  CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "47954  [O]C(=O)c1cc2c3c(F)cc4c(c3cc(c2cc1F)F)c(F)c(c(...   \n",
       "2496                                                 NaN   \n",
       "73135                          [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "1871                             N#Cc1cc(C)c(c(c1C)C)C#N   \n",
       "7877   N=Nc1cc(C(=O)[O])c(c(c1c1nnc(nn1)c1ccc(c2c1n[n...   \n",
       "37619                  CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "70608  Brc1c(Br)ncc2c1ccc1c2c(Br)ccn1.[O]C(=O)c1cc2c(...   \n",
       "67699            CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]   \n",
       "38804  [O]C(=O)C#C[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@...   \n",
       "6921                        CCc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "18983  [O]C(=O)C#C[C@]12C[C@@H](N)[C@]([C@H]([C@@H]1N...   \n",
       "32230               CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]   \n",
       "17089  CO[C@@H]1C[C@@]([CH2])(C(=O)[O])C([C@@H]([C]1C...   \n",
       "52620               [O]C(=O)/C(=C/C=C/C(=O)[O])/c1ccccc1   \n",
       "39512                                                NaN   \n",
       "48600   [O]C(=O)c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)C(=O)[O])O   \n",
       "55026                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "41993             F[C@@H]1CN2[C@@H](CN1[C@H]([C@H]2F)F)F   \n",
       "21243                                   [O]C(=O)C(=O)[O]   \n",
       "45891  [O]C(=O)c1ccc(cc1Cl)/N=N/c1c(Cl)cc(c(c1Cl)Cl)C...   \n",
       "42613              [O]C(=O)/[C]=C/C(=C/C(=O)[O])/Br.[Br]   \n",
       "43567                                  N=CC(=[N])Cl.[Cl]   \n",
       "68268  [O]C(=O)c1cc(C)c2c(c1)c1cc(C)c3c(c1c(c2)C)cc(c...   \n",
       "\n",
       "                                               L2_Smiles  \\\n",
       "69176  Clc1cc(C(=O)[O])c(cc1/N=N/c1c(Cl)cc(c(c1Cl)Cl)...   \n",
       "21102                                                NaN   \n",
       "46293  Clc1cc2c(c3c1c(Cl)c(Cl)n[c]3)cc(c1c2cncc1Cl)Cl...   \n",
       "18090                                                NaN   \n",
       "13754  [O]C(=O)c1c(F)cc(c(c1F)F)n1c(=O)c2c(F)cc3c4c2c...   \n",
       "39495  CO[C]1N(c2[c]cc(cc2C)C(=O)[O])C(=O)c2c3c1c(C)c...   \n",
       "20161  [O]C(=O)c1ccc2c([c]1)c1[c]cc(c(c1cc2Cl)C(=O)[O...   \n",
       "18876                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "12776  FOC(=O)c1cc[c]c2c1[c]c(F)c1c2[c]c(c(c1)F)C(=O)...   \n",
       "65757  CCc1cc(ccc1C#Cc1c(CC)cc(c(c1CC)CC)C(=O)[O])C(=...   \n",
       "14868         [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br   \n",
       "7449                                                 NaN   \n",
       "2533                           [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "36442               [O]C(=O)c1c(O)c(O)c(c(c1O)O)C(=O)[O]   \n",
       "62958                 N#Cc1cc(C(=O)[O])c(cc1C(=O)[O])C#N   \n",
       "46722                                                NaN   \n",
       "2770   CC[C@@]12[C@@H]3[C@@H]4[C@]2([C@@]2([C@H]1[C@]...   \n",
       "63998  Oc1cc(C(=O)[O])c(c(c1C#Cc1c(O)cc(c(c1O)O)C(=O)...   \n",
       "65587          Fc1[c]c(/C=[C]/c2cc(F)nc(c2F)F)cc(n1)F.FF   \n",
       "59592  [O]C(=O)C#C[C@@]12[C@@H](C)[C@@H](C)[C@@](C([C...   \n",
       "47742  CCCO[C@]12[C@@]3(C#CC(=O)[O])[C@H]4[C@@]2([C@]...   \n",
       "60628            N#C/C(=C(\\C#CC(=O)[O])/C#N)/C#CC(=O)[O]   \n",
       "75522             CCCC1(CCC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "49203  CCOCOc1cc(ccc1/C(=[C]/c1[c]cc(cc1)C(=O)[O])/O[...   \n",
       "36370                                                NaN   \n",
       "64251                             Oc1cc(O)cc(c1)C(=O)[O]   \n",
       "76287                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "56573            c1ccc(cc1)c1nccc(c1)C#Cc1ccncc1c1ccccc1   \n",
       "31235                                                NaN   \n",
       "42073              [O]C(=O)/C=C/C(=[C]/C(=O)[O])/Br.[Br]   \n",
       "...                                                  ...   \n",
       "41504                 COc1cc(C#CC(=O)[O])ccc1C#CC(=O)[O]   \n",
       "52489                                                NaN   \n",
       "68755  CCc1c(C#Cc2cc(CC)c(c(c2)CC)C(=O)[O])c(CC)c(c(c...   \n",
       "49811                                                NaN   \n",
       "797                                                  NaN   \n",
       "31921                                  N#CC(C#N)(C#N)C#N   \n",
       "24675                                                NaN   \n",
       "47954     [O]C(=O)c1ccc(c(c1)F)C#Cc1cc(F)c(cc1F)C(=O)[O]   \n",
       "2496                                                 NaN   \n",
       "73135                      [O]C(=O)/C=C/C=C(/C(=O)[O])\\F   \n",
       "1871   [O]C(=O)[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@]1(...   \n",
       "7877                                                 NaN   \n",
       "37619                        CCO/C(=C\\C#[C])/C#CC(=O)[O]   \n",
       "70608                                                NaN   \n",
       "67699  CCCOc1cc(cc2c1ccc1c2c(OCCC)cc(c1C(=O)[O])OCCC)...   \n",
       "38804            Oc1ncc(c(c1)c1ccc(c(c1)O)c1cc(O)ncc1O)O   \n",
       "6921                    CCc1c(ccc(c1CC)C(=O)[O])C(=O)[O]   \n",
       "18983  N#C[C@@]12C[C@@H](N)[C@@](C(C1)(N)N)([C@H]([C@...   \n",
       "32230               CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]   \n",
       "17089                                                NaN   \n",
       "52620  [O]C(=O)C#C[C@]12[C@@H]3[C@@H]4[C@H]1[C@@]1([C...   \n",
       "39512                                                NaN   \n",
       "48600  OOc1n(c2[c]cc(c[c]2)C(=O)[O])c(OO)c2c3c1CC=c1c...   \n",
       "55026                                                NaN   \n",
       "41993  [O]C(=O)[C@@]12CC[C@@](C(C1)(F)F)(C(C2(F)F)(F)...   \n",
       "21243                                       Fc1ncc(nc1)F   \n",
       "45891                                         Clc1cnccn1   \n",
       "42613                                Br/N=C(\\C(=N)Br)/Br   \n",
       "43567                                       [O][C]C=[CH]   \n",
       "68268  [O]C(=O)c1ccc(c(c1C)C)c1[c]cc([c]c1)c1cc(C)c(c...   \n",
       "\n",
       "                                          L3_Smiles  \\\n",
       "69176                                           NaN   \n",
       "21102                                           NaN   \n",
       "46293                                           NaN   \n",
       "18090                                           NaN   \n",
       "13754                                           NaN   \n",
       "39495                                           NaN   \n",
       "20161                          [CH]/[C]=C(\\Cl)/[CH]   \n",
       "18876                                           NaN   \n",
       "12776                                           NaN   \n",
       "65757                                           NaN   \n",
       "14868      [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449                                            NaN   \n",
       "2533                                            NaN   \n",
       "36442                                           NaN   \n",
       "62958                                           NaN   \n",
       "46722                                           NaN   \n",
       "2770                                            NaN   \n",
       "63998                                           NaN   \n",
       "65587                                           NaN   \n",
       "59592                                           NaN   \n",
       "47742                                           NaN   \n",
       "60628                                           NaN   \n",
       "75522                                           NaN   \n",
       "49203                                           NaN   \n",
       "36370                                           NaN   \n",
       "64251                     Oc1cc(O)c(c(c1)C(=O)[O])O   \n",
       "76287                          COc1cccc(c1)C(=O)[O]   \n",
       "56573                                           NaN   \n",
       "31235                                           NaN   \n",
       "42073                                           NaN   \n",
       "...                                             ...   \n",
       "41504                                          C[O]   \n",
       "52489                                           NaN   \n",
       "68755                                           NaN   \n",
       "49811                                           NaN   \n",
       "797                                             NaN   \n",
       "31921                              [O]C(=O)c1ccccc1   \n",
       "24675                                           NaN   \n",
       "47954                                           NaN   \n",
       "2496                                            NaN   \n",
       "73135                                           NaN   \n",
       "1871                                            NaN   \n",
       "7877                                            NaN   \n",
       "37619                                           NaN   \n",
       "70608                                           NaN   \n",
       "67699                                           NaN   \n",
       "38804                                           NaN   \n",
       "6921           CCc1cc(C(=O)[O])c(c(c1C(=O)[O])CC)CC   \n",
       "18983                                           NaN   \n",
       "32230                                           NaN   \n",
       "17089                                           NaN   \n",
       "52620                                           NaN   \n",
       "39512                                           NaN   \n",
       "48600  [O][C]c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)[C][O])O   \n",
       "55026                                           NaN   \n",
       "41993                                           NaN   \n",
       "21243                                           NaN   \n",
       "45891                                           NaN   \n",
       "42613                                           NaN   \n",
       "43567                                           NaN   \n",
       "68268                                           NaN   \n",
       "\n",
       "                                      L4_Smiles  \\\n",
       "69176                                       NaN   \n",
       "21102                                       NaN   \n",
       "46293                                       NaN   \n",
       "18090                                       NaN   \n",
       "13754                                       NaN   \n",
       "39495                                       NaN   \n",
       "20161                                       NaN   \n",
       "18876                                       NaN   \n",
       "12776                                       NaN   \n",
       "65757                                       NaN   \n",
       "14868  [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449                                        NaN   \n",
       "2533                                        NaN   \n",
       "36442                                       NaN   \n",
       "62958                                       NaN   \n",
       "46722                                       NaN   \n",
       "2770                                        NaN   \n",
       "63998                                       NaN   \n",
       "65587                                       NaN   \n",
       "59592                                       NaN   \n",
       "47742                                       NaN   \n",
       "60628                                       NaN   \n",
       "75522                                       NaN   \n",
       "49203                                       NaN   \n",
       "36370                                       NaN   \n",
       "64251                   [O]C(=O)c1ccc(c(c1O)O)O   \n",
       "76287                      COc1cccc(c1)C(=O)[O]   \n",
       "56573                                       NaN   \n",
       "31235                                       NaN   \n",
       "42073                                       NaN   \n",
       "...                                         ...   \n",
       "41504                                      C[O]   \n",
       "52489                                       NaN   \n",
       "68755                                       NaN   \n",
       "49811                                       NaN   \n",
       "797                                         NaN   \n",
       "31921                          [O]C(=O)c1ccccc1   \n",
       "24675                                       NaN   \n",
       "47954                                       NaN   \n",
       "2496                                        NaN   \n",
       "73135                                       NaN   \n",
       "1871                                        NaN   \n",
       "7877                                        NaN   \n",
       "37619                                       NaN   \n",
       "70608                                       NaN   \n",
       "67699                                       NaN   \n",
       "38804                                       NaN   \n",
       "6921        CC/C(=C\\C(=O)[O])/C=C(/C(=O)[O])\\CC   \n",
       "18983                                       NaN   \n",
       "32230                                       NaN   \n",
       "17089                                       NaN   \n",
       "52620                                       NaN   \n",
       "39512                                       NaN   \n",
       "48600    [O][C]c1c(O)ccc2c1c(O)cc1c2c[c]c(c1O)O   \n",
       "55026                                       NaN   \n",
       "41993                                       NaN   \n",
       "21243                                       NaN   \n",
       "45891                                       NaN   \n",
       "42613                                       NaN   \n",
       "43567                                       NaN   \n",
       "68268                                       NaN   \n",
       "\n",
       "                                        L5_Smiles  ... std_CH4_v/v_35_bar  \\\n",
       "69176                                         NaN  ...          38.761663   \n",
       "21102                                         NaN  ...          38.761663   \n",
       "46293                                         NaN  ...          38.761663   \n",
       "18090                                         NaN  ...          38.761663   \n",
       "13754                                         NaN  ...          38.761663   \n",
       "39495                                         NaN  ...          38.761663   \n",
       "20161                                         NaN  ...          38.761663   \n",
       "18876                                         NaN  ...          38.761663   \n",
       "12776                                         NaN  ...          38.761663   \n",
       "65757                                         NaN  ...          38.761663   \n",
       "14868  [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br  ...          38.761663   \n",
       "7449                                          NaN  ...          38.761663   \n",
       "2533                                          NaN  ...          38.761663   \n",
       "36442                                         NaN  ...          38.761663   \n",
       "62958                                         NaN  ...          38.761663   \n",
       "46722                                         NaN  ...          38.761663   \n",
       "2770                                          NaN  ...          38.761663   \n",
       "63998                                         NaN  ...          38.761663   \n",
       "65587                                         NaN  ...          38.761663   \n",
       "59592                                         NaN  ...          38.761663   \n",
       "47742                                         NaN  ...          38.761663   \n",
       "60628                                         NaN  ...          38.761663   \n",
       "75522                                         NaN  ...          38.761663   \n",
       "49203                                         NaN  ...          38.761663   \n",
       "36370                                         NaN  ...          38.761663   \n",
       "64251                      [O]C(=O)c1ccc(c(c1)O)O  ...          38.761663   \n",
       "76287                  COc1cc(OC)cc(c1C(=O)[O])OC  ...          38.761663   \n",
       "56573                                         NaN  ...          38.761663   \n",
       "31235                                         NaN  ...          38.761663   \n",
       "42073                                         NaN  ...          38.761663   \n",
       "...                                           ...  ...                ...   \n",
       "41504                                        C[O]  ...          38.761663   \n",
       "52489                                         NaN  ...          38.761663   \n",
       "68755                                         NaN  ...          38.761663   \n",
       "49811                                         NaN  ...          38.761663   \n",
       "797                                           NaN  ...          38.761663   \n",
       "31921                            [O]C(=O)c1ccccc1  ...          38.761663   \n",
       "24675                                         NaN  ...          38.761663   \n",
       "47954                                         NaN  ...          38.761663   \n",
       "2496                                          NaN  ...          38.761663   \n",
       "73135                                         NaN  ...          38.761663   \n",
       "1871                                          NaN  ...          38.761663   \n",
       "7877                                          NaN  ...          38.761663   \n",
       "37619                                         NaN  ...          38.761663   \n",
       "70608                                         NaN  ...          38.761663   \n",
       "67699                                         NaN  ...          38.761663   \n",
       "38804                                         NaN  ...          38.761663   \n",
       "6921                 CCc1cc(ccc1C(=O)[O])C(=O)[O]  ...          38.761663   \n",
       "18983                                         NaN  ...          38.761663   \n",
       "32230                                         NaN  ...          38.761663   \n",
       "17089                                         NaN  ...          38.761663   \n",
       "52620                                         NaN  ...          38.761663   \n",
       "39512                                         NaN  ...          38.761663   \n",
       "48600                                         NaN  ...          38.761663   \n",
       "55026                                         NaN  ...          38.761663   \n",
       "41993                                         NaN  ...          38.761663   \n",
       "21243                                         NaN  ...          38.761663   \n",
       "45891                                         NaN  ...          38.761663   \n",
       "42613                                         NaN  ...          38.761663   \n",
       "43567                                         NaN  ...          38.761663   \n",
       "68268                                         NaN  ...          38.761663   \n",
       "\n",
       "      norm_CH4_v/v_65_bar mean_CH4_v/v_65_bar std_CH4_v/v_65_bar  \\\n",
       "69176                 NaN          179.769656          39.064489   \n",
       "21102           -0.955686          179.769656          39.064489   \n",
       "46293                 NaN          179.769656          39.064489   \n",
       "18090                 NaN          179.769656          39.064489   \n",
       "13754            1.005855          179.769656          39.064489   \n",
       "39495                 NaN          179.769656          39.064489   \n",
       "20161                 NaN          179.769656          39.064489   \n",
       "18876           -1.230404          179.769656          39.064489   \n",
       "12776                 NaN          179.769656          39.064489   \n",
       "65757                 NaN          179.769656          39.064489   \n",
       "14868                 NaN          179.769656          39.064489   \n",
       "7449                  NaN          179.769656          39.064489   \n",
       "2533             0.512362          179.769656          39.064489   \n",
       "36442                 NaN          179.769656          39.064489   \n",
       "62958           -0.571949          179.769656          39.064489   \n",
       "46722                 NaN          179.769656          39.064489   \n",
       "2770             0.464247          179.769656          39.064489   \n",
       "63998           -0.221266          179.769656          39.064489   \n",
       "65587                 NaN          179.769656          39.064489   \n",
       "59592                 NaN          179.769656          39.064489   \n",
       "47742                 NaN          179.769656          39.064489   \n",
       "60628           -0.072655          179.769656          39.064489   \n",
       "75522                 NaN          179.769656          39.064489   \n",
       "49203                 NaN          179.769656          39.064489   \n",
       "36370            1.286482          179.769656          39.064489   \n",
       "64251           -0.488360          179.769656          39.064489   \n",
       "76287                 NaN          179.769656          39.064489   \n",
       "56573                 NaN          179.769656          39.064489   \n",
       "31235           -0.653463          179.769656          39.064489   \n",
       "42073                 NaN          179.769656          39.064489   \n",
       "...                   ...                 ...                ...   \n",
       "41504            0.374758          179.769656          39.064489   \n",
       "52489                 NaN          179.769656          39.064489   \n",
       "68755            0.210698          179.769656          39.064489   \n",
       "49811                 NaN          179.769656          39.064489   \n",
       "797                   NaN          179.769656          39.064489   \n",
       "31921                 NaN          179.769656          39.064489   \n",
       "24675                 NaN          179.769656          39.064489   \n",
       "47954                 NaN          179.769656          39.064489   \n",
       "2496                  NaN          179.769656          39.064489   \n",
       "73135                 NaN          179.769656          39.064489   \n",
       "1871                  NaN          179.769656          39.064489   \n",
       "7877            -0.394362          179.769656          39.064489   \n",
       "37619                 NaN          179.769656          39.064489   \n",
       "70608                 NaN          179.769656          39.064489   \n",
       "67699                 NaN          179.769656          39.064489   \n",
       "38804           -0.440158          179.769656          39.064489   \n",
       "6921                  NaN          179.769656          39.064489   \n",
       "18983                 NaN          179.769656          39.064489   \n",
       "32230            0.936851          179.769656          39.064489   \n",
       "17089           -2.080005          179.769656          39.064489   \n",
       "52620                 NaN          179.769656          39.064489   \n",
       "39512                 NaN          179.769656          39.064489   \n",
       "48600            1.562211          179.769656          39.064489   \n",
       "55026                 NaN          179.769656          39.064489   \n",
       "41993                 NaN          179.769656          39.064489   \n",
       "21243           -3.095739          179.769656          39.064489   \n",
       "45891                 NaN          179.769656          39.064489   \n",
       "42613            0.021731          179.769656          39.064489   \n",
       "43567                 NaN          179.769656          39.064489   \n",
       "68268            1.535577          179.769656          39.064489   \n",
       "\n",
       "      norm_CH4_v/v_100_bar mean_CH4_v/v_100_bar std_CH4_v/v_100_bar  \\\n",
       "69176            -0.354921           204.360203           42.586407   \n",
       "21102            -0.810894           204.360203           42.586407   \n",
       "46293            -0.016301           204.360203           42.586407   \n",
       "18090             0.465522           204.360203           42.586407   \n",
       "13754             0.887350           204.360203           42.586407   \n",
       "39495             1.090932           204.360203           42.586407   \n",
       "20161            -0.038009           204.360203           42.586407   \n",
       "18876            -0.556891           204.360203           42.586407   \n",
       "12776             0.441928           204.360203           42.586407   \n",
       "65757             0.555553           204.360203           42.586407   \n",
       "14868             0.126752           204.360203           42.586407   \n",
       "7449              0.292017           204.360203           42.586407   \n",
       "2533              0.375926           204.360203           42.586407   \n",
       "36442             0.457002           204.360203           42.586407   \n",
       "62958            -0.704145           204.360203           42.586407   \n",
       "46722             0.372548           204.360203           42.586407   \n",
       "2770              0.172103           204.360203           42.586407   \n",
       "63998             0.162555           204.360203           42.586407   \n",
       "65587             0.394484           204.360203           42.586407   \n",
       "59592             1.039952           204.360203           42.586407   \n",
       "47742             0.066911           204.360203           42.586407   \n",
       "60628             0.167162           204.360203           42.586407   \n",
       "75522            -2.133011           204.360203           42.586407   \n",
       "49203             1.481760           204.360203           42.586407   \n",
       "36370             1.445445           204.360203           42.586407   \n",
       "64251            -0.827030           204.360203           42.586407   \n",
       "76287            -0.016299           204.360203           42.586407   \n",
       "56573             0.626222           204.360203           42.586407   \n",
       "31235            -0.707547           204.360203           42.586407   \n",
       "42073            -0.418621           204.360203           42.586407   \n",
       "...                    ...                  ...                 ...   \n",
       "41504             0.643970           204.360203           42.586407   \n",
       "52489             0.247676           204.360203           42.586407   \n",
       "68755             0.552423           204.360203           42.586407   \n",
       "49811             0.965705           204.360203           42.586407   \n",
       "797              -0.481962           204.360203           42.586407   \n",
       "31921            -0.143828           204.360203           42.586407   \n",
       "24675             0.535856           204.360203           42.586407   \n",
       "47954            -0.319036           204.360203           42.586407   \n",
       "2496              0.227695           204.360203           42.586407   \n",
       "73135            -0.202383           204.360203           42.586407   \n",
       "1871              0.816438           204.360203           42.586407   \n",
       "7877             -0.388359           204.360203           42.586407   \n",
       "37619             0.784153           204.360203           42.586407   \n",
       "70608             0.186952           204.360203           42.586407   \n",
       "67699             0.365916           204.360203           42.586407   \n",
       "38804             0.114671           204.360203           42.586407   \n",
       "6921             -1.646670           204.360203           42.586407   \n",
       "18983             0.940483           204.360203           42.586407   \n",
       "32230             0.558683           204.360203           42.586407   \n",
       "17089            -2.395274           204.360203           42.586407   \n",
       "52620            -0.844760           204.360203           42.586407   \n",
       "39512             0.459399           204.360203           42.586407   \n",
       "48600             1.364967           204.360203           42.586407   \n",
       "55026             0.105969           204.360203           42.586407   \n",
       "41993            -0.346433           204.360203           42.586407   \n",
       "21243            -3.379274           204.360203           42.586407   \n",
       "45891             0.559909           204.360203           42.586407   \n",
       "42613            -0.128717           204.360203           42.586407   \n",
       "43567             0.564215           204.360203           42.586407   \n",
       "68268             1.563105           204.360203           42.586407   \n",
       "\n",
       "      norm_CH4_v/v_248_bar mean_CH4_v/v_248_bar std_CH4_v/v_248_bar  \n",
       "69176                  NaN           247.593271           56.068422  \n",
       "21102            -1.173564           247.593271           56.068422  \n",
       "46293                  NaN           247.593271           56.068422  \n",
       "18090             0.270543           247.593271           56.068422  \n",
       "13754             0.592185           247.593271           56.068422  \n",
       "39495             1.049613           247.593271           56.068422  \n",
       "20161                  NaN           247.593271           56.068422  \n",
       "18876             0.536843           247.593271           56.068422  \n",
       "12776                  NaN           247.593271           56.068422  \n",
       "65757                  NaN           247.593271           56.068422  \n",
       "14868                  NaN           247.593271           56.068422  \n",
       "7449                   NaN           247.593271           56.068422  \n",
       "2533              0.365204           247.593271           56.068422  \n",
       "36442                  NaN           247.593271           56.068422  \n",
       "62958            -0.847480           247.593271           56.068422  \n",
       "46722                  NaN           247.593271           56.068422  \n",
       "2770             -0.304832           247.593271           56.068422  \n",
       "63998             0.803649           247.593271           56.068422  \n",
       "65587                  NaN           247.593271           56.068422  \n",
       "59592                  NaN           247.593271           56.068422  \n",
       "47742                  NaN           247.593271           56.068422  \n",
       "60628             0.383556           247.593271           56.068422  \n",
       "75522                  NaN           247.593271           56.068422  \n",
       "49203                  NaN           247.593271           56.068422  \n",
       "36370             1.157183           247.593271           56.068422  \n",
       "64251            -1.037695           247.593271           56.068422  \n",
       "76287                  NaN           247.593271           56.068422  \n",
       "56573                  NaN           247.593271           56.068422  \n",
       "31235            -1.201540           247.593271           56.068422  \n",
       "42073                  NaN           247.593271           56.068422  \n",
       "...                    ...                  ...                 ...  \n",
       "41504             0.839435           247.593271           56.068422  \n",
       "52489                  NaN           247.593271           56.068422  \n",
       "68755             1.024328           247.593271           56.068422  \n",
       "49811                  NaN           247.593271           56.068422  \n",
       "797                    NaN           247.593271           56.068422  \n",
       "31921                  NaN           247.593271           56.068422  \n",
       "24675                  NaN           247.593271           56.068422  \n",
       "47954                  NaN           247.593271           56.068422  \n",
       "2496                   NaN           247.593271           56.068422  \n",
       "73135            -0.490113           247.593271           56.068422  \n",
       "1871                   NaN           247.593271           56.068422  \n",
       "7877             -0.326295           247.593271           56.068422  \n",
       "37619             0.470121           247.593271           56.068422  \n",
       "70608                  NaN           247.593271           56.068422  \n",
       "67699            -0.099079           247.593271           56.068422  \n",
       "38804             0.651970           247.593271           56.068422  \n",
       "6921                   NaN           247.593271           56.068422  \n",
       "18983                  NaN           247.593271           56.068422  \n",
       "32230             0.207872           247.593271           56.068422  \n",
       "17089            -2.519903           247.593271           56.068422  \n",
       "52620                  NaN           247.593271           56.068422  \n",
       "39512                  NaN           247.593271           56.068422  \n",
       "48600             0.891679           247.593271           56.068422  \n",
       "55026                  NaN           247.593271           56.068422  \n",
       "41993                  NaN           247.593271           56.068422  \n",
       "21243            -3.107787           247.593271           56.068422  \n",
       "45891                  NaN           247.593271           56.068422  \n",
       "42613            -0.232652           247.593271           56.068422  \n",
       "43567                  NaN           247.593271           56.068422  \n",
       "68268             1.274043           247.593271           56.068422  \n",
       "\n",
       "[76382 rows x 469 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)\n",
    "fp_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_space(x, y, n, force_int=False):\n",
    "    step = (y - x) / (n - 1)\n",
    "    if force_int:\n",
    "        return [int(x + step * i) for i in range(n)]\n",
    "    return [x + step * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define default params\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":2, \"n_unit\":10, \"activation\":'relu', \"loss\":'mse', \n",
    "            \"opt\":'adam', \"val_pct\":.2} #patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial grid\n",
    "init_grid = {\"patience\":eq_space(20, 1000, 5, True), \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":[.3, .5, 5]}\n",
    "#patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_used = 'norm_CH4_v/v_1_bar' #column name of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10 #num. steps to take w/o improvement before quitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pct = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(ml_data, total_frac, start_str, end_str, training_pct, batch_size=1, norm=True):\n",
    "    '''\n",
    "    get normalized training and test data\n",
    "    '''\n",
    "    fp_dat = ml_data.sample(frac=total_frac, random_state=0)\n",
    "    train_dataset = fp_dat.sample(frac=training_pct,random_state=2)\n",
    "    test_dataset = fp_dat.drop(train_dataset.index)\n",
    "    train_label = train_dataset[property_used]\n",
    "    test_label = test_dataset[property_used]\n",
    "    for ind, col in enumerate(ml_data.columns):\n",
    "        if start_str in col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "\n",
    "\n",
    "    features = list(ml_data.columns[start_col:end_col])\n",
    "    other_props = ['norm_Dom._Pore_(ang.)',\n",
    "     'norm_Max._Pore_(ang.)',\n",
    "     'norm_Void_Fraction',\n",
    "     'norm_Surf._Area_(m2/g)',\n",
    "     'norm_Vol._Surf._Area',\n",
    "     'norm_Density',\n",
    "      'norm_valence_pa',\n",
    "       'norm_atomic_rad_pa_(angstroms)',\n",
    "         'norm_affinity_pa_(eV)',\n",
    "           'norm_ionization_potential_pa_(eV)',\n",
    "               'norm_electronegativity_pa']\n",
    "\n",
    "    features = features + other_props\n",
    "\n",
    "    train_fp = train_dataset[features]\n",
    "    test_fp = test_dataset[features]\n",
    "    \n",
    "    if norm:\n",
    "        # Summary of training ( and test)\n",
    "        train_stats = train_fp.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "\n",
    "        test_stats = test_fp.describe()\n",
    "        test_stats = test_stats.transpose()\n",
    "        ######################################\n",
    "        \n",
    "        # Remove features with 0 std\n",
    "        my_set ={}\n",
    "        my_set.update(train_stats['std'][train_stats['std'] == 0])\n",
    "        my_set.update(test_stats['std'][test_stats['std'] == 0])\n",
    "\n",
    "\n",
    "        train_fp1 = train_fp.drop(my_set.keys(), axis=1)\n",
    "\n",
    "        test_fp1 = test_fp.drop(my_set.keys(), axis=1)\n",
    "        ###################################################\n",
    "        \n",
    "        # Normalization (check it this is required. Try without first)\n",
    "        def norm(x):\n",
    "            stats = x.describe()\n",
    "            stats = stats.transpose()\n",
    "            return (x - stats['mean']) / stats['std']\n",
    "\n",
    "        normed_train_fp = norm(train_fp1)\n",
    "        normed_test_fp = norm(test_fp1)\n",
    "        train_fp = normed_train_fp\n",
    "        test_fp = normed_test_fp\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_fp.to_numpy().astype(np.float32), \n",
    "                                                     train_label.to_numpy().astype(np.float32))).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_fp.to_numpy().astype(np.float32), \n",
    "                                                    test_label.to_numpy().astype(np.float32))).batch(batch_size)\n",
    "\n",
    "    return train_data, test_data\n",
    "    #model.fit(train_data, validation_data=train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def buildModel1(train_data, n_layer, n_unit, activation, loss, opt):\n",
    "    \n",
    "    #n_col = train_fp.shape[1]\n",
    "    n_col = train_data.element_spec[0].shape[1]\n",
    "    print(n_col)\n",
    "    model_guts = [layers.Dense(n_unit, activation=activation, \n",
    "                               input_shape=[n_col])] + [layers.Dense(n_unit, \n",
    "                                activation=activation) for i in range(n_layer - 1)] + [layers.Dense(1, activation='linear')]\n",
    "    model = keras.Sequential(model_guts)\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "        optimizer=opt,\n",
    "        metrics=['mae', 'mse'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, patience, val_pct, train_data):\n",
    "    EPOCHS = 2000\n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                          verbose=1, save_best_only=True, mode='min')\n",
    "    # early_history = model.fit(normed_train_data, train_label.to_numpy(), \n",
    "    #                     epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop,checkpoint_callbacks])\n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    early_history = model.fit(train_data,\n",
    "                        epochs=EPOCHS, validation_data = train_data, verbose=1,\\\n",
    "                              callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct, batch_size=10, norm=True):\n",
    "    '''\n",
    "    This function creates a model and returns its mse\n",
    "    '''\n",
    "    print(\"Patience: \", patience)\n",
    "    print(\"training_pct: \", training_pct)\n",
    "    print(\"n_layer: \", n_layer)\n",
    "    print(\"n_unit: \", n_unit)\n",
    "    print(\"activation: \", activation)\n",
    "    print(\"loss: \", loss)\n",
    "    print(\"opt: \", opt)\n",
    "    print(\"val_pct: \", val_pct)\n",
    "    train_data, test_data = getData(ml_data, total_frac, start_str, \n",
    "                                                                           end_str, \n",
    "                                    training_pct, batch_size=batch_size, norm=norm) #get normalized training and test data\n",
    "    model = buildModel1(train_data, n_layer, n_unit, activation, loss, opt)\n",
    "    \n",
    "    \n",
    "    trainModel(model, patience, val_pct, train_data)\n",
    "    \n",
    "    loss, mae, mse = model.evaluate(test_data, verbose=2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varyParams(ml_data, default_params, grid, total_frac, start_str, end_str):\n",
    "    par_d = {}\n",
    "    for grid_k,grid_v in zip(grid.keys(), grid.values()):\n",
    "        for val in grid_v:\n",
    "            for def_k,def_v in zip(default_params.keys(), default_params.values()):\n",
    "                exec(def_k+\"=def_v\")\n",
    "                exec(\"global \" + def_k)\n",
    "            \n",
    "            exec(grid_k+\"=val\")\n",
    "            exec(\"global \" + grid_k)\n",
    "            l = list(default_params.keys())\n",
    "            mse = eval('evaluate_model(ml_data, total_frac, start_str, end_str, ' + l[0] + ',' + l[1] + ',' + l[2] + ',' + l[3] + ',' + l[4] + ',' + l[5] + ',' + l[6] + ',' + l[7] + ')')\n",
    "            par_d[val] = mse\n",
    "\n",
    "    #r = eval('[' + l[0] + '_d' + ', ' + l[1] + '_d' + ', ' + l[2] + '_d' + ', ' + l[3] + '_d' + ', ' + l[4] + '_d' + ', ' + l[5] + '_d' + ', ' + l[6] + '_d' + ', ' + l[7] + '_d' + ']')\n",
    "    return par_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fp_dat.sample(frac=training_pct,random_state=2)\n",
    "test_dataset = fp_dat.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_dataset[property_used]\n",
    "test_label = test_dataset[property_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, col in enumerate(ml_data.columns):\n",
    "    if start_str in col:\n",
    "        start_col = ind + 1\n",
    "    elif end_str == col:\n",
    "        end_col = ind\n",
    "\n",
    "\n",
    "features = list(ml_data.columns[start_col:end_col])\n",
    "other_props = ['norm_Dom._Pore_(ang.)',\n",
    " 'norm_Max._Pore_(ang.)',\n",
    " 'norm_Void_Fraction',\n",
    " 'norm_Surf._Area_(m2/g)',\n",
    " 'norm_Vol._Surf._Area',\n",
    " 'norm_Density',\n",
    "  'norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "\n",
    "features = features + other_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = train_dataset[features]\n",
    "test_fp = test_dataset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishi/.virtualenvs/efrc-p3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/rishi/.virtualenvs/efrc-p3/lib/python3.6/site-packages/ipykernel_launcher.py:5: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'norm_CH4_v/v_1_bar.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ad361e43decb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.png'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mproperty_used\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2083\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'norm_CH4_v/v_1_bar.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of test and training sets\n",
    "fig,ax = plt.subplots(figsize = (8,5))\n",
    "n_bins=30\n",
    "n, bins, patches = plt.hist(train_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='#FDA65F', alpha=1,label = 'Training set')\n",
    "n, bins, patches = plt.hist(test_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='green', alpha=1, label = 'Test set')\n",
    "plt.xlabel('y_val',fontsize=labelfontsize)\n",
    "plt.ylabel('Count',fontsize=labelfontsize)\n",
    "#ax.set_xlim(2,12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig('%s.png'%property_used,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C1</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.266536e-07</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C2</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.168371e-06</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C3</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.409526e-05</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C3_Br1</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.574442e-08</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C3_C1</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.367559e-06</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_valence_pa</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.231356e-03</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>-1.379504</td>\n",
       "      <td>-0.918210</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_atomic_rad_pa_(angstroms)</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.866145e-04</td>\n",
       "      <td>1.000712</td>\n",
       "      <td>-2.006160</td>\n",
       "      <td>-0.842487</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>1.009197</td>\n",
       "      <td>1.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_affinity_pa_(eV)</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.319463e-04</td>\n",
       "      <td>1.000480</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-0.566297</td>\n",
       "      <td>1.290621</td>\n",
       "      <td>1.290621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_ionization_potential_pa_(eV)</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>-6.576941e-04</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.882982</td>\n",
       "      <td>0.882982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_electronegativity_pa</td>\n",
       "      <td>68744.0</td>\n",
       "      <td>-1.999659e-04</td>\n",
       "      <td>1.001651</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-0.193532</td>\n",
       "      <td>0.231429</td>\n",
       "      <td>2.305082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     count          mean       std       min  \\\n",
       "Mafp_Br1_C2_C1                     68744.0  6.266536e-07  0.000120  0.000000   \n",
       "Mafp_Br1_C2_C2                     68744.0  7.168371e-06  0.000603  0.000000   \n",
       "Mafp_Br1_C2_C3                     68744.0  1.409526e-05  0.000911  0.000000   \n",
       "Mafp_Br1_C3_Br1                    68744.0  4.574442e-08  0.000012  0.000000   \n",
       "Mafp_Br1_C3_C1                     68744.0  7.367559e-06  0.000676  0.000000   \n",
       "...                                    ...           ...       ...       ...   \n",
       "norm_valence_pa                    68744.0  1.231356e-03  1.000226 -1.379504   \n",
       "norm_atomic_rad_pa_(angstroms)     68744.0  6.866145e-04  1.000712 -2.006160   \n",
       "norm_affinity_pa_(eV)              68744.0  2.319463e-04  1.000480 -1.103888   \n",
       "norm_ionization_potential_pa_(eV)  68744.0 -6.576941e-04  0.999688 -1.561358   \n",
       "norm_electronegativity_pa          68744.0 -1.999659e-04  1.001651 -1.177238   \n",
       "\n",
       "                                        25%       50%       75%       max  \n",
       "Mafp_Br1_C2_C1                     0.000000  0.000000  0.000000  0.026917  \n",
       "Mafp_Br1_C2_C2                     0.000000  0.000000  0.000000  0.100000  \n",
       "Mafp_Br1_C2_C3                     0.000000  0.000000  0.000000  0.125000  \n",
       "Mafp_Br1_C3_Br1                    0.000000  0.000000  0.000000  0.003145  \n",
       "Mafp_Br1_C3_C1                     0.000000  0.000000  0.000000  0.142857  \n",
       "...                                     ...       ...       ...       ...  \n",
       "norm_valence_pa                   -0.918210  0.926967  0.926967  0.926967  \n",
       "norm_atomic_rad_pa_(angstroms)    -0.842487  0.754378  1.009197  1.009197  \n",
       "norm_affinity_pa_(eV)             -1.103888 -0.566297  1.290621  1.290621  \n",
       "norm_ionization_potential_pa_(eV) -1.561358  0.057798  0.882982  0.882982  \n",
       "norm_electronegativity_pa         -1.177238 -0.193532  0.231429  2.305082  \n",
       "\n",
       "[385 rows x 8 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of training ( and test)\n",
    "train_stats = train_fp.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C1</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C2</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C2_C3</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C3_Br1</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mafp_Br1_C3_C1</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_valence_pa</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>-0.011083</td>\n",
       "      <td>0.997964</td>\n",
       "      <td>-1.379504</td>\n",
       "      <td>-0.918210</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_atomic_rad_pa_(angstroms)</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>0.993608</td>\n",
       "      <td>-2.006160</td>\n",
       "      <td>-0.842487</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>1.009197</td>\n",
       "      <td>1.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_affinity_pa_(eV)</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>-0.002088</td>\n",
       "      <td>0.995729</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-0.566297</td>\n",
       "      <td>1.290621</td>\n",
       "      <td>1.290621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_ionization_potential_pa_(eV)</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>1.002849</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.882982</td>\n",
       "      <td>0.882982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>norm_electronegativity_pa</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-0.193532</td>\n",
       "      <td>0.231429</td>\n",
       "      <td>2.305082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    count      mean       std       min  \\\n",
       "Mafp_Br1_C2_C1                     7638.0  0.000000  0.000000  0.000000   \n",
       "Mafp_Br1_C2_C2                     7638.0  0.000013  0.001144  0.000000   \n",
       "Mafp_Br1_C2_C3                     7638.0  0.000002  0.000218  0.000000   \n",
       "Mafp_Br1_C3_Br1                    7638.0  0.000000  0.000000  0.000000   \n",
       "Mafp_Br1_C3_C1                     7638.0  0.000001  0.000106  0.000000   \n",
       "...                                   ...       ...       ...       ...   \n",
       "norm_valence_pa                    7638.0 -0.011083  0.997964 -1.379504   \n",
       "norm_atomic_rad_pa_(angstroms)     7638.0 -0.006180  0.993608 -2.006160   \n",
       "norm_affinity_pa_(eV)              7638.0 -0.002088  0.995729 -1.103888   \n",
       "norm_ionization_potential_pa_(eV)  7638.0  0.005919  1.002849 -1.561358   \n",
       "norm_electronegativity_pa          7638.0  0.001800  0.985079 -1.177238   \n",
       "\n",
       "                                        25%       50%       75%       max  \n",
       "Mafp_Br1_C2_C1                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C2_C2                     0.000000  0.000000  0.000000  0.100000  \n",
       "Mafp_Br1_C2_C3                     0.000000  0.000000  0.000000  0.019048  \n",
       "Mafp_Br1_C3_Br1                    0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C3_C1                     0.000000  0.000000  0.000000  0.009259  \n",
       "...                                     ...       ...       ...       ...  \n",
       "norm_valence_pa                   -0.918210  0.926967  0.926967  0.926967  \n",
       "norm_atomic_rad_pa_(angstroms)    -0.842487  0.754378  1.009197  1.009197  \n",
       "norm_affinity_pa_(eV)             -1.103888 -0.566297  1.290621  1.290621  \n",
       "norm_ionization_potential_pa_(eV) -1.561358  0.057798  0.882982  0.882982  \n",
       "norm_electronegativity_pa         -1.177238 -0.193532  0.231429  2.305082  \n",
       "\n",
       "[385 rows x 8 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats = test_fp.describe()\n",
    "test_stats = test_stats.transpose()\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mafp_C1_N2_N3    0.0\n",
       "Mafp_N2_O2_N3    0.0\n",
       "Mafp_N3_N2_O2    0.0\n",
       "Mmfp_MQNs22      0.0\n",
       "Mmfp_MQNs23      0.0\n",
       "Mmfp_MQNs24      0.0\n",
       "Mmfp_MQNs25      0.0\n",
       "Name: std, dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats['std'][train_stats['std'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Mafp_Br1_C2_C1', 'Mafp_Br1_C3_Br1', 'Mafp_Br1_C3_N3', 'Mafp_Br1_C4_N2',\n",
       "       'Mafp_Br1_C4_O2', 'Mafp_Br1_N3_C3', 'Mafp_Br1_N3_N2', 'Mafp_Br1_O2_C2',\n",
       "       'Mafp_Br1_O2_C4', 'Mafp_C1_C2_C4', 'Mafp_C1_C2_F1', 'Mafp_C1_C2_H1',\n",
       "       'Mafp_C1_C2_O1', 'Mafp_C1_C2_O2', 'Mafp_C1_C3_O1', 'Mafp_C1_C4_C2',\n",
       "       'Mafp_C1_C4_C3', 'Mafp_C1_C4_O1', 'Mafp_C1_C4_O2', 'Mafp_C1_N2_C3',\n",
       "       'Mafp_C1_N2_N3', 'Mafp_C1_N3_C4', 'Mafp_C1_N3_N2', 'Mafp_C1_N3_N3',\n",
       "       'Mafp_C1_N3_O1', 'Mafp_C2_C3_N1', 'Mafp_C2_N2_O1', 'Mafp_C2_N3_O1',\n",
       "       'Mafp_C2_O2_C2', 'Mafp_C2_O2_N3', 'Mafp_C3_N3_N1', 'Mafp_C4_C2_O1',\n",
       "       'Mafp_C4_N2_C4', 'Mafp_C4_N2_H1', 'Mafp_C4_N2_O1', 'Mafp_C4_N3_F1',\n",
       "       'Mafp_C4_N3_O1', 'Mafp_Cl1_C3_Cl1', 'Mafp_Cl1_C3_H1', 'Mafp_Cl1_C3_O1',\n",
       "       'Mafp_Cl1_C4_N2', 'Mafp_Cl1_N2_N2', 'Mafp_F1_C3_F1', 'Mafp_F1_C3_O1',\n",
       "       'Mafp_F1_C4_O1', 'Mafp_F1_N3_N3', 'Mafp_H1_C2_N2', 'Mafp_H1_C2_N3',\n",
       "       'Mafp_H1_C2_O1', 'Mafp_H1_O2_O1', 'Mafp_N1_C2_N3', 'Mafp_N1_C3_O1',\n",
       "       'Mafp_N1_C4_N2', 'Mafp_N1_C4_O2', 'Mafp_N1_N3_N2', 'Mafp_N2_C2_N2',\n",
       "       'Mafp_N2_C2_O2', 'Mafp_N2_N3_N2', 'Mafp_N2_O2_N3', 'Mafp_N3_C2_O1',\n",
       "       'Mafp_N3_N2_N3', 'Mafp_N3_N2_O1', 'Mafp_N3_N3_O1', 'Mafp_N3_O2_N3',\n",
       "       'Mafp_O1_N3_O2', 'Mafp_O2_N3_O2', 'Mefp_fam_ketone', 'Mmfp_MQNs22',\n",
       "       'Mmfp_MQNs23', 'Mmfp_MQNs24', 'Mmfp_MQNs25'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats['std'][test_stats['std'] == 0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set ={}\n",
    "my_set.update(train_stats['std'][train_stats['std'] == 0])\n",
    "my_set.update(test_stats['std'][test_stats['std'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Mafp_C1_N2_N3', 'Mafp_N2_O2_N3', 'Mafp_N3_N2_O2', 'Mmfp_MQNs22', 'Mmfp_MQNs23', 'Mmfp_MQNs24', 'Mmfp_MQNs25', 'Mafp_Br1_C2_C1', 'Mafp_Br1_C3_Br1', 'Mafp_Br1_C3_N3', 'Mafp_Br1_C4_N2', 'Mafp_Br1_C4_O2', 'Mafp_Br1_N3_C3', 'Mafp_Br1_N3_N2', 'Mafp_Br1_O2_C2', 'Mafp_Br1_O2_C4', 'Mafp_C1_C2_C4', 'Mafp_C1_C2_F1', 'Mafp_C1_C2_H1', 'Mafp_C1_C2_O1', 'Mafp_C1_C2_O2', 'Mafp_C1_C3_O1', 'Mafp_C1_C4_C2', 'Mafp_C1_C4_C3', 'Mafp_C1_C4_O1', 'Mafp_C1_C4_O2', 'Mafp_C1_N2_C3', 'Mafp_C1_N3_C4', 'Mafp_C1_N3_N2', 'Mafp_C1_N3_N3', 'Mafp_C1_N3_O1', 'Mafp_C2_C3_N1', 'Mafp_C2_N2_O1', 'Mafp_C2_N3_O1', 'Mafp_C2_O2_C2', 'Mafp_C2_O2_N3', 'Mafp_C3_N3_N1', 'Mafp_C4_C2_O1', 'Mafp_C4_N2_C4', 'Mafp_C4_N2_H1', 'Mafp_C4_N2_O1', 'Mafp_C4_N3_F1', 'Mafp_C4_N3_O1', 'Mafp_Cl1_C3_Cl1', 'Mafp_Cl1_C3_H1', 'Mafp_Cl1_C3_O1', 'Mafp_Cl1_C4_N2', 'Mafp_Cl1_N2_N2', 'Mafp_F1_C3_F1', 'Mafp_F1_C3_O1', 'Mafp_F1_C4_O1', 'Mafp_F1_N3_N3', 'Mafp_H1_C2_N2', 'Mafp_H1_C2_N3', 'Mafp_H1_C2_O1', 'Mafp_H1_O2_O1', 'Mafp_N1_C2_N3', 'Mafp_N1_C3_O1', 'Mafp_N1_C4_N2', 'Mafp_N1_C4_O2', 'Mafp_N1_N3_N2', 'Mafp_N2_C2_N2', 'Mafp_N2_C2_O2', 'Mafp_N2_N3_N2', 'Mafp_N3_C2_O1', 'Mafp_N3_N2_N3', 'Mafp_N3_N2_O1', 'Mafp_N3_N3_O1', 'Mafp_N3_O2_N3', 'Mafp_O1_N3_O2', 'Mafp_O2_N3_O2', 'Mefp_fam_ketone'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp1 = train_fp.drop(my_set.keys(), axis=1)\n",
    "#test_fp1 = test_fp.drop(train_stats['std'][train_stats['std'] == 0].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp1 = test_fp.drop(my_set.keys(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (check it this is required. Try without first)\n",
    "def norm(x):\n",
    "    stats = x.describe()\n",
    "    stats = stats.transpose()\n",
    "    return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data = norm(train_fp1)\n",
    "normed_test_data = norm(test_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mafp_Br1_C2_C2</th>\n",
       "      <th>Mafp_Br1_C2_C3</th>\n",
       "      <th>Mafp_Br1_C3_C1</th>\n",
       "      <th>Mafp_Br1_C3_C2</th>\n",
       "      <th>Mafp_Br1_C3_C3</th>\n",
       "      <th>Mafp_Br1_C3_C4</th>\n",
       "      <th>Mafp_Br1_C3_N1</th>\n",
       "      <th>Mafp_Br1_C3_N2</th>\n",
       "      <th>Mafp_Br1_C3_O1</th>\n",
       "      <th>Mafp_Br1_C4_Br1</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_Max._Pore_(ang.)</th>\n",
       "      <th>norm_Void_Fraction</th>\n",
       "      <th>norm_Surf._Area_(m2/g)</th>\n",
       "      <th>norm_Vol._Surf._Area</th>\n",
       "      <th>norm_Density</th>\n",
       "      <th>norm_valence_pa</th>\n",
       "      <th>norm_atomic_rad_pa_(angstroms)</th>\n",
       "      <th>norm_affinity_pa_(eV)</th>\n",
       "      <th>norm_ionization_potential_pa_(eV)</th>\n",
       "      <th>norm_electronegativity_pa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2533</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.131789</td>\n",
       "      <td>-0.269948</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.019821</td>\n",
       "      <td>-0.130862</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182468</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>0.036961</td>\n",
       "      <td>0.771642</td>\n",
       "      <td>-0.135330</td>\n",
       "      <td>-0.908979</td>\n",
       "      <td>-0.841687</td>\n",
       "      <td>-0.566630</td>\n",
       "      <td>0.874571</td>\n",
       "      <td>0.233108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57166</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.131789</td>\n",
       "      <td>-0.269948</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.019821</td>\n",
       "      <td>-0.130862</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.746120</td>\n",
       "      <td>0.153708</td>\n",
       "      <td>-0.333752</td>\n",
       "      <td>0.539013</td>\n",
       "      <td>0.142517</td>\n",
       "      <td>0.939964</td>\n",
       "      <td>0.765450</td>\n",
       "      <td>-1.106526</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>-1.196897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46381</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.131789</td>\n",
       "      <td>-0.269948</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.019821</td>\n",
       "      <td>-0.130862</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.309773</td>\n",
       "      <td>-3.386881</td>\n",
       "      <td>-2.005931</td>\n",
       "      <td>-3.051263</td>\n",
       "      <td>4.165331</td>\n",
       "      <td>-1.371214</td>\n",
       "      <td>-0.516840</td>\n",
       "      <td>0.919723</td>\n",
       "      <td>0.766610</td>\n",
       "      <td>1.739006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10963</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.131789</td>\n",
       "      <td>-0.269948</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.019821</td>\n",
       "      <td>-0.130862</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719377</td>\n",
       "      <td>0.604310</td>\n",
       "      <td>0.573255</td>\n",
       "      <td>0.205722</td>\n",
       "      <td>-0.714765</td>\n",
       "      <td>0.939964</td>\n",
       "      <td>0.765450</td>\n",
       "      <td>-1.106526</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>-1.196897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48347</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.131789</td>\n",
       "      <td>-0.269948</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.019821</td>\n",
       "      <td>-0.130862</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>...</td>\n",
       "      <td>1.733951</td>\n",
       "      <td>0.479461</td>\n",
       "      <td>0.126681</td>\n",
       "      <td>0.012718</td>\n",
       "      <td>-0.530498</td>\n",
       "      <td>-1.371214</td>\n",
       "      <td>-0.516840</td>\n",
       "      <td>0.919723</td>\n",
       "      <td>0.766610</td>\n",
       "      <td>1.739006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mafp_Br1_C2_C2  Mafp_Br1_C2_C3  Mafp_Br1_C3_C1  Mafp_Br1_C3_C2  \\\n",
       "2533        -0.011442       -0.011442       -0.011442       -0.131789   \n",
       "57166       -0.011442       -0.011442       -0.011442       -0.131789   \n",
       "46381       -0.011442       -0.011442       -0.011442       -0.131789   \n",
       "10963       -0.011442       -0.011442       -0.011442       -0.131789   \n",
       "48347       -0.011442       -0.011442       -0.011442       -0.131789   \n",
       "\n",
       "       Mafp_Br1_C3_C3  Mafp_Br1_C3_C4  Mafp_Br1_C3_N1  Mafp_Br1_C3_N2  \\\n",
       "2533        -0.269948       -0.048896       -0.019821       -0.130862   \n",
       "57166       -0.269948       -0.048896       -0.019821       -0.130862   \n",
       "46381       -0.269948       -0.048896       -0.019821       -0.130862   \n",
       "10963       -0.269948       -0.048896       -0.019821       -0.130862   \n",
       "48347       -0.269948       -0.048896       -0.019821       -0.130862   \n",
       "\n",
       "       Mafp_Br1_C3_O1  Mafp_Br1_C4_Br1  ...  norm_Max._Pore_(ang.)  \\\n",
       "2533        -0.016174        -0.084605  ...              -0.182468   \n",
       "57166       -0.016174        -0.084605  ...              -0.746120   \n",
       "46381       -0.016174        -0.084605  ...              -1.309773   \n",
       "10963       -0.016174        -0.084605  ...               0.719377   \n",
       "48347       -0.016174        -0.084605  ...               1.733951   \n",
       "\n",
       "       norm_Void_Fraction  norm_Surf._Area_(m2/g)  norm_Vol._Surf._Area  \\\n",
       "2533             0.032779                0.036961              0.771642   \n",
       "57166            0.153708               -0.333752              0.539013   \n",
       "46381           -3.386881               -2.005931             -3.051263   \n",
       "10963            0.604310                0.573255              0.205722   \n",
       "48347            0.479461                0.126681              0.012718   \n",
       "\n",
       "       norm_Density  norm_valence_pa  norm_atomic_rad_pa_(angstroms)  \\\n",
       "2533      -0.135330        -0.908979                       -0.841687   \n",
       "57166      0.142517         0.939964                        0.765450   \n",
       "46381      4.165331        -1.371214                       -0.516840   \n",
       "10963     -0.714765         0.939964                        0.765450   \n",
       "48347     -0.530498        -1.371214                       -0.516840   \n",
       "\n",
       "       norm_affinity_pa_(eV)  norm_ionization_potential_pa_(eV)  \\\n",
       "2533               -0.566630                           0.874571   \n",
       "57166              -1.106526                           0.051731   \n",
       "46381               0.919723                           0.766610   \n",
       "10963              -1.106526                           0.051731   \n",
       "48347               0.919723                           0.766610   \n",
       "\n",
       "       norm_electronegativity_pa  \n",
       "2533                    0.233108  \n",
       "57166                  -1.196897  \n",
       "46381                   1.739006  \n",
       "10963                  -1.196897  \n",
       "48347                   1.739006  \n",
       "\n",
       "[5 rows x 313 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mafp_Br1_C2_C2</th>\n",
       "      <th>Mafp_Br1_C2_C3</th>\n",
       "      <th>Mafp_Br1_C3_C1</th>\n",
       "      <th>Mafp_Br1_C3_C2</th>\n",
       "      <th>Mafp_Br1_C3_C3</th>\n",
       "      <th>Mafp_Br1_C3_C4</th>\n",
       "      <th>Mafp_Br1_C3_N1</th>\n",
       "      <th>Mafp_Br1_C3_N2</th>\n",
       "      <th>Mafp_Br1_C3_O1</th>\n",
       "      <th>Mafp_Br1_C4_Br1</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_Max._Pore_(ang.)</th>\n",
       "      <th>norm_Void_Fraction</th>\n",
       "      <th>norm_Surf._Area_(m2/g)</th>\n",
       "      <th>norm_Vol._Surf._Area</th>\n",
       "      <th>norm_Density</th>\n",
       "      <th>norm_valence_pa</th>\n",
       "      <th>norm_atomic_rad_pa_(angstroms)</th>\n",
       "      <th>norm_affinity_pa_(eV)</th>\n",
       "      <th>norm_ionization_potential_pa_(eV)</th>\n",
       "      <th>norm_electronegativity_pa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41814</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>-0.010906</td>\n",
       "      <td>-0.121191</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.12308</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.088075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504187</td>\n",
       "      <td>-0.381349</td>\n",
       "      <td>-0.108947</td>\n",
       "      <td>0.595724</td>\n",
       "      <td>-0.084446</td>\n",
       "      <td>0.925527</td>\n",
       "      <td>0.753155</td>\n",
       "      <td>-1.103590</td>\n",
       "      <td>0.058474</td>\n",
       "      <td>-1.175098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44438</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>-0.010906</td>\n",
       "      <td>-0.121191</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.12308</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.088075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618206</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.018691</td>\n",
       "      <td>1.326974</td>\n",
       "      <td>0.115437</td>\n",
       "      <td>-0.919234</td>\n",
       "      <td>-0.842573</td>\n",
       "      <td>-0.566257</td>\n",
       "      <td>0.883915</td>\n",
       "      <td>0.231247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68236</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>-0.010906</td>\n",
       "      <td>-0.121191</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.12308</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.088075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390167</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.093881</td>\n",
       "      <td>0.926201</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>-0.919234</td>\n",
       "      <td>-0.842573</td>\n",
       "      <td>-0.566257</td>\n",
       "      <td>0.883915</td>\n",
       "      <td>0.231247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52777</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>-0.010906</td>\n",
       "      <td>-0.121191</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.12308</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.088075</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.074283</td>\n",
       "      <td>-0.316489</td>\n",
       "      <td>-0.629692</td>\n",
       "      <td>0.495188</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>-0.919234</td>\n",
       "      <td>-0.842573</td>\n",
       "      <td>-0.566257</td>\n",
       "      <td>0.883915</td>\n",
       "      <td>0.231247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15033</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>-0.010906</td>\n",
       "      <td>-0.121191</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.12308</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.088075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521987</td>\n",
       "      <td>0.701869</td>\n",
       "      <td>1.000800</td>\n",
       "      <td>0.364433</td>\n",
       "      <td>-0.845444</td>\n",
       "      <td>0.925527</td>\n",
       "      <td>1.007792</td>\n",
       "      <td>1.289769</td>\n",
       "      <td>-1.561187</td>\n",
       "      <td>-0.193013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mafp_Br1_C2_C2  Mafp_Br1_C2_C3  Mafp_Br1_C3_C1  Mafp_Br1_C3_C2  \\\n",
       "41814       -0.011888       -0.015473       -0.010906       -0.121191   \n",
       "44438       -0.011888       -0.015473       -0.010906       -0.121191   \n",
       "68236       -0.011888       -0.015473       -0.010906       -0.121191   \n",
       "52777       -0.011888       -0.015473       -0.010906       -0.121191   \n",
       "15033       -0.011888       -0.015473       -0.010906       -0.121191   \n",
       "\n",
       "       Mafp_Br1_C3_C3  Mafp_Br1_C3_C4  Mafp_Br1_C3_N1  Mafp_Br1_C3_N2  \\\n",
       "41814       -0.261907       -0.044134       -0.020211        -0.12308   \n",
       "44438       -0.261907       -0.044134       -0.020211        -0.12308   \n",
       "68236       -0.261907       -0.044134       -0.020211        -0.12308   \n",
       "52777       -0.261907       -0.044134       -0.020211        -0.12308   \n",
       "15033       -0.261907       -0.044134       -0.020211        -0.12308   \n",
       "\n",
       "       Mafp_Br1_C3_O1  Mafp_Br1_C4_Br1  ...  norm_Max._Pore_(ang.)  \\\n",
       "41814       -0.016607        -0.088075  ...              -0.504187   \n",
       "44438       -0.016607        -0.088075  ...              -0.618206   \n",
       "68236       -0.016607        -0.088075  ...              -0.390167   \n",
       "52777       -0.016607        -0.088075  ...              -1.074283   \n",
       "15033       -0.016607        -0.088075  ...               0.521987   \n",
       "\n",
       "       norm_Void_Fraction  norm_Surf._Area_(m2/g)  norm_Vol._Surf._Area  \\\n",
       "41814           -0.381349               -0.108947              0.595724   \n",
       "44438            0.172969                0.018691              1.326974   \n",
       "68236            0.056474               -0.093881              0.926201   \n",
       "52777           -0.316489               -0.629692              0.495188   \n",
       "15033            0.701869                1.000800              0.364433   \n",
       "\n",
       "       norm_Density  norm_valence_pa  norm_atomic_rad_pa_(angstroms)  \\\n",
       "41814     -0.084446         0.925527                        0.753155   \n",
       "44438      0.115437        -0.919234                       -0.842573   \n",
       "68236      0.052444        -0.919234                       -0.842573   \n",
       "52777      0.547000        -0.919234                       -0.842573   \n",
       "15033     -0.845444         0.925527                        1.007792   \n",
       "\n",
       "       norm_affinity_pa_(eV)  norm_ionization_potential_pa_(eV)  \\\n",
       "41814              -1.103590                           0.058474   \n",
       "44438              -0.566257                           0.883915   \n",
       "68236              -0.566257                           0.883915   \n",
       "52777              -0.566257                           0.883915   \n",
       "15033               1.289769                          -1.561187   \n",
       "\n",
       "       norm_electronegativity_pa  \n",
       "41814                  -1.175098  \n",
       "44438                   0.231247  \n",
       "68236                   0.231247  \n",
       "52777                   0.231247  \n",
       "15033                  -0.193013  \n",
       "\n",
       "[5 rows x 313 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(100, activation='relu', input_shape=[len(train_fp.keys())]),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "        optimizer='adam',\n",
    "        metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 100)               38600     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,901\n",
      "Trainable params: 58,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48884 samples, validate on 12222 samples\n",
      "Epoch 1/5000\n",
      "47360/48884 [============================>.] - ETA: 0s - loss: 0.1948 - mae: 0.2142 - mse: 0.1948\n",
      "Epoch 00001: val_loss improved from inf to 0.14850, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.1929,  mae:0.2137,  mse:0.1929,  val_loss:0.1485,  val_mae:0.1824,  val_mse:0.1485,  \n",
      "48884/48884 [==============================] - 2s 44us/sample - loss: 0.1929 - mae: 0.2137 - mse: 0.1929 - val_loss: 0.1485 - val_mae: 0.1824 - val_mse: 0.1485\n",
      "Epoch 2/5000\n",
      "47296/48884 [============================>.] - ETA: 0s - loss: 0.1436 - mae: 0.1733 - mse: 0.1436\n",
      "Epoch 00002: val_loss improved from 0.14850 to 0.13426, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 38us/sample - loss: 0.1444 - mae: 0.1734 - mse: 0.1444 - val_loss: 0.1343 - val_mae: 0.1626 - val_mse: 0.1343\n",
      "Epoch 3/5000\n",
      "48032/48884 [============================>.] - ETA: 0s - loss: 0.1370 - mae: 0.1678 - mse: 0.1370\n",
      "Epoch 00003: val_loss did not improve from 0.13426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1373 - mae: 0.1676 - mse: 0.1373 - val_loss: 0.1352 - val_mae: 0.1722 - val_mse: 0.1352\n",
      "Epoch 4/5000\n",
      "47520/48884 [============================>.] - ETA: 0s - loss: 0.1316 - mae: 0.1601 - mse: 0.1316\n",
      "Epoch 00004: val_loss improved from 0.13426 to 0.11838, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1303 - mae: 0.1599 - mse: 0.1303 - val_loss: 0.1184 - val_mae: 0.1453 - val_mse: 0.1184\n",
      "Epoch 5/5000\n",
      "48800/48884 [============================>.] - ETA: 0s - loss: 0.1254 - mae: 0.1563 - mse: 0.1254\n",
      "Epoch 00005: val_loss did not improve from 0.11838\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1253 - mae: 0.1563 - mse: 0.1253 - val_loss: 0.1212 - val_mae: 0.1497 - val_mse: 0.1212\n",
      "Epoch 6/5000\n",
      "47904/48884 [============================>.] - ETA: 0s - loss: 0.1204 - mae: 0.1527 - mse: 0.1204\n",
      "Epoch 00006: val_loss did not improve from 0.11838\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1226 - mae: 0.1529 - mse: 0.1226 - val_loss: 0.1318 - val_mae: 0.1700 - val_mse: 0.1318\n",
      "Epoch 7/5000\n",
      "47904/48884 [============================>.] - ETA: 0s - loss: 0.1174 - mae: 0.1496 - mse: 0.1174\n",
      "Epoch 00007: val_loss did not improve from 0.11838\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1201 - mae: 0.1504 - mse: 0.1201 - val_loss: 0.1297 - val_mae: 0.1662 - val_mse: 0.1297\n",
      "Epoch 8/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1180 - mae: 0.1483 - mse: 0.1180\n",
      "Epoch 00008: val_loss improved from 0.11838 to 0.11304, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1175 - mae: 0.1481 - mse: 0.1175 - val_loss: 0.1130 - val_mae: 0.1446 - val_mse: 0.1130\n",
      "Epoch 9/5000\n",
      "48096/48884 [============================>.] - ETA: 0s - loss: 0.1174 - mae: 0.1472 - mse: 0.1174\n",
      "Epoch 00009: val_loss did not improve from 0.11304\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1165 - mae: 0.1470 - mse: 0.1165 - val_loss: 0.1150 - val_mae: 0.1468 - val_mse: 0.1150\n",
      "Epoch 10/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1127 - mae: 0.1445 - mse: 0.1127\n",
      "Epoch 00010: val_loss improved from 0.11304 to 0.11078, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1141 - mae: 0.1446 - mse: 0.1141 - val_loss: 0.1108 - val_mae: 0.1472 - val_mse: 0.1108\n",
      "Epoch 11/5000\n",
      "48064/48884 [============================>.] - ETA: 0s - loss: 0.1138 - mae: 0.1436 - mse: 0.1138\n",
      "Epoch 00011: val_loss did not improve from 0.11078\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1129 - mae: 0.1433 - mse: 0.1129 - val_loss: 0.1121 - val_mae: 0.1402 - val_mse: 0.1121\n",
      "Epoch 12/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1115 - mae: 0.1417 - mse: 0.1115\n",
      "Epoch 00012: val_loss improved from 0.11078 to 0.11070, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1107 - mae: 0.1415 - mse: 0.1107 - val_loss: 0.1107 - val_mae: 0.1430 - val_mse: 0.1107\n",
      "Epoch 13/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.1059 - mae: 0.1396 - mse: 0.1059\n",
      "Epoch 00013: val_loss did not improve from 0.11070\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1085 - mae: 0.1398 - mse: 0.1085 - val_loss: 0.1160 - val_mae: 0.1538 - val_mse: 0.1160\n",
      "Epoch 14/5000\n",
      "48256/48884 [============================>.] - ETA: 0s - loss: 0.1092 - mae: 0.1397 - mse: 0.1092\n",
      "Epoch 00014: val_loss did not improve from 0.11070\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1087 - mae: 0.1396 - mse: 0.1087 - val_loss: 0.1145 - val_mae: 0.1428 - val_mse: 0.1145\n",
      "Epoch 15/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1076 - mae: 0.1387 - mse: 0.1076\n",
      "Epoch 00015: val_loss did not improve from 0.11070\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1079 - mae: 0.1387 - mse: 0.1079 - val_loss: 0.1167 - val_mae: 0.1450 - val_mse: 0.1167\n",
      "Epoch 16/5000\n",
      "48256/48884 [============================>.] - ETA: 0s - loss: 0.1067 - mae: 0.1366 - mse: 0.1067\n",
      "Epoch 00016: val_loss improved from 0.11070 to 0.10836, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1061 - mae: 0.1365 - mse: 0.1061 - val_loss: 0.1084 - val_mae: 0.1378 - val_mse: 0.1084\n",
      "Epoch 17/5000\n",
      "48192/48884 [============================>.] - ETA: 0s - loss: 0.1061 - mae: 0.1366 - mse: 0.1061\n",
      "Epoch 00017: val_loss did not improve from 0.10836\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1054 - mae: 0.1364 - mse: 0.1054 - val_loss: 0.1186 - val_mae: 0.1482 - val_mse: 0.1186\n",
      "Epoch 18/5000\n",
      "48096/48884 [============================>.] - ETA: 0s - loss: 0.1049 - mae: 0.1355 - mse: 0.1049\n",
      "Epoch 00018: val_loss did not improve from 0.10836\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1040 - mae: 0.1354 - mse: 0.1040 - val_loss: 0.1096 - val_mae: 0.1430 - val_mse: 0.1096\n",
      "Epoch 19/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1029 - mae: 0.1343 - mse: 0.1029\n",
      "Epoch 00019: val_loss did not improve from 0.10836\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1025 - mae: 0.1344 - mse: 0.1025 - val_loss: 0.1109 - val_mae: 0.1349 - val_mse: 0.1109\n",
      "Epoch 20/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1028 - mae: 0.1336 - mse: 0.1028\n",
      "Epoch 00020: val_loss did not improve from 0.10836\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1023 - mae: 0.1336 - mse: 0.1023 - val_loss: 0.1119 - val_mae: 0.1385 - val_mse: 0.1119\n",
      "Epoch 21/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.1007 - mae: 0.1319 - mse: 0.1007\n",
      "Epoch 00021: val_loss improved from 0.10836 to 0.10426, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1003 - mae: 0.1319 - mse: 0.1003 - val_loss: 0.1043 - val_mae: 0.1387 - val_mse: 0.1043\n",
      "Epoch 22/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1012 - mae: 0.1329 - mse: 0.1012\n",
      "Epoch 00022: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1003 - mae: 0.1326 - mse: 0.1003 - val_loss: 0.1083 - val_mae: 0.1342 - val_mse: 0.1083\n",
      "Epoch 23/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1002 - mae: 0.1307 - mse: 0.1002\n",
      "Epoch 00023: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0995 - mae: 0.1305 - mse: 0.0995 - val_loss: 0.1077 - val_mae: 0.1358 - val_mse: 0.1077\n",
      "Epoch 24/5000\n",
      "48320/48884 [============================>.] - ETA: 0s - loss: 0.0984 - mae: 0.1289 - mse: 0.0984\n",
      "Epoch 00024: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0981 - mae: 0.1290 - mse: 0.0981 - val_loss: 0.1099 - val_mae: 0.1389 - val_mse: 0.1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.0977 - mae: 0.1295 - mse: 0.0977\n",
      "Epoch 00025: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0970 - mae: 0.1296 - mse: 0.0970 - val_loss: 0.1079 - val_mae: 0.1444 - val_mse: 0.1079\n",
      "Epoch 26/5000\n",
      "48192/48884 [============================>.] - ETA: 0s - loss: 0.0973 - mae: 0.1283 - mse: 0.0973\n",
      "Epoch 00026: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0966 - mae: 0.1283 - mse: 0.0966 - val_loss: 0.1070 - val_mae: 0.1374 - val_mse: 0.1070\n",
      "Epoch 27/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0960 - mae: 0.1277 - mse: 0.0960\n",
      "Epoch 00027: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0955 - mae: 0.1275 - mse: 0.0955 - val_loss: 0.1088 - val_mae: 0.1409 - val_mse: 0.1088\n",
      "Epoch 28/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.0953 - mae: 0.1268 - mse: 0.0953\n",
      "Epoch 00028: val_loss did not improve from 0.10426\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0946 - mae: 0.1267 - mse: 0.0946 - val_loss: 0.1104 - val_mae: 0.1401 - val_mse: 0.1104\n",
      "Epoch 29/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.0948 - mae: 0.1267 - mse: 0.0948\n",
      "Epoch 00029: val_loss improved from 0.10426 to 0.10319, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0942 - mae: 0.1266 - mse: 0.0942 - val_loss: 0.1032 - val_mae: 0.1306 - val_mse: 0.1032\n",
      "Epoch 30/5000\n",
      "48192/48884 [============================>.] - ETA: 0s - loss: 0.0942 - mae: 0.1262 - mse: 0.0942\n",
      "Epoch 00030: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0936 - mae: 0.1262 - mse: 0.0936 - val_loss: 0.1040 - val_mae: 0.1310 - val_mse: 0.1040\n",
      "Epoch 31/5000\n",
      "48256/48884 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.1253 - mse: 0.0932\n",
      "Epoch 00031: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0929 - mae: 0.1254 - mse: 0.0929 - val_loss: 0.1037 - val_mae: 0.1344 - val_mse: 0.1037\n",
      "Epoch 32/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.0927 - mae: 0.1252 - mse: 0.0927\n",
      "Epoch 00032: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0921 - mae: 0.1252 - mse: 0.0921 - val_loss: 0.1074 - val_mae: 0.1361 - val_mse: 0.1074\n",
      "Epoch 33/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.0921 - mae: 0.1240 - mse: 0.0921\n",
      "Epoch 00033: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0916 - mae: 0.1240 - mse: 0.0916 - val_loss: 0.1065 - val_mae: 0.1343 - val_mse: 0.1065\n",
      "Epoch 34/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.0901 - mae: 0.1234 - mse: 0.0901\n",
      "Epoch 00034: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0911 - mae: 0.1236 - mse: 0.0911 - val_loss: 0.1076 - val_mae: 0.1371 - val_mse: 0.1076\n",
      "Epoch 35/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.0911 - mae: 0.1233 - mse: 0.0911\n",
      "Epoch 00035: val_loss did not improve from 0.10319\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0903 - mae: 0.1231 - mse: 0.0903 - val_loss: 0.1077 - val_mae: 0.1342 - val_mse: 0.1077\n",
      "Epoch 36/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.0904 - mae: 0.1229 - mse: 0.0904\n",
      "Epoch 00036: val_loss improved from 0.10319 to 0.10185, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0897 - mae: 0.1228 - mse: 0.0897 - val_loss: 0.1019 - val_mae: 0.1312 - val_mse: 0.1019\n",
      "Epoch 37/5000\n",
      "48064/48884 [============================>.] - ETA: 0s - loss: 0.0896 - mae: 0.1219 - mse: 0.0896\n",
      "Epoch 00037: val_loss improved from 0.10185 to 0.10043, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0888 - mae: 0.1217 - mse: 0.0888 - val_loss: 0.1004 - val_mae: 0.1303 - val_mse: 0.1004\n",
      "Epoch 38/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.0895 - mae: 0.1222 - mse: 0.0895\n",
      "Epoch 00038: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0889 - mae: 0.1220 - mse: 0.0889 - val_loss: 0.1019 - val_mae: 0.1321 - val_mse: 0.1019\n",
      "Epoch 39/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.0883 - mae: 0.1211 - mse: 0.0883\n",
      "Epoch 00039: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0876 - mae: 0.1209 - mse: 0.0876 - val_loss: 0.1037 - val_mae: 0.1307 - val_mse: 0.1037\n",
      "Epoch 40/5000\n",
      "48320/48884 [============================>.] - ETA: 0s - loss: 0.0871 - mae: 0.1199 - mse: 0.0871\n",
      "Epoch 00040: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0865 - mae: 0.1198 - mse: 0.0865 - val_loss: 0.1045 - val_mae: 0.1294 - val_mse: 0.1045\n",
      "Epoch 41/5000\n",
      "48096/48884 [============================>.] - ETA: 0s - loss: 0.0864 - mae: 0.1199 - mse: 0.0864\n",
      "Epoch 00041: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0860 - mae: 0.1201 - mse: 0.0860 - val_loss: 0.1093 - val_mae: 0.1369 - val_mse: 0.1093\n",
      "Epoch 42/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.0875 - mae: 0.1220 - mse: 0.0875\n",
      "Epoch 00042: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0868 - mae: 0.1219 - mse: 0.0868 - val_loss: 0.1028 - val_mae: 0.1322 - val_mse: 0.1028\n",
      "Epoch 43/5000\n",
      "48320/48884 [============================>.] - ETA: 0s - loss: 0.0852 - mae: 0.1196 - mse: 0.0852\n",
      "Epoch 00043: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0846 - mae: 0.1194 - mse: 0.0846 - val_loss: 0.1051 - val_mae: 0.1325 - val_mse: 0.1051\n",
      "Epoch 44/5000\n",
      "48448/48884 [============================>.] - ETA: 0s - loss: 0.0849 - mae: 0.1191 - mse: 0.0850\n",
      "Epoch 00044: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0846 - mae: 0.1190 - mse: 0.0846 - val_loss: 0.1053 - val_mae: 0.1302 - val_mse: 0.1053\n",
      "Epoch 45/5000\n",
      "48352/48884 [============================>.] - ETA: 0s - loss: 0.0853 - mae: 0.1192 - mse: 0.0853\n",
      "Epoch 00045: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0848 - mae: 0.1192 - mse: 0.0848 - val_loss: 0.1020 - val_mae: 0.1297 - val_mse: 0.1020\n",
      "Epoch 46/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.0773 - mae: 0.1165 - mse: 0.0773\n",
      "Epoch 00046: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0815 - mae: 0.1171 - mse: 0.0815 - val_loss: 0.1058 - val_mae: 0.1421 - val_mse: 0.1058\n",
      "Epoch 47/5000\n",
      "47968/48884 [============================>.] - ETA: 0s - loss: 0.0833 - mae: 0.1196 - mse: 0.0833\n",
      "Epoch 00047: val_loss did not improve from 0.10043\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0826 - mae: 0.1196 - mse: 0.0826 - val_loss: 0.1078 - val_mae: 0.1341 - val_mse: 0.1078\n"
     ]
    }
   ],
   "source": [
    "# NN model training\n",
    "EPOCHS = 5000\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                      verbose=1, save_best_only=True, mode='min')\n",
    "# early_history = model.fit(normed_train_data, train_label.to_numpy(), \n",
    "#                     epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop,checkpoint_callbacks])\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_history = model.fit(train_fp.to_numpy(), train_label.to_numpy(), \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "                          callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MAE')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if run converged\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "#plt.ylim([0, 0.15])\n",
    "plt.ylabel('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7638/7638 - 0s - loss: 0.0781 - mae: 0.1400 - mse: 0.0781\n",
      "Testing set Mean Abs Error:  0.14 bg\n",
      "68744/68744 - 1s - loss: 0.0856 - mae: 0.1237 - mse: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './norm_CH4_v/v_1_bar_test_parity_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e56c0bec0ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./%s_test_parity_%s.png'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2083\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/efrc-p3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './norm_CH4_v/v_1_bar_test_parity_1.png'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAHpCAYAAACBYEV/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd3hUVfr4PyfJJCEk0hGVUFSQFlGERARBIIjIoijIV6WvIkoR3f2JuoqKoLhgR5oUBUF9VgF1pYlSLAsRdgVRmlIkoCJNpTOTeX9/3MxkemaSCWnv53nuM8y555773mEm7znveYsRERRFURRFKXvEFLcAiqIoiqIUDarkFUVRFKWMokpeURRFUcooquQVRVEUpYyiSl5RFEVRyiiq5BVFURSljBJX3AJEm+rVq0u9evWKWwxFURRFKVKOHDnC7t27AQ6JSI1Afcqckq9Xrx4bNmwobjEURVEUpcjYsmULaWlpXHvttXzxxRc/Beun5npFURRFKWU0adKEuXPnsnTp0pD9VMkriqIoSilh5syZbmt1nz59qFixYsj+quQVRVEUpRTwyiuvMHjwYCZNmhT2NWVuTz4Qdrudffv2cfr0ab9ziYmJ1K5dG5vNVgySKYqiKEr+TJgwgYcffpiePXsyY8aMsK8LW8kbY1YCX4nI6IIIWJzs27ePlJQU6tWrhzHG3S4iHD58mH379lG/fv1ilFBRFEVRAjN27FieeOIJbr/9dt566y3i4sJfn0dirr8aiI1YuhLA6dOnqVatmpeCBzDGUK1atYArfEVRFEUpbnJyctiwYQP9+vVj3rx5ESl4iMxc/wOQGtHoJQhfBZ9fu6IoiqIUFyLC8ePHSUlJ4b333iM2NpbY2MjX2ZGs5GcC3YwxdSK+i6IoiqIoYSEiPPjgg7Rt25Zjx44RHx9fIAUPka3k/w10Br4yxvwTWA/8CkgAAfcWSBpFURRFKcc4nU6GDx/O1KlTGTlyJMnJyYUaLxIlvwtLoRvglRD9JMJxzwkiEtA0L+I3R1EURVGUc47T6WTIkCHMnDmTUaNG8dxzzxV6SzkSZTyXAKv20kBiYiKHDx/2c75zedcnJiYWo3SKoiiKAk888QQzZ85k9OjRjBkzJio+Y2EreREZWOi7FRO1a9dm3759HDx40O+cK05eURRFUYqT++67j1q1ajF8+PCojWnKmrm6ZcuWogVqFEVRlNLA2bNnmTZtGsOGDSuwc50x5r8i0jLQOU1rqyiKoijFwJkzZ+jVqxcjR47k008/LZJ7ROwgZ4xpBXQBLgISAnQREbmrsIIpiqIoSlnl1KlT3HrrrSxbtowpU6bQpUuXIrlPJGltDfAm0BfLw97lae9CPNpVySuKoihKAE6cOMHNN9/MypUrmTlzJnfdVXQqMxJz/XCgH/AW0BJLob8MXAP8AzgGvAtcHGUZFUVRFKXMsH37dr7++mvefPPNIlXwEJm5fgCw3eVln+va/7uIrAPWGWOWA+uAFcAbUZZTURRFUUo1drsdm81GixYt2LVrF9WrVy/ye0aykm8ErPRpc08SROQb4GNgaBTkUhRFUZQyw9GjR2nbti1Tp04FOCcKHiL3rv/D498ngKo+53/AmgwoiqIoigIcOnSITp06sXHjxnOelyUSc/1+LI96F7uAq3z6NMBS/oqiKIpS7vntt9/IzMxkx44dfPjhh9xwww3n9P6RrOS/xlupLwXSjTGjjTFNjTHDgJux9uUVRVEUpVxz6tQprrvuOn788UcWL158zhU8RKbkFwCxxpj6ue8nAD8BY4BvgUnA78AjUZVQURRFUUohFSpU4N5772Xp0qV06tSpWGQoVFpbY0wlYDBwCbAHmCsiv0RHtIKhaW0VRVGU4uSnn37i559/pnXr1ufkfqHS2haqJKyI/AE8X5gxFEVRFKWssHPnTjp27AjADz/8QHx8fLHKU+LqviuKoihKaWT79u106tSJ06dPs2LFimJX8FCAAjXGmD7GmM+MMUeMMY7c18+MMX2KQkBFURRFKels2bKF9u3bc/bsWVatWsWVV15Z3CIBkeWutwHvA3/BSmmbAxwEqgMdgOuMMb2BXiJiLwJZFUVRFKVEMmnSJIwxrF69miZNmhS3OG4iWck/CnQHsrCUeqKIXAAkAh2xQuz+AjwcbSEVRVEUpSTicl5/9dVXycrKKlEKHiJT8v2BH4HrRGSNiOQAiEiOiKwGrsNKkDMwyjIqiqIoSonj66+/pk2bNhw4cACbzUadOnWKWyQ/IlHytYEPReRsoJMicgb4EO+seIXGGPOgMeZ7Y8x3xph3jDGJ0RxfURRFUSLlq6++IjMzkwMHDnD69OniFicokSj5nwFbPn1suf2igjHmIuB+oKWINANigdujNb6iKIqiRMrq1avp0qULF1xwAWvWrKFu3brFLVJQIlHybwO9jDHnBTppjKkM9ALmR0MwD+KACsaYOCCJKE4iFEVRFCUSvvjiC2688Ubq1q3L6tWrz3nBmUiJRMk/DWwAvjbG3GmMqW2MseW+9sHKWf81MDZawonIfqxkO3uBX4A/ROQT337GmHuMMRuMMRsOHjwYrdsr5YXsbBgxAtLTrdfs7OKWSFFKBPrT8Kdhw4b85S9/YdWqVVxwwQXFLU7+iEjAA3Bihcn5Hvm1O4KNGekBVMGqYV8DayvgA6BvqGuuuuoqKSreeOMNAdyHzWaTiy++WB599FE5deqUV99Vq1a5+y1fvtxvrN27d4sxRgCZMWOG17lFixbJtddeKzVq1JDExESpU6eO3HzzzbJ06dKA4wc6jh49GvQ5du/eLU8++aTs3LmzkJ9IcF566SVZsGBBVMd8/fXX5bLLLpP4+Hhp2LChTJ06Nd9rHA6HTJw4UTp06CA1a9aU5ORkufLKK2XmzJmSk5MjsnevSJUqIjab1A3xeQ4ZMsRr3JUrV0qbNm0kMTFRqlSpIn379pVff/01pCxDhgwRQPr06VOoz0FRzgUePw0B67VKFau9PLJ27Vo5e/ZscYsREGCDBNGJoeLkP8/9A1ecZAK7ReQggDFmIXANMK84hXrvvfeoXbs2x44dY9GiRYwfP55jx44xadIkv74pKSm89dZbXH/99V7tc+fOJTk5mWPHjnm1v/rqq4wcOZK//vWvPPTQQ1SsWJGdO3eyePFiVq5c6VfF6NVXX6VVq1YB7xuMPXv2MGbMGNq2bcvFF18cyaOHzcsvv0zbtm259dZbozLejBkzGDJkCI8++iiZmZl89tlnDB06FBHhvvvuC3rdqVOnGDduHP3792fkyJEkJyezZMkSBg8ezLZt25h4+jQcPw52O4uAMwCxsXDLLfD3v7Nw4UImTpzITTfd5B7ziy++4Prrr6dLly4sWLCAw4cP8/jjj9OpUyf++9//kpCQ4CfHV199xbx58zjvvIC7XYpS4pgwwf3TAKzX48et9gB/6so0CxYs4Pbbb2fYsH+QkzOGrCzIyIBRoyA1tbily4dg2r8kHEAG8D3WXrwB5gAjQl1zLlbyP/zwg1d7ZmamJCUlWSvDXFwr7QEDBkjFihXl+PHjXtdceumlMnDgQL+VfGpqqvTo0SPg/QONv2LFioifozDXhkvdunWjtmK12+1So0YN6d+/v1f7oEGDpFq1aiFn1w6HQw4fPuzXPmjQIElISJCTV11lLVN8j/R0ERHp2LGj1KpVSxwOh/vaTp06ySWXXCJ2u93dtn79egFk8uTJfvc6e/asNG3aVJ599tmofi6KUpS0ahXyp1Fk7N0rMny4df/hw4vfcvDOO+9IbGysXHXVNVK58h8l0rJBiJV8xGltzyUikoWVZe9/wGYsH4LXi1WoALRo0YKTJ09y6NAhv3O33norxhgWLlzobvvPf/7Dzp076devn1//I0eOUKtWrYD3iYkp/H/X6tWr6dChAwCdO3fGGOPO0uTi9ddfp3nz5iQmJlK9enXuuusujhw54jXOK6+8QuPGjalQoQJVqlShZcuWLFq0CIB69erx008/MX/+fPf4AwcOLLDMa9eu5eDBg/Tt29ervV+/fhw+fJgvv/wy6LWxsbFUrVrVr71Vq1acOXOGQ82agc0naMRmg/R09u7dy6pVq+jTpw+xsbHu0+vWraNz587ExeUZwlq2bEm1atXcn4EnEydOJCcnh//3//5fuI+sKMVORkbQn0aRkZ0NzZvD9Omwfr312rx58fkCzJkzhz59+tCmTRuuumo5J06cF9CyUZIp0UoeQESeFJFGItJMRPqJFY9fotizZw+VKlWiWrVqfueSkpLo2bMnb731lrtt7ty5tGnTJqCpPD09nTlz5jBx4kR27NiR772dTicOh8PryMnJCdq/RYsWTJ48GbBM/WvXrmXt2rW0aNECgEceeYRhw4aRmZnJRx99xMSJE1m2bBldu3Z1jzt//nz+/ve/c8cdd7BkyRLmz59Pr1693BOBRYsWUatWLbp06eIef/To0YBlOfKVN9Dh+Qzff/89AM2aNfN6lqZNmwJWzuhIWbNmDZUrV+aCJ5+E5OS8v2Y2m/V+1CjeeustRIQBAwZ4XRsbGxuw8ERCQgLfffedV9uPP/7IuHHjmDJlCjbfv5iKUoIZNSroT6PICLVFcK45fPgwI0eOpGPHjixdupRvvkl2y+XCboevvz73skVEsCV+aT3Ohbl+27ZtYrfb5ciRIzJr1iyJjY2VSZMmefX1NIl/9tlnEhMTI/v375fTp09LlSpV5PXXX5fdu3f7meu3b98uaWlpboevatWqye233+7nvBfK8a5p06YhnyOYuX737t0SExMjY8aM8Wr/8ssvBZBFixaJiMiwYcPkyiuvDHmPYGbp/BwGXUf79u3d1zzzzDMC+Dk32u12AeTpp58OKYsvy5YtE2OMjBs3zmpw2QfT073sgw0bNgz4nK1atZJ0H5vlnj17xBgj8fHxXu2ZmZlen4Oa65XSRJCfRpFRXFsEwdi4caP7787w4XlOiK7DZrPaixsK6HinBKFRo0Ze74cOHcrw4cOD9u/QoQMXXXQR8+fPp379+pw6dYrevXtz9OhRv74NGzbkm2++4auvvuKTTz5h3bp1LFq0iHfffZexY8fy+OOPe/WfPHky6T72swoVKhTouVasWIHT6aRPnz44HA53e0ZGBikpKXz++ef06NGDVq1aMWXKFEaMGMHNN9/MNddcQ1JSUlj3uOqqq1i/fn2+/UI5DhaGLVu2cMcdd9ChQwcefji3zEJqqp8n0bp169ixYwevvPKK3xgjR46kb9++PP7449x///0cOXKEe+65h5iYGK8tlXnz5rF+/Xq2b99eJM+iKEVNgJ9GkZKRARs34rViLuotAl9eeuklAB588EGaN2/ubh81CubPz7M0nAvLRlQIpv1L63EuVvKLFi2S9evXy5IlSyQzM1MAmTNnjldf39Xyo48+KmlpafKXv/xFevfuLSIScCUfiP3790taWprExcXJkSNHAo4fCcGuHTduXMjVtcvxzel0yrRp06RVq1YSExMjCQkJcsstt8ju3bvdYwVbsTqdTrHb7fkeno5uU6ZMEUB+/vlnr7EOHDgggLz22mthPffOnTvlwgsvlBYtWsgff/wRsu99990nNptNDh48GPD8448/LomJiQKIMUZuv/126d69u9SvX19ERI4dOyY1atSQZ599Vo4ePeo+UlNTpXfv3nL06NESG46jKMVFcYftjR8/XgC57bbbxOl0BpTvXFo2woUQK/liV8rRPs61d/3p06elYcOGUrNmTS8Pel9FunXrVgEkJiZGPv74YxEJX8mLiLzyyisCSFZWVsDxIyHYtVOnThVAPvnkE1m/fr3fsWvXLr+xjhw5Iu+++65cdNFFXibsaJrr16xZE1Be11grV67M95mzs7OlXr160qhRo6CK24VrS+Xmm28O2e/48ePy7bffuuPjGzVqJP369RORvP/bUIdr+0NRlDyKS5GOGTNGALnzzju9ImdKA6GUvJrrC0lCQgITJ07k5ptvZsqUKTz00EMB+zVq1Ihhw4Zx8OBBunTpEnS8X375JWAWpW3btgEE9byPVGawYsg96dy5MzExMezdu5fOnTuHNVaVKlX4v//7P7Kyspg+fbrXPXzHh4KZ61u3bk316tWZP38+mZmZ7vZ58+ZRtWpV2rRpE3KsgwcPuq9bsWIF1atXD9n/3//+N0ePHvVzuPOlYsWKpKWlAbBs2TK2bdvGrFmzAOv/adWqVX7X3H777aSlpfHYY4/5ORIqinLutwgARo8ezbhx4xgwYACzZs3yiqYp7aiSjwI33XQTrVq14oUXXmD48OFB98Rfe+21fMdq1qwZmZmZ3HjjjdSvX58///yTJUuWMG3aNHr37u1XynDr1q0kJyf7jZOWlkbFihUD3qNhw4bExcUxe/ZsqlatSkJCApdddhmXXHIJDz/8MMOHD2f79u20b9+exMREsrOzWbFiBXfffTcdOnTgnnvuISUlhdatW1OzZk127Njhl/CnSZMmfPHFF3z88cfUqlWL6tWrU69ePVJSUmjZsmW+n4MnNpuNsWPHMnToUC666CIyMzNZuXIls2fPZtKkSV6e7nfddRdz5sxx+xScOnWKLl26sGfPHmbPns2+ffvYt2+fl5y+CWrmzp1LtWrV6NatW0B5vvnmG5YuXeqOSPjyyy+ZOHEio0aN4pprrgEgMTGR6667zu/axMREzj///IDnFEUpHi688EIGDx7MtGnTohKqXKIItsQvyAFUBepEc8xIj+JIhiMisnz5cgHkxRdfFJHwzOmBzPVTp06V7t27S506dSQhIUGSkpLkiiuukH/+859y5swZd7/8zN7r168P+SzTpk2T+vXrS2xsrACyatUq97m5c+dKRkaGJCUlScWKFaVRo0YybNgwyc7OFhGRN998U9q3by81atSQ+Ph4qVevnjzwwANe+9xbt26Vtm3bSoUKFdxJgQrLtGnTpEGDBhIfHy+XXnppwMQzAwYMEOtrbZGf2dzzuUVEfvvtN4mLi5PhIVxmv/vuO2nTpo1UqlRJEhMT5corr5TZs2eH9QzqXa8oJQOn0ynbtm3zel9aIYS53ljno4Mx5g2gn4gUm4WgZcuWsmHDhuK6vaIoilLCcTqdDBs2jLlz5/Ltt99yySWXFLdIhcIY818RCWgiLQq7hCmCMRVFURSl0OTk5LhN8/fff3+R1e8oKZSxzQdFURRFCYzD4WDgwIHMnj2bJ554gmeffRZjyva6NKRZ3RizN8LxqhRCFkVRFEUpMmbOnMm8efMYN24cjz32WHGLc07Ib++8dgHGjN4mv6IoiqJEicGDB3PRRRfRvXv34hblnJGfuf4AsFFEYsI5gLnnQOZi480333RXVTPGEB8fzyWXXMI//vEPTp8+7dV39erV7n6ffPKJ31h79uwhJiYGYwwzZ870OvfBBx/Qrl07atasSYUKFahbty49evRg2bJlAccPdPz+++9Bn2PPnj089dRT7Nq1q5CfiD8uuTyr2p0LZsyYQaNGjdzhgNOmTcv3mpycHJ5//nk6duzI+eefT0pKCi1atGDWrFk4nU6//u+//z5XXnkliYmJ1KpVi+HDh3Ps2DG/Pj179qRu3bpUqFCByy67jEcffdSvn4t169Zxww03ULlyZXfc/bvvvluwD0FRFD9Onz7NsGHD2L9/P7GxseVKwUP+Sv4boIkxJlxv+XKxin/vvfdYu3YtixcvpkuXLowfPz5oEpyUlBSvCnQu5s6dGzC+/dVXX+WWW26hQYMGzJo1i8WLF7vz1a9cuTJgf1elN88jVO73PXv2MGbMmCJR8i1atPCqancumDFjBkOGDKFnz54sW7aM2267jaFDhzJ16tSQ1506dYpx48bRrFkzXn/9dT744AM6dOjA4MGD8/La5/LOO+9w22230bx5cz788EOeeuop3nnnHW699Vavfs8//zyxsbE8++yzLFu2jPvuu4+pU6fSuXNnv4nD4sWLadeuHbVq1eLtt9/mww8/ZPDgwX4TRkVRCsapU6fcicrWrFlT3OIUD8Fi63JD68YDOUDzUP08+r8J5ITTt6iO4oiTz8zMlKSkJMnJyXG3ueLYBwwYIBUrVvRKeSsicumll8rAgQP94uRTU1OlR48eAe8faPxoprUNhNPp9IrPL2nY7XapUaOGO6++i0GDBkm1atVC5od3OBxy+PBhv/ZBgwZJQkKCnDx50t12ySWXeKXaFRF57733BJDFixe723777Te/8ebMmSOAfPbZZ+62P//8U2rUqCEjR47M9xkVRYmc48ePS4cOHcQYE3Yei9IKIeLk81vJLwReJfwV+nigY0SzjDJAixYtOHnyJIcOHfI7d+utt2KMYeHChe62//znP+zcuZN+/fr59T9y5EjQ1LXRyMS0evVqOnToAFhpbF3mfZd5vV69evTt25fZs2fTqFEj4uPjWbx4MQBPPvkkLVq04LzzzqN69ep07NiRdevW+Y3va66/7rrraNu2LZ9++iktWrQgKSmJZs2asWjRokI/z9q1azl48CB9+/b1au/Xrx+HDx/myy+/DHptbGwsVatW9Wtv1aoVZ86ccf9/Hjp0iJ07d9K1a1evfjfccAOA13PUqFEj4HgA+/fvd7e99957HDx4kL///e/5PaKiKBFy7Ngxunbtypo1a5g7dy6DBg0qbpGKjZBaQ0TWi8iDIvJtOIOJyHYRKXc2kT179lCpUiWqVavmdy4pKYmePXt6meznzp1LmzZtAsZnpqenM2fOHCZOnMiOHTvyvbfT6cThcHgdOTk5Qfu3aNGCyZMnA96mfk/z+qpVq3jxxRd58sknWbZsGZdffjlgKakHH3yQDz/8kDfffJOaNWvSrl07Nm/enK+cO3fuZOTIkfztb39j4cKFXHDBBdx22238+OOP7j4i4vcsgQ7P5/v+++8B/PLAN23aFLBKy0bKmjVrqFy5sruGgCuPtWf6XLDS7Rpj+O677/IdD6Bx48buti+//JKqVauyefNm0tLSiIuLIzU1lTFjxoT8/1MUJX/Onj3LiRMneOedd/wWAOWOYEv80nqcC3P9tm3bxG63y5EjR2TWrFkSGxsrkyZN8urraRL/7LPPJCYmRvbv3++ucPb6668HTGu7fft2SUtLc6ddrVatmtx+++2yfPnygOMHOpo2bRryOUKZ6+vWrSsVKlSQX375JeQYDodD7Ha7NGzYUO6//36/sT3TxbZv317i4uJkx44d7rYDBw5ITEyMPPPMM2E9k+fhaTZ/5plnBJBTp055yWe32wWQp59+OuRz+LJs2TIxxsi4ceO82mvUqOEuEezCVR2vYcOGQcfbt2+f1KhRQzIzM73au3TpIomJiVKpUiV5/vnnZdWqVfLYY49JbGysPPDAAxHJrCiKxZEjR+T06dMiIl7lqss6aBW66NKoUSOv90OHDmX48OFB+3fo0IGLLrqI+fPnU79+fU6dOkXv3r05evSoX9+GDRvyzTff8NVXX/HJJ5+wbt06Fi1axLvvvsvYsWPdTnguJk+eTHp6uldbsAI54XL11VcH3DL49NNPeeaZZ/j22285cuSIu71+/fr5jtmgQQMaNGjgfl+zZk1q1qzJ3r15qRgKUqEummzZsoU77riDDh06+DnejRw5kieeeILXXnuNO++8k927d3PfffcRGxsbdBvl+PHj3HzzzcTFxfHGG294nXM6nZw+fZpnnnmGv/3tb4C1rXH48GEmT57MU089RaVKlYrkORWlLHLo0CE6d+5MgwYN+Ne//lWmKskVBlXyBWDRokXUrl2bgwcP8uKLLzJlyhQyMjLo379/wP7GGPr27ctbb71F3bp1uemmm6hUqVJAJQ+Webhdu3a0a9cOgJ9//pkbbriBMWPGMGzYMKpUycs51LBhw4iruuVHoFK3//vf/7jxxhvp0qULs2bN4oILLiA2Npa77747LG/wQHvfCQkJXtcmJydzxRVX5DuWZ4Yq12dx9OhRL7ldk5BA9w3Erl276Ny5M/Xr12fRokXExXn/NB566CH27t3LAw88wIgRI4iLi2PYsGFUqFDBr4odWF693bt3Z9euXaxZs4batb1TTri2dnxL+l5//fVMmzaN77//3l3RTlGU0Bw4cIDMzEx+/PFHnnvuueIWp0ShaW0LQLNmzWjZsiVdu3bl448/pmHDhjz00EOcOHEi6DX9+/dn8+bNLFmyJOhkIBgXXnghd999Nw6Hgx9++KGw4udLoDSPCxYsIC4ujoULF9KjRw8yMjJo2bJl0IlKQVizZg02my3fo1OnTu5rXHvvrr15F669+CZNmuR733379tGpUyfOO+88li9fHlBpx8fHM336dA4dOsSmTZs4cOAAEydO5IcffqBt27Zefe12O7169WLDhg0sWbLEXXPeE5fcwShz5S4VpYj4+eefue6669i1a5c7rFnJQ1fyhSQhIYGJEye6YzGDxcs3atSIYcOGcfDgwZBfwl9++SXgSnrbtm0AQT3vI5UZrNVmuJw8eZLY2FivCcDKlSvZu3dvWOb6cCiIub5169ZUr16d+fPnk5mZ6W6fN28eVatWpU2bNiHHOnjwoPu6FStWUL169ZD9K1euTOXKlQGYNm0aZ86c4a9//av7vNPppE+fPqxcuZKPP/6Yq6++OuA4PXr0YPTo0SxfvtxrErBs2TISExP9HAkVRfFHRLjlllvYt28fy5Yt49prry1ukUocquSjwE033USrVq144YUXGD58eNA98ddeey3fsZo1a0ZmZiY33ngj9evX588//2TJkiVMmzaN3r17U6dOHa/+W7duDZhUJy0tjYoVKwa8R8OGDYmLi2P27NlUrVrVnSUu1F73DTfcwMsvv8zAgQMZNGgQO3bsYOzYsVx00UX5PlO4pKSkRLz1YLPZGDt2LEOHDuWiiy4iMzOTlStXMnv2bCZNmuTlEX/XXXcxZ84cHA4HYE1yunTpwp49e5g9ezb79u1j37597v5NmjRxr+pXrFjBd999R7NmzTh9+jSffPIJU6ZMYdKkSdSrV899zbBhw3jvvfd47LHHqFixoleIYe3atd1m+2bNmjFw4ECeeOIJnE4nLVq04NNPP2XmzJmMHj064P+poijeGGN47bXXcDgctG7durjFKa4ETGMAACAASURBVJkE88grrUdxJMMREVm+fLkA8uKLL4pIeAlnAnnXT506Vbp37y516tSRhIQESUpKkiuuuEL++c9/eiWlyc8Tff369SGfZdq0aVK/fn2JjY318oavW7eu9OnTJ+A1r776qtSrV08SExOlZcuWsmLFCmnfvr2Xt3sw7/o2bdr4jVe3bl0ZMGBASDnDZdq0adKgQQOJj4+XSy+9VCZPnuzXZ8CAAWJ95S1cn3+ww/MZVq9eLS1btpTk5GRJSkqSa665Rj766KOAzxRsvCeffNKr75kzZ+Sxxx6T2rVri81mkwYNGsjLL78clc9DUcoyP/zwg19EU3mGEN71xjpfdmjZsqVs2LChuMVQFEVRioDt27fTsWNHzpw5w9atWwMmoCpvGGP+KyIBzaARmeuNMTHANUBzIAX4DfhCRIreG0xRFEUp13z33XdkZmYiIqxatUoVfBiEreSNMfcBj2KVnz0GnARq5p77ELhbRI4EH0FRFEUpLWRnw4QJkJUFGRkwahSkphafPJs2bSIzMxObzcbKlSv98pUogclXyedWoJsP9ASmA1NE5Pvcc/HArcCzwApjzDUicqYI5VUURVGKmOxsaN4cjh8Hux02boT582HTpuJT9Js2bSIpKYlPP/3UK7GWEppwgnHfAroDN4nIMJeCBxCRsyLyLpAJXAIMBTDGtDLGNA44mqIoilKimTAhT8GD9Xr8uNV+rnHlH+nfvz9btmxRBR8hIZW8MeYW4P+Ae0VkiTGmTqADcACrc/sCPAbMKkrBFUVRlKIhKytPwbuw2+Hrr8MfIzsbRoyA9HTrNTs7cjm+/PJLLr74YneRp2BhwUpw8jPXjwa+FJG5ue/3ELrs7LHc1+eA/xhjMkXk08KJqCiKopxLMjIsE72norfZLIUdDtEw969evZpu3bqRmprKpZdeGvlDKECIlXyuuf0KvFfk1wM7gSPARGAYMAE4CPwA3AIgIuty+/UpEqkVRVGUImPUKEhOthQ7WK/JyVZ7OBTW3L9ixQpuvPFG6tWrx+rVq6OadKu8EcpcfyXWqt2zPvx1wHlAmog8IiLTRORR4HKgEtDRo+9/gNA5RRVFUZQSR2qqteoeMsRavQ8ZEtkqvDDm/k2bNtG9e3caNGjA6tWrQ6byjsaWQFknlLneNXX62aOtH7BIRH717CgivxljFgL9sUz8APuBC6MlqKIoinLuSE2FSZMKdm1hzP1paWmMHj2ae++9112tMRAlMQKgJBJqJe8KhfOcCNQEnEH6m9zzLmxATsFFUxRFUUojBTH3f/TRR6xd+xMjR8bw4YeP8dRT1UKuzEtSBEBJJtRK3lWpowGwKfffO4BbjTFPichBV0djTE2s/fgdHtfXxdsKoCiKopQDXOb+CRMsE316euhkOm+//Tb9+vUjLu4OROaFtTKPRgRAeSDUSn4N1p789R5tT2Gt1jcbY8YZY+4xxjyDNQmoCYwFd/rb64Avi0BmRVEUJcpkZcHll1sr7ssvt94XBpe5PyvLeg2m4OfMmUPfvn254IJ2OJ3Twl6ZZ2TkWQpcRBIBUF4IquRF5DCwErgvN7MdIrIIKxb+DPAPYBpWqls70EdE3s+9/E6gGvCvohNdUUKjTjmKEh5ZWdC6NWzeDCdOWK+tWxde0efHjBkzGDRoEJmZmZx//mIcDu8Sy6FW5oWNACgvhKxCZ4y5GvgKeFVEHvQ51xCoARwSke0e7bWA/wI7RaRdkUgdAq1Cp4C/U47rD4A65SiKP5dfbil2X9LS4Ntvi+aeDoeDa665hho1arBgwQIeeiiR6dP9nfWGDAnuAOjKrx/OlkBZJlQVunxLzRpjxmGt1h8Wkefz6VsLWALUAVqJyO6CiVxwVMkrYK3cI/2DoSjlleRkawXvS8WK1kQ52uTk5BAbG8vvv/9OhQoVSEhI0Il5IQil5MPJXT8amAxMMMb82xjjN5AxJsEYMxj4H1bo3V+KQ8Erigt1ylGU8Ln44sjaC8P48ePp1q0bZ86coXLlyiQkJACFj81XApOvkheL+7H24hsDWcaYHcaYj40x840xK4HDWBXqvgRa5ma8U5RiQ51ylPJOJD4pM2aAMd5txljt0UJEGDNmDP/4xz+oXr06sbGxfn0COeupb03hyNdc79XZKjvbCcvjvh6QhJXSdiPwbxH5oQhkjAg11yuge/JK+aYg3/+sLBg8GHbtslbwM2ZYk+VoICI89thjjB8/noEDBzJz5syASj4az1EeCWWuz7eevCci4gCW5x6KUmKJNE5XUcoSoRLFBPNJycgoOie7p59+mvHjx3PPPfcwdepUYmLC2Sku2HMo3kSk5BWlNFGYtJyKUpoJ6pPyxnfA9HM+4+3RowenT5/m2WefxfjuC4RAfWsKT3jTKUVRFKXUENAnhbOkn1hlhZ00bx7R5nZB9sWdTicffvghIkLz5s0ZP368n4LPb1z1rSk8Ee3JlwZ0T15RlPKO3142Z0nmOJtoTir7IoonLci+eE5ODoMHD+aNN97gs88+o2PHju6Y9qwsS3n37Qtdu4YeV/fkw6OwIXSKoihKKcIrHK3idwxhep6Ch4hs3pEWgnE4HPTv35833niDJ598kg4dOriV9fTpsH699dq+PRw7FnrciMLq1A0/ILonryiKUgbJ80mZHjgzVJg270j2xe12O3369OG9997j2Wef5dFHHwUCTxQCEWjcsHxrzkHdWV9LRGlx5NWVvKIoSgmm0AvUQiZ5j2Rf/KuvvmLBggW88MILbgUPgScKgSjwfnsR150NZImI0K2h2FAlryiKUkKJinIpZCq5vn3BM+ItLs5/juDy7bruuuv4/tNP+dvu3V6zkkAThbg4SEiIUoGZInbDL8216wuk5I0xjYwxtxhj+kVbIEVRFMUiasol3LqvPmRnW85xOTl5bbGxsHRp3hAnT56ke/fuLFmyBLKzadSzp9+sZFTfn/2MCSkpsGZNlNLYFrEbfmkO5YtIyRtjrjDGbAC+B94H3vQ4194Yc9IY0z26IiqKopRPilu5uCYZDkdem9MJ8+ZZ/z5+/DjdunVjyZIlHDx4MOisJHXe+IDGhIyMAs09/CniurOlOZQvbCWfW1p2NXAZ8Aqw1KfL58ARoFe0hFMURSnPFLdyCTXJ+PPPP7nhhhv4/PPPmTdvHgMGDAh5QQGNCeFRxNVtSnPt+khW8k8C8UCGiPwNWO95UqxNmbVAq+iJpyiKUn6JhnIpjONesEnGlVee4PrrrycrK4t3332XO++8M/QF52JWUoSziNJcIS/sZDjGmF+BT0Wkb+77J4EnRCTWo8+LwF9FpHJRCBsOmgxHUZSyhCt06+uvoXFjq23LlvDCuAqbTCbY9Rs3Ci+88AAdOnSgR48e0bthCaA0hsqFSoYTiZI/A7wkIo/kvg+k5F8ChohIUuHFLhiq5BVFKYsURH+OGBE4RD7MZHfu+7omGWlpBxk48Bht24YoNO95QSmrDFVa5yjRynh3ALg0nz5NgVIQOagoilK6KIinfTQc91xW8A8//JW1a6/j7rtvxOHpiRfsggKazbOzYeBAOP986xg48NzFo5fmULlgRKLkVwLdjTGXBTppjGmFVWtey9AqiqJEmYIo7CZNAre7zP7hsn//ftq3b8+ePXuYOnUqcXFFkyw1OxvS0mDOHPjtN+uYM8dqOxeKvrijGYqCSJT8eMABfG6MuQ+4EMAY0zT3/b+BY8DzUZdSURSlnBOJT5vL2e7jjwt/371799K+fXt++eUXli9fTocOHQo/aBAmTIA///RvP3bs3KymizuaoSgIW8mLyHagJ5aH/WvA3YABvgUm57bfKiJ7i0BORVGUck24nvaeWfIOHw481tat4d/3kUce4dChQ3zyySe0bdu2YMKHSVYWBHITczrPzWq6NIfKBSOiZDgisgyoD/wN+BfwKbAQeAi4VERWRl1CRVEUBYCbboIqVaBmTbjzTv+yrCNGQMuW8PvvwXPFR7oynTp1Kp9//jlXX3114R8gHzIywKfkPGCl1S3sajqcUMLSHCoXDK0nryiKUsIJ5fUNMHq0lYVOxFr1BiNcb/Ft27bx9NNPM3PmTJKSkiIOKytoGJprT/6PP7zbK1WCzZsLrmxLq9d8uITyrkdEwjqATmH2GxPumEVxXHXVVaIoilKWGD5cxGYTsdS4ddhsIgMGiFSpImKM97lgR1qayN69ge+xd691n6ZNN0uFCjWlevWasmPHDtm717qH6/42m/U+1DiR9A90/YABIjVrWseAAeFfG4xgn9/w4YUbt6QAbJAgOjESF8kFxphrRWRziNnEP4DHsbLjKYqiKFEgmNf30qXW6jRcg2yFCoFXrq6V7rFjG3E4MoF4bLaVJCY2CBpW1q0bJCb6r9RDhaGFE5ufmgpvvhne84RLWfSaD5dI9uRPAEuMMbUDnTTGPAiMA/4TDcEURVEUi2Be3xBenXZX/2D72pZX+wYcjo5AEvA5p041cpvcAynIzZsDl78tiQq1LHrNh0skSv5GIAVYaoyp5HkiN4TuBax89l2jJ56iKEr5w9dJrG/fwF7fXbv6Ky+wnNeMsWq2e/YP5iWelQU5ORWBhli1xi51K+ZACtIT34QxJVGhlkWv+XCJJIRuE1YIXUPgQ2NMPIAx5m6skLpNQBcROR5NAY0xlY0x7xtjthljthpjWkdzfEVRlJKEZwica6Xctatlmvf1+r7vPsvz3JPYWOjfH9auhXvvDe0lnp0N/fvvZM8ewZjGWDXG6gF5itlXQQbCc6VeEhVqWfSaD5eI0haJyGfGmLuAOcBbxpjFwDRgG5ApIr8XgYyvAMtEpFfuxKLY8uIriqIUlHA9zoPtac+b572nnZ1tKf+cnLy2hARYs8YaH/Jeg8nTpMlKjh/vDozFiozOi1+LibEsCC4F6UpHf+qPs2zdDg7i3X1tnCW98Rkgxa9/SUlf78q2W+4I5pEX6gAeAZxADrADqFWQccK4TyVgN7mhfuEc6l2vKEpJIxKP81atAnvGp6d798vPY9zlLd+qlfXqe6/u3ZcJJAo0E/jV735xcYFl3DvgcanCEbFxxronZ6QKR2TvgMej94EpEUEI7/qg5npjTJ1gB/A28AFwBBgExPucjxb1gYPAG8aYb4wxM40xFaM4vqIoStTx3VMfPTr8wifh7mmHcnALZPL3dI77+OOP+fjjm4BGwCrgfD85HI7AMqZuWc4mLmcI00kniyFMZxOXk7r1k3A/HuVcEkz7k7dSD3UE6uMINmakB9ASK19+Ru77V4CxAfrdA2wANtSpU6eI50yKoijB8V21h4ph912dB7o+2Ko/1Eo+1Llff/1VKlSoIDVrtpS4uMP5xtb7yViAoPP8rApK4SDESj5oxjtjzJtAgdLhicigglwXQIZawDoRqZf7/lrgERHpFuwazXinKEpxEqiGeyBC1XUPpyR7qCxuPXtaK3hf0tMtC8Dy5cupXftqrr22kvv6mBj/bHm2GAdDqi9kUu8v8oSIMH1cWc82VxIIlfEuqOOdiAwsMonCRER+NcZkG2MuE6tATidgS3HLpSiKEoxAZnRfbDZISrKqq6Wn+zviheMkFsrBLSMDNm70liM2dh7VqiUBt9KlSxfA+/rGjeGDD+DkyVxlzFmSnScY9dvfYfoBmD8/TzNH4FlX2OQ4SuEo8bnrjTFXADOxqtztAgaJyNFg/XUlryhKcZLfSr5iRejVy0ehRnl167t6jo19g5ycu2jf/npWrVqKCVQFBg8Lwr/2kH5wMaPkOQAmMIosMshIO8Woxe0jkjE9PbRVQSk8oVbyEVWhKw5EZKOItBSRy0WkRygFryiKUty44sQD6VGbDQYNgpSUPAUPoR3xCoJnXHi9etPJyfkr7dp1ZsmShUEVvOu6SZMgq25vJslwAJqziekMYT3pTN98jZcDXzgUKjlOOKXjlJBEFCcPYIxpBXQBLgISAnQREbmrsIIpiqKURlwK1rcynGdSmJ49iz71a2oqNGw4iT177qdbt268//77JCYmhndxrr1/gn0Ux0nGnhsTb8cWsal91CjL0u+7J59vchxfc8TGjd5bBkpYhL2SNxZzgHXA01ge7QM9jgEe/1YURSm3uIqs7N4NQ4f6Z1k7V6lfd+/eTY8ePVi4cGH4Ch7c5ogsMtwK3kWkk5ECZ5sLtZmvhE3Ye/LGmBFYIWxzgVexQtZeBv4FXIeVIGcJ8KiI/FQUwoaD7smXHgpac1pRSjtF7XF+6NAhqlevjoiQk5NDXFzERlvIzmZEt11M33wNdvJmJKGiAqKKbuaHTbT25AcA20VkoIj8L7ftdxFZJyLPAR2wctt3LJy4Snkgv2QdilKW8VzdNm8OjRpBnTrWpLcwvwER4cknnyQtLY19+/ZhjCmYgs8VctTi9iRXsRVPHvqSWOmmFBKJkm8ErPRpc397ROQb4GNgaBTkUso4aolTCkNZ8MdKTbWU5d69sG2bpfSnTYMGDSzFH+lziQiPPvooTz/9NDfeeCMXXHBBVGQstsIuJbHSTSkk0ineHx7/PgFU9Tn/A3B9oSRSygUlsea0UjooS/5YvpNdh8M6vv0Wtm4N77mys+Gf/xTef//vHDjwEn373suMGZOJ8S1PV0CKrbBLSa10U8qI5FuwH8uj3sUu4CqfPg2wlL+ihEQtcUpBKelWoEisDKES54TzXK4Jz9Spkzlw4CViYu7n44+nsH9/iY+ODg93TF+W9aoKPmIi+SZ8jbdSXwqkG2NGG2OaGmOGATdjed8rSkjUEqcUlJJsBYrU1yTQZNeT/J7LNeFxOgcCU3A6X+bECVNiJjxK8ROJkl8AxBpj6ue+nwD8BIwBvgUmAb9jedkrSkiKda9PKdWUZCtQpFYG38muL6GeKycnh0WLxmO3HwOSgfsAU/AJT1lwdFD8KFRaW2NMJWAwcAmwB5grIr9ER7SCoSF0ZRMNt1NclOSCJwWJ+nJ9t7/4wnLAy8mx9uVDPZfD4aB///688847xMa+SU7OAPe5AoW4leQPVcmXIktrKyJ/iMjzInKfiPyzuBW8UjbRcDvFk5JsBSqIlcG17bxxI/zwA9x7b95zLV1qTQA8F9d2u5077riDd955h4cfHk9y8gBcPnYxMVbhm0DbXiEX6kFMENmjX9fFfWknWA1a3wOYDdyUT5+/ALPDHbMojquuuirMCrxKaaEA5asVpVjIrxZ8JHXVA41VufJpuf76mwWQF198UfbuFalUKa9mvTHWe99x861R36qVXyH5vdSWKrF/5FvXXil+CFFPPpKV/EDginz6NMdKmqMoUaMkO1opiicuK8Odd0LNmlClCtx0k3UuUotU4MX1Ab76agOvvfYaDz74IKNHW+VqXbuuIlbhG18fgHx9BQKYICaYRzguSSU2ikEJj2jHWSQAOVEeUynnlGRHK6X0kZUFl19ubTlffnnRZEj96CM4ehR++w3efttS5qNHR+aU98UXnpPb04DgcNShUaOtDBs2jOxsqwCO0+l9XaAJcL4T5QDhLlkxV2N3xgW/xgP12Su5RKrkg3rpGWMSgHbAr4WSSFF80HA7JVpkZUHr1rB5M5w4Yb22bh1dRR9s1bx0afgWqexsKxmOxXGgK/B3bDZo3TrFfR9fBQ8QQw7pp9a4NW12Npw+7d/PZoPGjXOVc89URtz0E9l3Pux2CMjo2zCsybX6zJRwgtnxLTM/uzwOJ3DEp811/AScxFrFTw41ZlEfuidfNnHtZaan57+XqZQfItnjFhFJS/Pbehaw2qNF8+aB71GzZvi+JQMGuPr8IdBGIFZgvsTEiFx+uXVN48aB7xOLXfbG1RepUkX2rtsvVaqIxMX597PZvOWJi/P3Hwi5j5+L+swUP4TYkw8ZQmeM2UPe6r0O8CdWLLwvOcBh4DNgnIicLNzUo+BoCJ2ilA8KEvWVnGyt4H2pWNEaJxoyNWgAZ854t9ts1j79Rx95yxsTA5ddBu3aeYeFnn8+/PbbUeAG4H/AO0Av93hxcVaYnT859GIBtThAFhmcjq3IVudlOCS8DOa+4Xeu8L5QWWW1WFzxEyqELuT/vIjU8xjECbwkIk9HVzxF8UZj4pVwCOVMFixG/OKLLRN9oPZoyZQTwCspJgbGjrUO35j4QHnqRZxAN+Ab4H2sZKJ5BFbwgg07H3EzZ7EBMZDjJJJdWd/tg3Dy1mdkWOF/nlsR6jNTcohkT74DMKeoBFEU0P09JXwKEnUxYwYY491mjNUeLZkCKeBGjSyF6VKa115r7ae7+vo64d14YwzwMPABvgo+OAY78Zwlnrw/7TGEcKUKSKTKWX1mSjZhK3kRWSMiP7neG2POM8akGmPOKxrRlPJISS8+opQcChJ1kZEBH3wAVataq+uqVeGDGb+RMS86ruHBZLr2Wu+2YBOUL7/8lY8++oixY6FSpZsx5sYI7i6AyT08MVguVfmTkBC5ci7JyYmUCNPaGmPigYeAvwL1PE7twUqWM1FEzkZRvojRPfnSje7vKeFSkD15v2vihOSc39kUexWpjt2FTucarkwjRlhWKk9FHxe3n+TkjogcYNeuXZw4UdW9H/7HH7B9e8Ti5OIkBidOYgi1rktIgDVrrImKUrqISlpbY0wK8CXwNFAX2ItVmW5v7vungS+MMcmFllgpt2hMvBIuBVlB+lmKHIbjUpEJjgdzG8I0HQUJDA9XJl8Td1zcTzid7XA4fmHx4sVUrVqV1FSrX3o67NtXgA8IACEGJzEIvn/uL7sMBgywxh8+3Eqpqwq+DBLM7d73AF7Esvm8D9T3OVcfq0qdE3gh3DGL4tAQutJNuGE7ilIQAmRvFRBJZ51PQ3rwQaL0JXWF/11++U5JSakj551XSdatWxf0NpEfOZLAaWlS5efAzxziEZXSBVFKa3sbsFFEeonIbp+Jwm6s+I5NQO/CTjyU8ku4KyHNsKUUhICWIs7SmC2M4FXSWccI8xrZja8PPkgBHUd8v7M//2y1HzmygLNnj/POOyvJ8FhK+94mEmrWhOHDY/hhbwId+1yg1rFyTNh78saYU8DLIvJoiD7PAfeLSFKU5IsY3ZMv+2hVzNLLuQiPDHUP/++OkGT/A4CTJGEnHhtnSa4Ux6bNMYFlS08ne/0vTGAUWaSTwdeMYgKp6RcGdRzxva8V5+4kLi4Gh0OIi/uFlJQLWbrUSlWblQU//WSlxY2UmBjo1w+2bLGev29f6NpVfy9lmVB78pGY67cCU/PpMxXYEu6YRXGoub7soxm2SifnYivG9x5xcSIJCVYWOldGPN/sib1uPC4x5OT7fXKb16vtlQROSRxnrL6ckSockb0DHg8ql/939luBpgLfed0zISGvn1VZzhmxmT4+3v8z/vBDK6tfxYrWq8eugFIGIErm+hlAb2NM7SAziTrA/+X2U5QiQ6vSlR48TdTdulkV0zyt3L//bhVuiRa+Jm6Hw8o+t2lTXs4Fl5lcxJJn4bKKuZ7nefh+nzzzN3x7OJUzJOAg3upLPMepyARG+T2zayvJ+zv7DVbakd8Bm9c9z5zJ65dnZA1sba1WDWJj8+L+bTbLQ97p9N9J6H2bsO17BydOwLbvHXTt4tQtrnJCeLkOLRZiFaD5nzHmZeBz4ABwPtAeGAmsARblKnw3IrI3OuIqimbYKi34mqgDIWKZp8eOjY7pONAE0IXdbin19u3zFOF//xukyEuM9/fJf3/cOxbdTjxfbIxn4EDreUSscTdutDLZ3XST6zu7HrgeOA9YCVySzxMZYrGTQyy+3vEXXgiLF1v3c6Wd/eILa0Lj+9zWRMH6c293xnH8j7NMGH2GSW+m5HN/pbQTiZLfRV62hbEBzhvgptzDk7xvl6JEgVGjrD+cvnuMmmGrZBGu45hI6FS0kRBoAuiJw+GdkS6Qggdrddy3r7USd+2Ph3oOm81KU/vtt54r8LyVNEBi4rfY7ZlANSwFX8/dLy7OWpXn5PhnzHMSgwFiOEsO8bj+DG/dau21e+6tjxhh7cV7y+ofPmcnnq+X/gyoki/rRKJ85xJpfkRFKQJcHvj5Fc5QipdQq2pPnM7obbX4TgALgjFwyy3ezmq+qXA9iYmxFHMwH2a73Zp4ZGY24IMPeiPyJOC969m4sZVat2tXawvDcywhFhtnSeEPjlANl8J2OPxz9QeaAMc47OQI7u0FsCIK0vka75xmSlkkbCUvIgOLUA5FiYhwCmcoxUugVbUx/sowmlstnhNAzyIwDkde1bdAq2WXXC6rkKsqne/+eEyMNSmJi/NW7qGClGJjv2LLlqZs2lSZYC5LFSpYn9emTdCypb9XvZ14zlABvxV5gIIyvhPgvr++Qtf37+Z47jg2zpLMCUZ13YxGPJd9InG8UxRFCZtAhUvOOw8qVSraYiauCeDGjVYWt3vvzcu5sGYNpKR4379SJejf3zsvg7/J26J6davfvffCHXdYJvZQCt6YZeTkZGK3/y1oH89JTmoq9O4dOJb/4oT92LAHvdb3+bOyrNeMF29nU6V2DDEzSCeLIWYGmyq1I3XsPcEFV8oMuleuKEqREGxbBc7dVksgi084Wz3BnDt7984bLz09+JaAMWDMv4FeJCQ05fTpiUFl9JzkZGdbzoHevgKCnTj+PBNHDDnEITiIx2YTkpNN/hOk1FRSNy9h0oQJ8PXc3Ideovtb5YSQyXCMMSvzud6JFQeyCZgnPpnwigNNhqMo5ZwoZNwJJ+FSoCIzLuLiFiLyf7RocSXNmy9n1qwqAVf8l10GV19tWQ7q1oWPPoKzAUt8uXyec4jHzqXsIrZqJUzti2jXTn1SyjuhkuHkp+TDq09oYcfKdjc9Qvmiiip5RSmfZGfDhNHHyJq3gwznOkbJc6TaDpCddBkTevyHrC0pEel811whkBUiKwuaNLHK1v75p6/J/gzQlFq1arJt21L+/LMSaWlWJTlPUlKsff0TJwLXoA+OEM9ZhBjs2DSDnRJSyednru+QBPHxxwAAIABJREFUz/kYoDpwDXAX8JoxZpOIrItcTEVRlPDwXay7U7f+UQG78yo2ksZ87mCp/Qa6/rGc429VwO4Rtx6JQnQp8J9/9va437gRkpKsmvSHD3tekQB8yoED1bj22hRmzIDNm62kP0uXWj26drVe58+PVMEDGM4SjytW3zN1vjqjKr5EVE8+5EDGpAHrgYUicmdUBi0AupJXlLJNIFN6IK95G2dpxFa20Ri7Z/iYzXKwC6UQA+Waz8kJHBnQqJHlxW+3zwI2AFPwTZbTq5cVa+9pSUhPh/XrC/1xuElPD5o6XynjFGYlHzYistkY8xHQNlpjKoqi+BKoCFwg7MSzi0u8FLyrv2/K2gkT4PPPLSXucMCBA3D0aF6fYKttu92aYNhsU7HbhwI3AGexVvN5vP++9eppSWjSpKBK3jXTyJtIaMZHJRjR9q7fgX/GO0VRlIgJ5j8XbpIdG2e5mF1so5HfSt6lELOzIS0t0L56eBgD5533CidPPkDt2t3Zt+89fBW8Jy7T+ujR1n5+wfC2EmjGRyUU0Y6TT8SaxiqKohQYz4Iw69fnFZfJzg5SEz63OIs7/j3GQXLsaWb0Wk5ypbigcfmjR1sOcQXftXyRL754gFtvvZXu3d8nlIJ3Ybdbe/MnT4Z/lwoV/NuMgcREqFLFyo2vKIGItpJvj5XjXlEUpcAEMskfO2ZVsvv8c8tEHpdrh3Qp7jVrrL329HQYMjSOTbvPI+O9h9i0OSavfYi3053LEa6gtGnThMsu689PP73LggXx+V+ApZxzciJJuyucPuUf6CQCp09b2fHefjtvEuRJoIp4SvkiKo53xpgYYDTwBPCciDxW6EELiDreKUrpJz+nNFdBl0aN4NprCx4nfv75/ilk80eAb0hJaUFMjLUid+W3j+TPaej+rrh41ys+//bH5VA4alTwtL4aalc2KbDjnTFmdj5jx2CVVGoF1AB+Bl4siJCKEogo5DVRSiHhVJMzxlLw4YaNBfoude0Kc+YEv8ZfEQvwCDCRtm2/5NNPrwma3z4/Qk0IEmLsnHHa8FbqvkrfG7vdUuzBCvScy1A7/d2WHKKZDGcNcJeIFKu5XlfyZYdwso4pZZNwatFD+GFjwb5LS5dCly7ejnfGwMUXW3v1v//u6VkvwIPAK1SocB+XXvoamzf773gmJloFbo4cKcxev5Ngu6kVK1rybd3qEzLoFc4XfOSiDrXT3+25pzAhdIPyOe8E/gA2ichPBRFOUYIRaF9Wk36UD3zz3p865a/UwFLEAwdaaWFdK0bwX0UG+y7Nm2clqvGq2tY3L+lN3v2cwAisGPiRnDr1Et9/b9w14D05c8ZyAjzvvIJ77VsK3n/VbnAyaFAMo0YFVqTG5F/3vqhD7fR3W7IIqeRFJIQhS1GKlkChUr4xzkrZxbO4jGt1eOyYt6Lfvt06wDLvv/WW9W/XPrkrLr1OneDfJd8iNiNGBLIgLMVS8KOA5wAT1CQvYqWqveQSS+n+/ntBPwH/PfnkCk6OHYuhZ888j/qtW/PS7k6YYL0PpuhjYqxJTFGiv9uShZaaVUoswUKlNOlH+cO1sm/cOHgfu91a2f/5p/8q0ukM/7sUOA6/G/AZLgWfHw6HNfkouIJ3YSn6RE7Qi/eJOXWCt+cL69dbHvUffGB9JllZloLv29e7vG+czzIuJ8eyUhSll73+bksWUUtrW1LQPfmyg+7tKb4UNBVstWqW4nWt8G0xDpLMKXpc+j0bY1vgjI3H4bCU4s8/W9nunE47lol+CMZcWYj99cKTThbpfM10hvhl8HM5+nn6GcybF3ybI5y0voVBf7fnnlB78rqSV0osrtVbsBhnpfwRaJXoj782PnzYUnR1LrATh50Y51lO5sQzf/tVbNoSz+bNwtat1v784cPgdJ4FbgemExPzBeedZ5neiw5L5ksu8T9j4yzpfE0W6X4KHvI8+T39DCZNslb3iYn+fgxFbTrX323JQpW8UqJx7ZdmZVmv5e4PhWYz8WLUKG9zdGACa+MTJ2Dn3jgc2DhDEnbicWALcM0ZoBewkOTklxk69H42b4aePaPyCCFkFtq2tTLYuTP0cZZkjjOKCWTwNbZ8Eor6KvDiMp2X+99tCUKVvKKUVELldi2n+K4SK1eGQCv34PjGnftyCugB/BuYgshIfv0VOneGxYst03jRYdi61eP5mp9hSMIcNsW1JJV9jIp7iWRzAltc6Oc9dSrvK+I7KdI89+UPVfKKUlIJFYtUDnAZMZo3h8svhyuusN5D3iqxbl0IxxEufGKwgo5mAfdx4oRVQW77dkt5hpPkpqAYcmjcOC/8L/3aBEat6Ubqvd0gPZ3Ue7uxae0p7uxjhe4FY+vWvLmgms4VdbxTlJJKMC+zsl44PDub7NGv03zeQxxzJuGQPBfxuDhISclTVCNGwLRpgsPhqegDpYTNj2NYtbWqRXhdtBBsJoccE4eIFYYXzGFtxAjLqJNfPHxROtcpJYsidbwzxtxvjOlY2HEURfGhPMYi5W5RTJhbi+M5iV4KHiwnst9/t6rHgWV2TkkxbhO2DTsJMQ4aNziLzRauov4Dqw78jUAOxaHgAewSi9OZlzwnmOEmnFK7GpeuuIiGuf5lLDdURVGiSRneUM3OtjLVnX++dQwcmLuPnLtFkSUtA3qSg6UE583zMUffa2jeHBql2bismY1OXeK5+eZwJDkKdAa+xkp0E8IOHlU8LajG5zWPQMo6nAgDm83aXlB/TSW/3PXhrNA/BZbgUZhGRFYWXrSCoeZ6pUzhqvThyrla0it95Mqb/fluJshDZMVcTca1CV5iZ2dDWpqVuMaTSpVgc92/kPrtYgby/9l78zi5qjLh/3tq6c7STROCDTJplrCEoG2jJNXqGKIoYkQdlMhobGhwxIh0zzuOY786v4mOxlGn1fEnYYSAC0uAMUQFFCPiFqJDOgmSEE3YAtgFyJZA0p2lu5bn/ePU7druvXVv7dV9vp/P+VRX3XPvPVX39n3Oc57t+9zMJSQdhK5ScOWV+RnxLBeGUCg/dCyTri4YHn6Jl18+F9gJrAPeW+q3Lzt2y+523zWR0FX5rGpz8Xj2exOnPrlxW673UqDGt9FeRKo1Hc7DCHmDoUakpE905Ei64lsZpYUYTYRDQkurmkjSsnatfXnXQAA++ZoNDOy6jM74H9lHG25L511dOm0tWLZ5d8GeSXs7HDz4fkZHfwH8BL1c78yRR+qc9GNjlXW+y0QpfV474Zw79+vpqW0CHENtKaVADcAocAe6QoMdvcBjwP8WNzyDwTAZiK64jsFXVrJWLuQV2pCUFh6LK155Bc4+Wy+1O9mTk0nYHHgjg8HPcTA+g2wBn+8Mlyls77vPu4AHa5LxbeAJ4K2ufWfNSgvaaFT7A6xZk1+Yxi9O9eSVSueYX7nSXvvOzbcPehkftNCvdgIcQ/1SSMh/HlgBzAUuFZHduR2UUr3ABhH5eAXGZzAY6hhL6N1xB+zb96XUp/natwiMu+dxIRDQYWNDXEJse649Pv+Yf/6zLq06bRo8/bTXET8NXAOsBI5PNSeEzk7F3XenBe2zz8Jtt5Uu4N147WvJOqff2uzd3XqFI3MyNdn9NQ3OFAyhU0q9AbgZOAH4rIhcnbM9CXy3XoS8Wa43NCJ+H+T1gJNtvVja2tJlX6+5phKC9CngHGAPsAU4zaWvoEhw1CwFgSBHH60nIY88AslkZUPsMiMki8kDb3LHTz1KCqETkT8CbwCuB76tlPqNUuqEMo/RYJiy1H1iO4fUuoODuvRrqRx9NPT2agHf0aGXqcsv4HcDi9He9L/CXsBL1qsQZM/LQfbs0cJ9167KC/hcjdsuH9LLL8P55zvfHyYBjiETLzZ5RGQM+JRS6qfADcAOpdRnRGR1JQdnMEwF3BLb1dxRKlcttAq0b9/O0FBHASe0wgIxFIIPfSj7e65ZU46BZ/II8HbgMPAb4PUO/TJD2ZzGXlkB3zIjycDIFyByD3R3M7TxG8RizXl9d+zQl8VJeNvZ7A1TE19x8qnQuNeiHfG+o5T6JUV43xsMhjR2yU3qxlHKZQbS3V0ol7u3uuu537P8yfyeA8LAb3EW8LmUJsyVyq/l7kQgoL39ly8bYTtddNz6nxNLOmfs+hEBZf+IHd0XZ3DBWhMIb3DFdzIcEdkvIpcAFwFnUv30UAbDpKKuE9u5zEB0trnSDm/3Pbu7CwlIr3rFK6nXxWhtvtPv8IpGRMepFypoEw5rX4StW2FV67/ScfARiMWIModLY6u5efwikg5fN5YMsfmFE+rQvmOoJ4rOeCciPwLmoafGXyrQ3WAwOFDXie1cZiAdHXrZuLdXh5kFg7odeSSuBVQyD2P3PQcGCu3vRa/4I3Aq2mcYoIlqLzoWiqlvbs6xl6cmVFHm0MV2buISkoRw+r5WnfmpVrjI4I+S0tqKyMsisl1EPAewGAyGbEp1lKpoyfkCM5CODrjhBti7Vy+9x+Pw0ENwxBHuqVcDAefv2dEB8+aVMujNaBv8TOAtqc9qUXRGL9vPnWu/rakpPcHp74fIX9bSr65mBV9ilJaJPAP2CIokPdYkJsO+U9H7wdB4iMikameddZYYDBbDwyJ9fSILF+rX4eFaj6i8DA+LzJolEg7r2mXhsH5fzPd0/K2sDZGI5x/R2mX6dKumWnbr7HTfv69PJBSy39e9/UGgVWCuwFNF7F/+FomILF2a/3k4LNLbm339QoyLIuHpuIq4zGKPDDNHH6Cvr6z3g6FxALaKg0x0FZjodFB+2263Y1a6GSFfH9RCuOaec9Omyf/A6+tLf79M4dHX5+845RYO1rU49VQb4aT0tfEyHqW8CtOkwNMCLQKnCkRrLtyt1t6um922kIqLUkmb7+Lt2GHGpE9dPXGxynU/5F7HyTpJniyUIuSTRbSE2zGLaejSUA8CPyvU1wj52lMLbcLunM3N+dpgKQ+8emThQvuHfyTi7zjlFA7DwyJtbfYCOhAQufPO/P52gmTTJpF587wKeJEjjhCB6wSerblgt5vYOI271BZpf3LiR3O6H2bO9C+kzapA4+Am5AvZ5E+yaVcB4rDtJHQK3HLzf4BdFTiuoQK4xX1X85xjY1XK4V1DI+gZZ2i7bybFeOY7OdGvXev/66xYobPg6fl5NsEg3Htv+ic780w49VS49hrRUWNXx+g69SBDdz3HkiXwxBNezngPMMT+/QCXA6/2N+AqYP0WgYkSIIJXH4HZs7UHfu51htS1vuhEonTQ3w9/+Yt9vwMH/Dvh1+L/2FABnKS/UwO+QAW0dZfzzQF+jc5HaTT5BqBc2mU5zpm3vFluTb6G6o6lMed+x7a2wqfP1Z57e/M1eUsDbWvT2zM1bTvTSF/vflnY/qQ0c8j1Gsyfn/2T5Wu0SWljr4SCXjTdOwWaBM4pm2ZcydYeeklmMuK5v3W/Dg/raxAM6tWQzFst1yxVrvu/Fv/HhuKg2OV62x2qL+TXAWehS0UZId8AlNsuWMo5m5srLH9r8WVdTh0IaGHght28pK1NN0uA2Al70OaPpqZsYaNNIkmBuIPQzj9WYac6LwJ7nUBIICLwcg2FdzJvvIFA/hJ9OCzS1/k76VNXS5ix7P4kZPa0/dLMYQmpmOP9aucD6XQfOP3GXoV0DW9tg0/chHxJIXSVRin1HuAFEXmgQL+PK6W2KqW2vvjii1UancGJWsR9O51zw4YK5/Cudrq6DNPA0Nqn8k6dTOoc627YLcMePAgXXKDzyNuh59va/DE+rnPLWzHgaZOIFfLlvgwt4qUsrJVa1onbgL8HIsAvgSMLHbCCKJRSEwl8wmGdJCgzjDAQ0L/XyLyz6Gn5CS0cIIwuyxdmnDb28WC8i8c4hU+wmkjwAZ0BL+d+tdLVDg3p144O+1swmYSjjiotyVJd528weMdJ+js1qqjJA19F14Z8Cp2b8iCwxm0fo8nXB0VEXTXkOauq7qRU8OHQSdLHVdLOXyUwoT17P7XbMqzd16m/lhRYKrBY8LH0XckWCumwwMx7L3OJ3dLqw2GRWW0J2bR0UPrafyiR9ielb94vZTh0UtH3kNMtmBueV8xKVk3+pwy+oZGX6zPO+1bMcv2UpW5Deappk+/rk+HQSTKLPRnLvemlYq+ndpuX5H4dp+X72rXDqdcxgQN1MJ506+pK3xLWvdrZ6SHCo0Tjt9staIT01MBNyHssoWAw1A6XQmi1L59ppasbHNRL9JFI5YrBDw0xGP8Uo7QQoyn1oUKR4FXtQS66yP7UubXqe3r072f9nqGQXk7euFH3X79e13Nfv14vy+/Z42VwQuUzyn0HuBZdaGY2TPwG9UEymX+v2pFnzenu1jd15g4+1tUL3YKmGt3URulJgMNGpX5j8/GJwAnABofdRETeXvrQimPBggWydevWWp3eUAH6+3X4T+4zcPnyKfYA6+8ncvXFbCH/4R+J2FdvyxU6ll11/Xpd0nXjRnj4YS3M43G9fcYMLbBGR7VKWR98C/hn4H3AWiC//Gqt6eqCRYvy79Vc8u5dp4tUF7NYQyOglHpARBbYbSukyb+1iG1181gwTA7quhRrNRkYoPv6X7Bt7MwMTR7CYSESUXka+8CAc6zzmjVayPT3w86daUe4WAz276+EcLcOWIy2/zXgc8CFwK3UgwZvOdJZhEJawNvdqxq90mHrvFbN1SDDlKOQkH9bVUZhMLhQzGqmncBr+GdmRwcDG87nlsVxRscUMcKEw0JLi6Knx96k0dHhPkGyE0qVEfCFvOWduAYt4D8M3EThR1ZlCQTguuvgM5/JNncEg3pVJJmEUFCIJ9KTmTDjnB54lOmdpxJZ1Gx/L1pu8wZDuXEy1jdqM453kw+/vm31lI6zEg6DXmOlw2Ht+OXm/F99b3q/CWueFfhXISeKoLotKUppb3Xr+lm/eVdXdvrkUEhEkZRQyjEyzJguIhM6Kc9bPuve6N0vw73/lpNdqB49TQ31COX0rq/3ZoT85MSPl3C9JPGo5mTDyUG7qyt3DEmZ1XxAhl93vvbW3/SM52xp1Z0IrBGI1cFYdJs/P31NC2UKDDMunWyTCJukj6t0lTjI8pbPuzcYk1ns1X1DoeyMQSZpvKEAJQl5YBBYBYRd+jSl+nyt0PEq3YyQN9RLOs5KTja8CJtAQFc/6+3VLdJ1WPqar0vHZKeEx/CmZ6SvT/f1XvWtUi0p8I8CCNxYc+FujckqrJM7aQsGHe41hlwvvF0p3RBj0sdV9gc0qeYMLrgJeVcDl1LqXcCngX8SEUd/UREZV0o9DFyllPq1iNxbBkuCwVAUJUYklY1KOQzahRTOmKHbwYPpcyaT8MILcNNN2pbcc8qDDCS+Skf8yfRgRkfpWPNVVq1axdCQ7u+GUtr+XDhjXTEkgU8Cq4FPARdX4iQeEdJOgopkEt7/fv07Zjop2hFmnAg5oQ4zZmR52913X/5vGKeJjbzF/qBT0tPUUA4KpbVdBuxFB6cW4jrgJaC31EEZDKVQL+k4u7tLSyvqhFta2uXLob09uxKZiA6Ru/mRhXTFtxJlTnpjhvCwG28mgQBccgl84hPWOSSnR+57PySAj6EF/GeBb1L5uHsnMgV8mmSy8ORGkSRJgBFa0r+zUvriZHjbie1PJSSdHsm1mKUaJgWFhPybgF+5afEWqT6/Ad5YjoEZDMViRSRVNGe9Byo12XBaIdi1Szton3CCvRBJEuRljuR8fpYWQCnhEY3CyIgWZHalSkHnY1+5Up9j61ZoCY2RFuylCHiAx9Dx718AvkJtBXxpeycIcSsfoYvt+ncWySsoELB98ir+ynFEGKKfq4gGT9Qfm6TxhhIoJOT/BnjSx/GeAo4rejQGQ5mwK+RRizF4mWz4LUfvtkIQjcLhw257B9hBpxZAoZOgpYVoz+fo6oJbb9UaP2ghdMIJ0NysQ8TmzYN77kmP/dlnYSSWmZBGUZxgtoTq6cBO4N+LPE4p5E5USjm/fqTGaGKUFgYZsNXCFy1ioqBN5jj2cBRbiLA6dCVdoT8T7XpP7WaphsmBk7Fe2/I5AHzdrU9O/0HggNf+lWjG8c7QSHj1wM90tOvt1WVhc/ex6ooXLuOqvbn7On8nMjxcVJnezk7LSa4Uh7YxgfcLrKqpY104LDJ7VlxCqvxhehGGbC+olxoBxtfO4BVKKDX7PHqK7ZX56GpxBoPBA04Z6QYH030sR7vVq2HLFu1INzICc+fqzy1Fb80ava8Xp7gYTax9fjGRCztYu9Z++X9sTBzH9cQTUJrGexj4APATtMNdLRBAmJY4wP59QlyCBffwQ5hxIp2HbLXw3FUeuxK/xtfOUA4KCfn7gXOUUscUOpBS6ljg7al9DAaDB7x44OdOBES07fyxx2B4OJ3Nzyml6rRp9ud+8UU9aXjpJbut+c5nmeOaO9fLt3PiEPB3wN1on95/LOVgJaC/34HkNGLJ8mbSC4ehZVYTA3cvdlxmzzQpXXRRvglGKXjqKW8mHIPBiUJC/gZgOrBGKTXdqZNSaho652Rzah+DoWj82qgbGS8e+E7C2yoiY2nXZ5xh7zR3wgnQ1pZvAxZJHyebhON4d+/W1+TLXwZLE/ZHAngvcC/wPWC5z/3LjSKJXw2+8HeeO9efGT3XSRP09XnhBb2C09U1uf8PDJXDVciLjne/E62hP6iU+phSaq5SqinVTlJKfQzYlupzp4j8qvLDNkxWcpemq/mAq8XkIvfhbhU+GRlJn98ttM3SrqNRuOMOe6/63bv1Me09unNJ0sQ4Tkvxe/bA1VfDsmXWJ36X7IPoSnI3AR/1uW8l8DZRCRJPvY5n7Jf5ms3jj/sbRebyfXt79rWyM+EYDJ5xMtZbDWgBfoE2nCUcWhJYD8wsdLxKN+N419jUKiVtLfPdDw9rZ7pgMJ1xLvP81ticstEddZTI7Nn2zltWCwS8ZrNLenSo89rPaq8IbK6pg12x3yUYFGlWY9LFH6WPq+ROzpdOtslMRuSo4MuOv3ex92y9ZGw0NA6U4HiHiIwCS9Dpp34PxEjHy8SAjUAP8G4ROVDOCYhh6lGrsrJeHOAqRUeHjkEPBNKaeOb5LS3vkkt0trncJfm9e7WGnb/sniaZtNfy8/GqmfsJmdsLvAN4F7Df4z7VxP17JBKQVAHOVA8B8GVWsJj7+HXwPN57ysO2+ySTxd+zlUqiZJiaePI2Sc0UbgFuUUoFgaNSm/aKiLMBz2DwSa1S0ta6Zv3QxjFiWXHn+vwbN2rTgVUy9w9/gN5eeOQRf8fPrX/uTjnj1F8CzkXHwP8IOKKMxy4n7t85lgxxMx9BUAhBtrCAqxNXEtptv18p9+zAgC4TbE06TS4cQyl4stJlIiIJEXkx1YyAN5SVWqWkran2FI3S/fBNhCfsvanzh4SHH872TzjvPO1V74dwOL1SUF2eB94KPAzcBbyn2gMoG0rpjIEy4aQXBBTxeL6QL/WerVbGxqnk4DqVUeJtDa9hWLBggWzdurXWwzCUQDSql6k3b9YPICtEzM++lubrdd/coi/Wg7pSD9esMY6sgFtuoSu+lVFaiNFEmHECAUUiEM6Ke1fK27L7rFnagevpp7Wn9/XXwzXXwI03lve7uPNZdHHKnwLnVPPEZSUcdi5Gk8vMmXDZZf7u2VpQzfvdUHmUUg+IyALbjU7G+kZtxvFu6lKq85zlANfeni7RWm7HO9sxBvfJMHNkmDnSx1UTdchfN/3hohzJQiH7bHWbNlXboS0m8FAdONYV32bO1PfBtGmF+9o5iOaWBK6XkvC1cnA1VAZKqSffaM0I+UmOy1Oz1AdXNTzsbccYiEmfujpv4H2dv8vrW8hDXin7PuGwyNKl1RCMTwqcJ/BszQV0qS0c1gLeLbLBLhqimvdTsRgP/smFm5CvupXOYCiaAkH0pTrPVcPD3naMyRA/kEvpV1frqmWptdOB60+19U+wI5gyFVuP61xiMfjZz8r3Pex5HDgbGAL+WumTlQGbHypFMJj+rUdH7X/T1lYd8eBkN69lxEYhjAf/1MEIeUPjUOCpWeqDqxoe9k6JbQ4wk9VqOZ3BnVw69z4ix/+VwTXHsX59tgPWBz6Qn7lOKZCk4Ca0oFB1ulJ5BFgMHAR+C7yhkicrA4KbR70IrF8PO3fa2+Pb2+HPf4YbbnCudFjriA03auXgaqg+RsgbGgenp+bGjUDpD64zzrD/fP78wvt69VTu6XH2co8lQ+xLtHLjI29ky/ZmVq+GJUv0+Net033WrcsvQCOiU7nUrgb7LrSAjwO/A86s0Tj84P5bJZO64I/dpMzKUzA4aH+drXvhL3/Jz2lQL9pytTz4DXWA0zp+ozZjk5/E9PXZ11Ftbp4wdFom+0jEv6NTb6+9nbK3130/P+VivZaCtbML59rn063Ukq+ltucF3iGwq+Z29HL+LpFI/rXNvTaF7PCF+hsM5QAXm7xjMhyl1OeLnzfIyiL3NRicGRjQsWC5qmwyqdWqVasmKntZYWoXXug9lG7nTvvPd+1y38/JirBihbbbWqFyIyPeS8FaxGJ62Tjz+JmEwxCIx4gLJGjK2OK+HF0eHgbmAu3ogjP1hBBmjBjNFPM7WBq3pfEODsLatbpin5VUKNNatGqV/iz3XgC9cnP00brSXL2H1hkmIU7SH52PPrfl5qvP/TwJJJyOWY1mNPnJzfD8d0ofV8nCVJjZMHPSapfVp0iv5mK98508lYPB7DEEg/61yXBYh/M5bW9qErnz3dfILPZKkLEqavb3C7QJ9NeBxm7XkhJgvGC/UEiktVX/joU0bi8e6cZr3VALKNK7/m027acpYX4TcBk6p/1lwM2pz++kkbNeGOqaaBS6nvgJq1nOFrpZzXK62E40dFKWodOXV3OGMX1gZAUtM5K+bfp2dlsrjWzmGLxXgksfo6VF2+XtSsiCXhX48Ywe3hTaTIIw1dHgfw+8Ezga+BcP/aWyw7HFKiHrfu7587UD3eOPQ19a2gnDAAAgAElEQVSfu33ai2On8Vo31B1O0j+3AZcAB4A3OGxfgHatvdjrMSvRjCY/eenrEwmHsrXUMGPS13xdltrlWZuyUfmH214rfb37fdn07VYOnLT2TO2+UHvb27Q9vqvLvV9AxaukvYvAbwVmCswTeLoONPZCzf138aNhe1khsvO7aG7WiYgMhkpBMTZ5Gz4F/FBE/ugwWdiqlFqb6ndzCfMOg8GWoSGI5eQKj9HE5tMvgY50cRfPRW5sVP6Og4+wqvVfYWhVwbFcfjk88QTMnTPODW/6PvduPYrNRIgsmc0IrdxyS7b9PRSCj3xE2+nXroUXX9RiwIkNG3S8dizmls5WSIqfinClcAj4MHAC8Gvg2Cqcs1T072L3+3nWsFMOHh1DQ2x/33kMMsDmXa22KZc7OrQPxeLF6WufSOjVGOO9bqgJTtI/t6G19P8o0OcrwAGvx6xEM5r85MWLzbxQbfYsMlR+K6XsQjZJX/sPXTX4TZtyM6AlRZGQOzlf+tTVsjC4VZa+ezRPa1QqrdEVqhFfLk21/G2LwAt1oJ37+95dXUVmnyvCwcOkjDVUG8qR1hZ4DvhdgT73Ac97PWYlmhHyk5dCz9vc7YGAFvaOOehTT+Nh5sgs9kg45bgWDsRcn+OdnfaCJ0Bs4hiKRF4fpwlJeYReJYXqHQLfqLFgz23eTRTW715UeGUREts43xmqjZuQ95MM525gkVLqG0qp1swNSqlWpdQ3gb9FO+cZDGWnUAKP3NV3y9GttdVhmTSVPWdQfXai+hvopDQvvwznn2+f7OSJJ+xGpx29rGPY/WvlZjvr6NAZ0+680/NPYINQ2aX624GlqdfxAn2rx9HsZSm308xB1ERwj+T1UyrtPGmFVzplqLPNaFRE2jrjfGeoK5ykf25DB8PuRv83vYJObfXD1Osr6PC5x4F2r8esRDOa/NRkeNg51MxVgxoeloXtTzpqgXYavZMm71WjtGPTJn3cmTO9VTyrTrtFICDwFoF9dTCe9G/dy/ezPpxYjVExAQ+rOHY3kN0yUW+vb02+ngvTGCYnlEOTF5EXgAjwPSCErkTxwdRrCLge6E71Mxh84TUtrNO+XV3akS2XghpURwfdF51om08e7EPvrr8+N6RNa5DBPE1XJrYVCsfr7oaHHtLne897XMZbkHxttjhuAHrQ/97rgSPKdNxSEcKp3znCJvq5iihz6OBpttPF8lf9mEgEPvlJePJJvVLiydnNKe4S3HMl29y4JmWsoa5wkv5uDS3UX4tenn8tECrmOJVoRpNvPErVfOzMppY25+U41vmdNEe7lQBL854+XeSoI+Ny8hF/lWY1JqGUJpntHJaQ5qZkVhiVU8Xc4eHsxCy1a9cKvFPgQB2MJf07NnFIWtmX9p9gTGaxRydFKsW7zc2Q7mTMNyq7oU6gTCF0mRODOPCnck00DFMbt+Q1q9wj2QB7synoVKJbt+q/+/vT6WXtwp62b9c2+B07so/htBJw3HHwhjfoVLiHDwfZK8cSCkEwGeNIeZE9HIWQqv9KgGQ8zpo1Ibq7tfLX2anT3CaTumruNdfo4jUA4zU1fT8D/A2wHLic+qhhpQAhkFoZGaEVyw8hRhOjtHA+dzMtGaN75DQGokVozW5xl5YxP5dSb1yDoRo4SX+nBoSBd6Hj4VdkfD4NbbcP+D1mOZvR5BuPUr2R3Ryg/ShbuX2VsrfrWv0CgfwxhxiTaeSHz2V+HyePeut8tdOYvykwQ2BbHWjvds3N7yGRd32dVkts2bRJZ62ZuJChwlq5caM31AmUybsepdS7gKfQnvbfBP49Y/OZwF+Bvy9p1mGYcpTqjexWYtZPiltLo1+2TCehUUonMrn1Vm3zt/wErGNahUoyidPEYWbkfR4OxCe+z/r19t9DxP6Y1eFrwKeB8wGHmrs1xy2KQD/KMosDdXXB6tV6pWT16uxrmEU0qrPVJBLpz4JBfaHclgSMG72hAfAs5JVSC4A7AEFr8bdmbheRTcCTwPvLOUDD5KfUOvBujk5OEVA/+IG9g19Hhw65s3LPW/0zJwZO5gGNkC+MhBkzAxPfJ1OW5O0thb9veRHgi8DngGXof2sHL8QGwa5yn2v9AmvWlpme0Coo70apN67BUAX8aPIr0FnvFojIVcBjNn22AF3lGJhh6uAmpL163TvFQNspWwAHDjhrd4VCo7u7dYpae+y1zQs+EJj4PiMjTvtqBbK6/AS9IHcpuu5UUW46dYV1vT2HtxcRCw8UTtxgMNQBfoT83wJ3iMhzLn2iwKtLG5JhKmInpK3QOE9Lrg7kKluZOGl3hVZhBwb8CmM1UZN+cNBuSV6r7yHGSSSqrcr/HfADdGRs1WcYZcdSppcs8bGSXsqye8EMO+6UEjpqMHjBj5BvAV4q0GeGz2MaDI74KhnrQKayNXNm/nY7ha2nJ7skbO4qbAdR5rV4fxoHg2l5MTSUvSqsUSgStDDq+ZilIcBK9Jw8iNbiG/vfNhzWE0BLmV65MntyFwrpa7pxo40wrdGyezkmsQZDIfz8Zz8DvKZAnzMB26SfBoNfil1FzcVSti67rLDCZueDFQhk+GClnsxn772LkMc0r8lkOjzO3nwgCEFeYRaVryaXBK4APg/cVuFzVQZl8xMpBT/9aVqZzpzcdXXpiVYioT/LE6Y1WnYvxyTWYCiEHyG/HjhPKfUWu41KqSXAm4GflWNgBkO5nZe9KGwFfbBSHXrkRoIkwUOGORFdljYazVwlsPbLdNSrtIBPAP8ArAb+FfhMhc9XGeycE8fH84WjNblbtEhfQ+ua2grTEpfdi6Fck1iDwQ0/Qv6r6Bz1v1RK/SepOBul1Pmp97ejQ+j+q+yjNExJil1FdbJzelHYhjaOuT94h4aIxo5hCb8gQQArUUshduyAU04Wzl10mPhYLGO/atSBB4gDl6DT1f478OUqntsfgQDMnp2vsSsF8+Y572ddo+jQs/S/bgORlj/T/7oNbPzNuOs1rZVd3ETgGaqCUwC9XQPegC5Ck8xoidTrY0Cnn+NVoplkOJMLv+VBS8o0OjwsvU23SIB4TmKdZLpUaefvpJ3nROX08Vb21K4OerXqwb8icKbAV+ogqY17s5IP2V3H3Jw16WuUuj82PSOz1N6stLdNHLL9nZcu1ecKBtOJjaqZmdZkxTWUC1yS4Si93TtKqSA6Y8abgNnAPmATcKfodLc1ZcGCBbLVymVqaFiiUb2c6pSK1on+fm1zzc1Ounx5fqbR3HP0PPd1zlt3OftoI63lCm1Nh7nnvuksWQKjI0Isbq8Bh0J2TnW1Zgy9YjANOJx6rW82bWIi/e/goNa458/X23buhBNOgLvuSqf/tVZ4tm+HwfM3sHrHmyZK/gIoEqnSv9nXralJ3ye5j0Cn+yWLYm9Qh8Ns3qw1+CIPY5jiKKUeEJEFthudpH+jNqPJNz6laDiOmUbbn8w6gF0WU63BJ3L2jUvv7Lsci+Bk7j97du214Ox2SODdAu+x1WTrsYVC+TVm7O6Htjatheeu8Cyc+aeyjKNQeWKjghvqCcqR1lYpdYlSammBPq9TSl3ibw5iMGTj5nVcyH5qa+dknMiLd0+4VEejcPbZMDaW7hOPQ5IA+W4qQbZxZoEsd3r//fuL/caV4CDwPuDnwHupV/t7LvF4vuOZ3f1g/dZ5yY/mvjRRitZCkSCA93zBBe3ixi3e0Eg4Sf/cRtr+vhZodujzBSDh9ZiVaEaTb3yctPGursIK1ISSFdAlX+1KkToViLFvSemcNyZ9ffYFaeqzjQi8VUAJfL8OxuOvdXZmX1On+yEYzFee7Wzybbwsba2JrPumuVmvGuQe01N5YlOYxlBnUK4CNcBeYCnwW6XU0eWbahgMaZy8jpNJnRI2U4F6+WVdIjbPg/7oHxNhiOWsZnsq03J/7JtEvvdxfrhmDO8oEsEmenpqWTzGLz3AfcDNwGU1Hou4bg0E8lME79qVHcfe3W0fGy9iEzbXfRzb7z/E8s77icz8M8s772fHpoPs+HMgK6piwwZdo8C6z5TSsfQXX6zvH3BZMTJu8YZGwkn65za0Jv95dDULy5v+tJw+X8Bo8oYScTJ5nnGGs/aXp3319cmm4Julk20ynVEJEJNgSrvzZ59OSnNTwtaju37bAwK318E40r+h02eBgP0KieUtb90PTiV4S1GenSI3CprcjU3eUGdQRk0eEfkCOg/m8cD/KqUWlWW2YTCkcIpnd8sXn2sSvev4Pt6Y+D07eB2HmEmSIIkJj2tvse1W37HxAGNjXvvXir3A9am/34BecKtntGqeTNqvkMRisHntUxCJ0DHYT8/7R7NSDUPpyrNT/puCJndTmMbQQBRVckpEblJKRYEfo5PjfExEbinv0AxTGesBnIndkq1FZnKTobue4+8GTrX2ynkl473YfO5EPTuuvQicCzwMnAOcXNvh5OH/t5twlnxhC2zbxsoZv+Ou1u2MHgwQi1U2vbynTHR2N6jBUIcUXZVCRH6LTmP7HHCTUmpF2UZlMJDvSf/617uVeIVDh/QDevH7Z6MFi5twyRXwSU4+PoZ3Dd86Rq15Hngb8AhwF/Un4L0RCGRkNgzEaeEAA/I1/UEsBgcO8L5jNzNrFrS3w7JllVOejcndMJnwnAxHKZUE/l1EvpTz+avQ+eoXAHuA2SJSs5qVJhnO5MCq0GUtm4bDMGOG3nbggH3SmVBIL+nrpXX/2uNMDnBsywi7R9txn/9a/zO11u6fAd6Orib3M7Swrxf8XYNTToG//VtdCIi9e1gSv4uVfJ4OnibKHLrYziitxAhnJb8pSsgXSGRjd++VdD6DocK4JcMpub6kiLwILAbuAIzHvaEs2NlFDx6ECy6AT3xCZ0BrasreJx63Yt/thEvuZDb//QFmsnv0GIf9Mym0SlAt/he9kHYP9SfgM18L9x8ehjvu0NESL8RncysfoYvtRJnDIAOM0kIMrV6XFJbuob6rMbkbJhN+bPInoQvU5CEih1OJcpaia8obDCXhZBfdtQvWrYObb06nNc0nV4u0E/BCiBhxmnL65wrv4lYFKksMCAMfRNvgZ9d2OHn493eIxbJTzMZoYpQWBhlgiO6sNLVQQrU2N6+6DBu7MbkbJgueNXkR+YuI7HPZLiJyu4jcWJ6hGaYybnbRwUHn7HJKQXMThLBmCPlL64okJ7Obo9hLkHHchZEfT/xq8Bi6AOQvU+/rTcBbOP1u9r+lFRSXSYwmNs88h+7OQ4RD2RuLtpGb+q6GKUbJy/UGQyVwKzM7NJQvECwCAdjwvm/wCXUdXTyInUYpBNnNybzAsSQIU19C3I2H0ZaxV4BjqnTOUn4bu8mTIsQ4ARJZnwYC+dET4TBELnsNA3cvpqVV+S45bIvxqjNMMRyFvFLqCaXUbqXUSRnvvbTd1Ru+oS4pQ4FuN7voGWc479fTA91/uZ1V0scifo9y1CatW9+Lfb0eluv/hBbwSeB3kMriV3m8fHc/E4EkH+E22tg3kWM+HBZaW+GII+wndWW1kbvNHg2GSYibTT5A9n9v7nsn6uGJaKgVua7J27bBLbeU5LnkMQAEgCuuANZ0E33wJdbGP4hgF+jRaLfoMPBWoBn4DTCvpqPJJ9MG726LbworVi57nJXbehlM/gubA28ksqh5QsY6lV0tm43cmjGY+q6GKYLvevL1jgmhqzF+Crq7MDQEixenK8WFQjrX+PbtcOGF2jHajr4+GOh5lq43TWefHEHSVsg3Gkng34CPAqfUeCxuCApJaQKZOkGGP4SCSy6BlSuNXDUYykVFQ+gMhizK4NgUjWYLeNDhcZYTdHe3876bN8OKa47jFY6cBAJ+CHgS/W/6FepbwAMoTmcnffw3EYY4mcfJXfwT0ZEROVFrBoOhQhghbygvZXBsWrEiW8BbxGKwcaNeXW1uzt8eCun4+TVrQMTPknw9rmZtBN4BLK/1QHwgPMEpDDDIOpbyFCdh94hJJqtffr0MbiIGQ0PiaJNXSl1S7EFF5KZi9zU0OAMD2gafmy7Mo2NTNKqFtBO7dulJwEknwaOPpoubWMv54GTDd7IVS4HtteA3wHvRNaBuqO1QfKEYI8wKvkQro4jL71nNqLUKuIkYDA2Dm+PdDfhXcSzPGyPkpyolOjYNDro72o2Pw0036T6hkJ5DnH46nHnqKOz4E+tufh3JpNd8TEIThxlnGvUj4O8BLkDnoP811QuVKxdBbqaHTv7kai6pZtSax/w3BsOkxE3IX1a1UTiglOpATxiOQU8erhORb9d2VIaClOAKPTRkX3o0E2sSYOWvPzw6zpp100iyMOVNb5fxzj5mu74EvKBt7/OAe4FX1XY4RZIkxJ94LU6/e7Wj1kz+G8NUxlHI10nmujjwaRH5o1KqFXhAKXWviOys9cAMZSZVNKT7L4t4UH2AuHjLuByPwyO7w2QLk9xQLichXk9L9NZY7kR70x9V2+GUhEolGQLre4WCQjCkOP10WLSoulFr3d16iT434MPkvzFMBera8U5E/ioif0z9PQLsAv6mtqMylJ2MoiE9L3yDhNinZHCuJ2+vpRemXgT8WuB84DBwJI0l4AtZ9PSEa/4Zisce08J21Sp/Ar5UpzmT/8YwlfFToKamKKVOBF6PjisyTCYyjKZruJgg8VThmDSzZ8PIiFtRmkZlDdALvBldeGZabYfjg1DIMpkUWhFRTJ9enOZeDqc5k//GMJXxJeSVUjOBTwLnoTVqm0AmREROLsPYMs/bAvwI+CcRyStNopT6OPBxgOOPP76cpzZUgwyj6RCRPAEPuk78JMvbBPwA+Ad0NrufAjNrOhpn8n0cjp52gA8d8XN6FjzMNfeezPrYO9hPC4eZTu4CYSBQ/NJ4uZzmTFU5w1TFs5BXSh0J/B5dAms/cASwD2gCpqe6PQvEbA9QJEqpMFrA3yIiP7brIyLXAdeBznhXzvMbiiNlYmdoSNtE3TSn6BnnMbi1lyFZwH5asdMME4l856l8CoXJ1cvyPOjglY8C7wR+Qn1XaM4W8ArhZ+PvpPuF++Hn0N3UBEvfR/Sxw3TuWsu+8WxnxtbW4pfGS3Ga83MPGgyTFhHx1IBvoD2CLkP/ByeBz6f+fiOwFdgATPN6TA/nVGjv+v/f6z5nnXWWGGrL8LDIrFki4bAuIBoO6/fDw+ntfX0iCxeK9PaKtLUmJMxYqthoQiAp6eKjus2bJxIKSd7njdseErhE4FAdjCW35f/+mS3MmPRxVfaHzc0iw8MyPKyvaXu7br29IsObnklf8L6+9I3ggb6+9H00cf6w/ryUe9BgmEwAW8VJjjptyOsIjwK/y3ifBD6f8b4deB5Y6fWYHs75FrQa9hCwLdXe7baPEfK1x+3BnPvwDQS8CZ6uLpGmploLv3K0DQWFaH009zFG2JT/oZ3kLVHaFrt7sZMDg6ERcRPyfrzrO4AHMt4nybDJi8gLwHrgQz6O6YqI/F5ElIi8TkTOTLWfl+v4hsrgtsSaa2MtFBNvcfiwl+X6euc/0OVib6/1QDzgbNoIM04Em/VyuzV0N6O6B4otM2ti4w0GjR/Hu4NowW6xDzg2p8/zmBC3KY9bXLLdw9cdARSPPpJAz0nrya7uFQH+HfgS0AN8oKajKYVwWGiJHaCHm+nnKoaI0M1mBkLfosPOu64M0rYYpzkTG28waPxo8lG0Nm+xEzhbKZV5jLcAz5VjYIbGxS0uubtbh155Rwt1ncmuUQX859AC/qNoh7t6ilyVnFdnZs6E5csV67/7DOdxL9/hk2yhm+/wSTrjDxLt+Vz+TmUoWFQMJjbeYND4EfIbgMVKTaQk+SE6wfbPlVJXKqVuRzvgmeX0KY7bEmtPj/aUz0UL/sKCxj+VOKYftgFfBz4BXA91V/42NzugPaEQXHaZ1qiv2fha9nHERG76JEH2cQQrrjkuf8caSdtil/ntMBXsDI2M0jZ7Dx2VegNwOfAVEYkqpULoVF0XZHT7A/BeEXml7CP1yIIFC2Tr1q21Or2hAJdeCjfaJEwOBLzY5617tdE0+i3AAhpv3JpgUAt5KyXtbbfBnj35/drb4fnnbQ5gxbI1YCaa3GQ81hzFVLAz1BNKqQdEZIHtNq9C3uXgZwGnAE8BW0TEoytVZTBC3h/W83fjRi1klYKzz67Mczga1SVi7TT5yUUS6AfehS4Z26gIzU2CECCZ1NntwmH9avfYcBTyDUx/P6xenW/bX77cJNcx1A9uQr5k46CIPEC2172hQbC0lJGRdEU3gJ07y1xvOzWTGFy7iETig3jTaAsvIdcnCXQWuxvRVeQaWcgrxmMqS6DHYs41BJYsqc6oqonx0jc0OnVdoMbgD7+2Qyu6KVPAg37vI8rJfUxDz3LpKRs55up/49oXLsBJcAeDmf5ZdgK+1rZ1L8SBi9EC/otoj/rGxk5jF9HmFUvYKwVtbbByZXXHVg1q5DdoMJQN38v1Sqn3AmcCc4CwTRcRkX8ow9iKYqou1xdjO4xEYMsW52NaIW+ljKnzlEM5aU7tNfSlS+HYY+F734NDh4o/Z+2Io1NE/Aj4GvB/azucChIOw7JlOl1tA5rZfWFs8oZGoCzL9UqpE9BVNF6D+zqqoNcrDVWkmEIedrHEFuXQVgYHYWS8Cfda75p774UdO/TrI4+Udt7aEAReDfwX8KkajyWT8pg9AkpIiiLMOC2BOCuveIWObhtv+kmGqWBnaHT82OSvAl4LfB+dT/4ZtPpiqAOKsR0ODGjbe65NPhQqT5TTxo1MhFllky90Dh7UD9KXXirtnNXnEDoH1Inof5F68yNwD5FTCo44Avbts987HIYZ05JcMPY/7Bo/hQhDDCS+RceSV6aMOmsq2BkaGT9C/hzgHhH5WKUGYyieYjJ8ZWoplnd9IKDDpErVVqJRePhh8KpJWhOSYL2FkbtyEPg74BFgF/VZKtb99w8G4eKLdf6Cyy+HJ56AOXOgsxOGh1Oa68gX6Lj1P5koMBkHRsP+671WGVOFzmDwFye/F/iuiNR1zihjk68P22F/P1x7bb5TnxuzZ2sHrieeqNy4ysco2nP+PnRd+EtqOxxbdFnYQiUqCvpeODlvlOq0UUHq7f/BYKgkbjZ5P971f0Av1xvqkHJm+CoHQ0P+BDzoBCuNIeD3o2PgNwI3U58CHkChEFpbXXooiMwfyQrLiA49mx2lccZ5DediXmJdHINh0uBHk389+qn2MRH5n4qOqgSmqiZfb9glEZk89APXArcBS2s8FncCJLm4N8DPfmafpS4YEJ5sfR0dBx+BWIxo6CS6Eg8wGjySWFwRDgktgYNsj7+GDhnW8XMNoBY34OKDwVA0ZdHkReRB4O3A1Uqp3yqlvqmU+rxNW1GugRsaFytl+eTkK8A91LuAB0gSYNcu+PCH85XxQAB6Th2aEPAAg/FPMSozicW1HT8WV4yOhxlMflqr/cGgjp+rYwEPJr7dYLDwE0LXhn66HYUuir3YoasAkzAthsEPlvng/PN1aJw/MnPU10vmuxeAFegQuVa0H2r9Ywk2K5Ii10a9svnLWcstQ0SI0ZR1jBhNbCaiPTPDYR0gX8cCHpy/r6lCZ5hq+LHJfwt4G/AroBc4N/U+tzXG089QcTo64O67YdYsrTV6R1FfAv6vwFvR9vc/1XYoPonHoefc5+gY7Gd7x3tYfvoGIl1jaZ+Ns0/KUnm72UyY8axjhBknQioWs0Fyutabj4rBUCv82ORfAB4VkbdUdkilYWzytcEKV7rvPpCxcQIvvcCi4P8ysGQHXHEFK645jjVrtDJomXWdCp3UF8+g563PAHfjvIBVn4RCwieC32VV8so8N/MoHQyuGGFozaN0JzcxIF+DUDjbJs84LYyynS46eNpUZzEY6pCyVKFTSo0A15gQOkMu+YVutAYeYpxWDrC97Ww6dvycoWc7smKxn3vOOQlLfTCMFvAvAOuBv63tcPLwVnp3JqNcxg8YYHBCUEeX/V+67lqZXs4OxGlRB9ne83W44goG1xzH5o1jRB6+iYHEV+mIP9kQDncGw1SkXFXoHgTmlmdIhkYmN8nIyEhuoRstdOI0MQoM7v8EAyuuY0mGUGmM1LWH0OUZfgXUo8eWt2p+B2hhNcu5hY9ojTz2NIPrO7NDzJIhRsNHMNi6klXdsKoboBmi74LBh0xOV4OhQfGjyZ8L/Ax4u4j8vqKjKgGjydtTruxfdklGkkn3GvEzGWXutL+yK36q79j52vA80I4WogmwTc3beIQZZzmrWRX+NJFZj7LlhRPz+tR7iJnJYmcw5FMuTf7VaCH/G6XUrega8raLrSJyk+9RGipGrmDetq34evF2SUassqP280WtSe44fHKpX6NK7EIv0X8C+AKNKuCDwfyJl/aS74aWFrqXzGbbrf7SINeact7HBsNUwY/P8w3ABeiJwSXAt9H5PDPbDalXQ43JrC1//vl6Sb0c2b/sCuFYOe9DE1NGyXi1lpQD1H9N+B1oxzqhdjHwXn4j9z6hELz//TZx4owTOepxWL+egZWttLSk+9QixCzzHu3v1+/dMFnsDAb/+NHkP0r9P6UN5Gs8dhQbCeVUCMeqL77xe4+QPHSYxzmFQ3kFW+ohHM6JB9FRoc3Ab4B5NRqHl9/IvU88DnfdBdOmASJZXvID+/8NlrxCx/btbN/eUbMSqsVo5cVUWjQYpjqehbyI3FDBcRjKSK7GY0exS7N2SUZmzNDbhoZg0SnP0bPzX7k88R120En2YlG9xL3nMorORT8TLeAbxbTgzPi4vj4Xzn2Q4d1xXSKWQTriT09UkOtYtapmkXBuWrnTmIqptGgwTHX8ON59H9ghIt+q7JBKwzjeOefttvATCWXn6ARMaIDz58Mdd+h68LEYhIJCIiEEiJOgifoV7Ln8FHgdcEKtB+IbZ38IaFbjPCYn69C5TGrsYVdMbnlTWc5gsKdcVeiWoV2ODXWOXd5upWDePH/Zv6yH6urV+oG8erV+D8ZwiVsAACAASURBVFrbGhrSS/SWgAeIJxSCSgl4qG8Bfx+wLvX3e2lEAQ/62joxJiEG1WezP6wD9dd3bvlo1DlrnxHwBoMzIuKpAQ8D3/Pav1btrLPOkqnO8LBIW5uI1u/SralJZNOmVIe+PpGFC/Xr8LDtcfr6RMLh7GOEw/pzi4UL88/TGO1XAjMEOgVidTAePy2Z9T4UEgkEnPtH2KQ7WRdw1qysa+7xdigrw8N6GNb9ZTOsIjsbDFMPYKs4yEQ/mvytwBKl1KxKTDYM5aOjAy64IF/DGx+HxWcL0c5356vnNq7NQxvHCjo6dXfrcK3G4hfAe9C5ne7Fn/9pLciMVsg3f8Tj0NSU21cTYpyI2qrtKjbLOE6rNYU83bPw6yaPz9zyxq3eYCgeJ+mf29Cpv+4CtqGfkMd43beazWjyGmcNOyF96ursD3PVcxGR4WHpa75OwozldE1mdd20yZ/WWft2l0CTwJkCL9bBeLy2hDQHxmUpP8y7Jtm/ce7fCdnEQpGuLtv7xMtqjSvV0LKdbuZIpHznMBgaGMqkyR8Gzkd7J90JPKuUSti0hshpNtnp7nbaEmBzrn+GXRzS4CADia/SwuhEVbIw47QEDk0430WjcN55bqPI1zprz/1AF9qL/ugaj8UPAZJJYSYHmMFBsjV25fK3sIaL4eGH7VdrvISluWnq1dCyTXF4g6Fo/Aj5jWhPpQ2pV6e2scxjNBTBwAA0N+d/HlJxvXybid0Dc2iIjviTbKeL5awmwhDLWc320/9+Ykl1xYrCBWYCuMTxVZUDqdf/AH4HNJ7VKUYTuziDC/gJiqTHvYLpWvA2greg/Cy0nl+N4PWBAWqeucdgaFD8xMm/tYLjMJSZjg7YsAEWL4axMf1ZKAStMwMMcC0cDGfHIeU+MFNByR2xp1nFP+rPwmFYtHyiy/r1hceRJETtNfqbgM+h56enADNqOBYv2GvpVl33ISKIx3S7ioSuBe8geO3yHmTdDoUC2qsRvG4Z8GuVucdgaGSc1vEbtRmbfDaW53QkkuE5bfuhzY4FbK3t7bW2U3tp3xVQAucIjNbBeLy1XG/5sIrJLPbIMHOkj6ts7PJ2LSltvCzDzHE1tLveDoXs4cbz3WCoObjY5D0nw8lEKRUGTgeORBep2SUidbEua5LhlBErE46D9vTBD8K6dS7715zvAFeis9n9GJhe2+F4IBwGSQrxRObKh9A57XHuPnwOHTxNlDl0sZ0RWohP5CPQKzWJhJbCIDQxzn0soju8rfisMf39eok+V1Nfvjydmq7AfWIwGCqLWzIcX0JeKXUEMAhcDEzL2HQYuBn4rIi8UsJYS8YI+eoQjUJnZ2GbfO34EbrIzHuB29E56RsBe9NG5OjdDO2bPyFsh1jIYu5jjCYgYJv1LhyIs/zoH7Pqoo3lrS1s0swZDHVFWTLepQT8H4CPA3G0g93a1Gss9fnvU/0MdYTXMGYv/aw+r3+9nYAXqJsaRu8CvojOaNcoAh4sj/hMwowTmf1ElvPZGtVLkgDWv7DdXD2WDLH5xIu0xl2sQPYV0G4wGOoOp3X83AZ8FUgC/w0cmbOtDViV2v5Vr8esRDM2+Wy8mky99LP6BIPuduDa2rO/J7Cv5jb1UpoiPmFzDzOmbfFd78kyni9sf7LgcXzFuxsMhoaFMsXJfwDYJCJXSs6SvIjsE5F+dBDyhSXPPAxFEY3CpZfCMcfodumlOszNyTm6UM35V17R+1usWKE/SyTcRlErL3oBVgD/gLbFNyrChfwkO2wxtICORSdq7TlVNKD7ohPzQt9ymTHDRJkZDFMdP/k8T0AbOt3YAHyq+OEYisXORn7jjRAI6BDpTGIx2LgxO3TKDhG48UaBTUNcoa5lzSPfQ6Qec9gK8H+Br6OF/GdqOxzfpO3wTWF4NHEax8rzrJOldISftw1xzA19y0Upndq4WDN8buVBszpvMDQmfjT5AxSuQvcq4GDxwzEUy+Ag7N+f/3kyqQV9JuGw/rxQzXmN4uZHFrL44WtJ1Iu5PQsB/hkt4K8ArgOPMeT1QbajXTyheCj5Oq6WKziRp5g/8yn63/cXomRL2UxT+cyZNkcV2LXL4xAylnSil66gqzNZWi57g8FQN/gR8luADyqlTrXbqJQ6Gbgo1c9QZYaG7J2vQGt1ucnClPIi4DVJgikvbifhWUvp/yLae/7/oN1F/NzS1UJs/k5i50mfXnUJkCTIw68cx7W3tNoKWmv1/rLLSsj6mpPRbvCmYxndFze1YAyGSYKfJ+LXgRZgi1JqpVLqHKXUfKXU25RSX0QL9xbgG5UYqMGd7m77uuJKwSmnwKxZ0N4Oy5ZpDfDss+1rzjsTANtUqvYhX5UnmWrtwB+Bb9VoHG7oaIOWFsWRzaPM5kXmsYsuHqSdF/A63njc3o/CioAoKetrTka7IVlALCP2HsqfpdZgMFQPv3Hyy4FvoyvSZW1Ch9H9k4hcU77h+Weqxsm7xa2HQlpQZIY4Q37487Rp2vnOHkvzDFB7YZoAPoqeU15N7cdjh/V/pQgTo6VpnO3NEToOPwaxGP3qalbL5XkC1Y2uLhgetg9ZhyLz0UQiel0+RT9XsZrlWeMKh+H00/X9YWz0BkP9UZY4eQARWQ2cBnwe+Am6lNdP0G7Np9VawE91LrgAZs/WD+Ojj4aTT9afx1N1ATOXXu3Cn887z0mbt4R7PQj4GNCDzkf/amo/HicU1thihBkZD3Pu6I85Jvgix4Re4rmT3sSM1lCW9u2GnR9F7vVMOd77C4vPqVAzwCAtHCAciE+cNx7X9n1jozcYGo+i0trWM1NRk7dLSjZjhn5vF+7W3g5bt2YLgmgUTjqpUHgc5C/PV3O5fhxYhg7y+E+g0eLDsn8rpeC00/SkbNEiOPdcuOiidEEhi1AIWlvh+OPTWnsmkYgW7hbRqA53tAoILVkCK1c6CH6bmyc6Yx6DF/wvm3e1cuiQFvDxjALSuVltDQZDbXHT5P2E0BnqFLtCYfv3OzvivfSSfq5nJi5bscKLgId8gV5NTboHLeC/BfxTFc9bDvInQyLw2GPQ1pZeAn/sMX09N25MR0YsWqS3Dw7Czp3uBd+cQinvuAN27LAR9DYV3joGBljV0QroY2cKeDA2eoOhkfCtySul2oEF6ILctu7WInJT6UMrjqmoyeeYVT2RqY151+Jrzc+AKDpUrp6wW81I2+S1P4O7ZayzE+6+232Z3Usa+f5++M538nMjKAVXXulf++6/dITVNzZn2+gZZ3nvGKtuaPV3MIPBUBHKlbs+rJT6HvAM8FO0UfQHOe2G1KuhiuSYVT2RqY0NDuYLBY1Q2/A40OkZfpH6+z00hoCHZsbo5QYiDNHJDhTuM6gdOwrbur2kkR8asr+WIsVp3xM2esYBLeBbOMAAg55rIhgMhtrhZ7l+JXAZsBu4Ba1SxV33MFSWVGqygfue5JbAWkZD04nFVaoimbvtPBCAp57SD+f77nNe2rfbt3qMoAX7/cDjwPE1GIMbzr9LG69wAx8FIMocOtnBPtoc+4NeYl+wQNvlcz3Yc7PQrVtnr/V3d8MDD9hr8p7i5nPo2HkP27mBQQbYTIQImxlgELadmbWqsG2bzsBnatcYDHWGU1L73AYMAw8D073uU4s2ZQrU5FSU2RR8s3QGdsjM6QmZPi3huRhKOCzS3FyoX1KqX3jmFYE3CQQFbqvyuUttSenl+zLMHOnjKlnIJunl+7KUH8psnhdFwvX3zC0O5LXIkNW3rS3/mG1t9v1z9+3rE1m4UL8OD4v+wzpxxgD7On9n97EpiGMw1ABcCtR4Fp7omvH/5bV/rdqUEfIZD99h5sgs9kxULiskROyESiBQWHBVT0juFVgoEBJYVwdC209LShOHZVPwzVnXJMyYtPGy9KobZL7aJUq5/56ZAtNBzjoK1OFhkd5ekfZ23Xp7vQl424nEpmdsNyzsOmw77kiknDd55bGd2BgMDYabkPezXD8MmFrx9cLQ0ISb9SADjNBCPOUcpYsLCgESJD3kcY/FdEa8l19261XN5fofAtuBHwPvreJ5CyEU+h0CAcXt1+1jzbe/wuiOVmKpvFExmthHmJvl4lQdeHcyfSYyLrXt9lw6OuCGGwqeIgu7CI3RURhccxyrcrzvGRige7CZbQU8/eudXEdGY3IwTEb8JMO5AViilGqr0FgMPoiecR796moibOI2PjQh4NMoZrGHdp4jUMDpKxz277hXGST1uhx4iPoS8KAFvLj2CATg0s+0szGweELAZ+7vRcBDtsA84wz7IkPlFKiuEwmbTDslpdKtExwnNiZPv2ES4UfIfw34PfCrVL56o9XXiGgUuu74IqvlcrbQzR5mky98hON4jq0spI39roI+Hte57EM1zZrwV2AxsAMtTOfVcjAuuGvy8bhODVyMp7mVbTBTYEajOsY915Gu3LXi7SI03CYSXjz96x2/KyQGQyPi57Fu/Tso4FcAyiEHqoiYJDsVZHAQRg8GMmKX7eZqisDsWXScfBzb5/8X//ziZ1n3c5uapGgt8cEH85OeVI+ngXOAZwFXm0EdUHjJPh6HvXuzP9MRD877zJwJS5fq7HKZuef7++FgTvHmUmrFO5Fbn96LZm4p+I1Kd7deom9kk4PBUAg/wngjhdYqDVXBTgPJJRSCRR/ugFVDdADH9qcL1eSSSMDu3RUZqgeeQgv4PcAvgTfXaiAeKc434bTT4JFHnLcfOABXXKEFTyZ211rER614j9gkvpv0hWiKmdgYDI2GZyEvIm+t4DgMPujutte8LW1RKS24b7tNLx2vXKmFhbOmXlg7rQzD6CX6/ejFoYU1GEN1aGuDefPcBf3ll8NDD2V/Vm5tMzfePlOQN7pm7pepOLExTD18VaEz1Ac9PfYCe8kSCAbTAU179ui85Z2d2nnLucJcrSq5taM1999QfQFf3UWp+fP1tbC/Bponnsh+H43qSVoyaW+v90t06Fm6Tj3I6qtjuqLctTLlK8oVXb3PYGgQjJBvQK5xKOi7e7e93Xf/fv2a66GtqYWAfwRte58G3Aa8vgZjKP/3DoWgyaU8fHc33H8/NDfbb587N/23Fd516616VUYpPYFbtqxIB7dolMHFdzM6FkqH9cUVo6PC4IoR2/y0Jm2twdD4GAe5BsQqIZrLo4/aC3kRvc9558HPf17ZsRXmIeAdwJuAO2s8Fi/Yr3ScfLIW6sPDMH06HHssnHOOThGcu+QOaRt6dzds2ABvelP2tVIKrr8+/T43vMvyrv/jH/WrtexuVatTSkdIOC43Dw4yNHZxVqEZgFhMsXnNoxBYnRUsHl3/J7qWHGdiyA2GBscI+UmEm/f2iy/Cr35VvbHY80fgXGA68PUaj8UbCkFshPzs2dk13C36+7VAd7OhWxr95ZfrJfq5c7WAz3S6c3Ku3LEDXvMavSpz4EC22WbnThdBPDREN6exjTPzKspFkpsgkR0sPnj5Y4yOHmcbQz6V7PYGQ6NjlusbkLPP9r+PCIyPl38s3tkMvB1oAe4DTqvlYDwiNHPYpoKccOiQ/fL1wICOYbds6ErZx7R3d2uNf3RUv+Z61btVFhwZ0SaYXL+MeNwlmUt3NwOhb9HCaHZFOXWQAfladt9YjKEnjjYx5AbDJMAIeUMVSAIfB45CC/i57t3rAr0scpgZE2mC058rdu1yLw2bKeSLwcoo5zg6h1UbR0E8MEBH6ytsDy1gOauJsJnlzTey/cIv0RF+PrtvOEz33Jd8JccxGAz1iRHyDchvf1vrEfglAPwELeBPqPFYvGBJUJXxKmTa55205sFBnbzGsqEnk3pZ/fzz/TmwWeFd06b5G7mjIE4dsOMT57MqsoahvptZ9di76PivT9nmpx24/tSGT1trMBhAiZshtwFZsGCBbN26tdbDqCjTp8Phw7UehRd+BdwBXEUjzCcDSlAkOTKwnz2JWZ72iUSybfORCGzZ4tzfEpZeHdguvVSH3uXS1KQnEJlL9qEQtLYW4RxnefHlBIs7fGwwGOoMpdQDIrLAbpuj451S6vhiTygiw8XuayjMzJnlEPKVjo9fD7wfnYN+BKjnukZCe7vioosUAwNBBgdnce21uTbvBAHIqupnpzXbJa/JxK8D28qVOne9FS8fCGhBfs89sGZN2rs+EIBFi4oUxA5ZcKZachyDYTLiqMkrpZIUlzGkprnrp4Im/8EPwrp1xe8fCOQXPCkvdwEfBF6LTlU7u5InKxHtO3//JjXh/DY0lBvipv9oDY9xWJqJxZWjRp5bvtSJ3BUAN4xGbTAY3ChKkwduIl/InwScDewDtgHPAccCZ6JVtfuAJ0sdcCZKqXcB3waCwHdFcl2BDX6YPh0uughuvTFmUwq1HPwI+BDwBuAe4MgKnKNUrFUM/aqUsGRJWmCvWaMTz6Q1eS3UP7BsGq2t7sI2N1XqoUM6pC5zVcCvA5vRqA0GQ7E4CnkRuTTzvVJqHnA/8C3giyKyP2PbEcAXgUvQbtRlQSkVBP4bHVz9NLBFKXWXiOws1zkakfvuc97W1KQ1SCdXixkz9Ou0cIJYLET5l+yPQOej/3Hq71rgZooQmgJxxpPp754UlbWEbpfnPxbTwtqL9p0plHM1e+PAZjAYqonfevI7ROTTmQIeQET2i8ingD+n+pWLCPC4iDwhIuPA/wB/V8bjTzrmz4crr9SCxS7FqpXPfiTmkFu1aB5PvZ4L3EvtBDw4CfhwGGbNUpz+2nBen8zQM7+11d2YDHXXDQZD4+JHyJ8N/L5An9+j1bhy8TdAZsDR06nPpjRLljhvW7RIa5HbtsHttzvlqwct5MqlxX8XOB3tbGcdu36YNk1PeiwBe/bZ7kLcilEvV/hYvRZBMbnpDYbJjx8h34y2v7vx6lS/qqKU+rhSaqtSauuLL75Y7dNXnZUrtYe9HT09+mF96aVwwQWVdrADbU25HHgn8NZKn6woZszIXtUolJVuKmjflhlh9Wod8rd6tXtyH4PB0Jj4EfIPAh9SStmWDFNKnQX8PTpBebl4Bsh8tM5JfZaFiFwnIgtEZMGrXvWqMp6+PunosNfmQyFdoa6rM8lNNyZdc9mXh28BfcD70Mluplf6hEWxd69oQfadOF2dSZ59Vn/ulpWuXrXvcpFbACcztM9gmApMlZUsz8lwlFLvAH4BJIBb0J70zwPHoJfol6EnDe8SkV+XZXBKhYBH0UnPnwG2AMtE5M9O+0yFELpoFE480UlLF3Qa2aDdxjIyBLwRuBC4FXCpsVpR/MX7hxln7lx49MmmrElQOAzLl42wqvVftWTv7q58rJoVG1et82XglLTHT2ifwdCoODnENuqKnVsIHSLiuQFLgT1oKZLIaMnU5xf6OZ7Hc74bLeh3A/9fof5nnXWWTHb6+kS0/3yt248EYjU4bzLn1Wl7of2zWyS4VSQc1m/CYZFZs0SGhytzEYeH9fGrdb4c+vrSp7ZaOKw/NxjqkeFhfX8uXKhfS/lXmWz3P7BVHGSir1yjIrIOOB7oQa/Vfj/12gMcLyI/8jf/8HTOn4vIaSJysoj8R7mP34jUTtMS4MvAA6n3H6BW1YpPZDcBEuSnckjS3OS8OpVfUc76PKlLrlZr/brG6+Xldi40GCpJuX1I7Eo5T9Yqi74TiovIARG5VUT+RUQuT73eKiIHKjFAQz65ZUmrgwADwArgtloMIIunmEv+Ur3Q3KT40IcDtmVaFQkCDsv7AcS25GrF/utr/JSZCs6FhslDuefE5QyTrXeKrhqilJqllDKPhBrQ01PtMwrwT8A3gCuBWntnpZLY5GXsU7QeobjiipSWGtIafYAEQeJcwhp6mtZOfP7/2rvzMCnKc+/j398sKMhiBBVRXGKUoAJqCGgI7jFoFHHLMRK3IOKGUfSQEOKS+EbfGGJcMImagxLFLGoMBPcFECOImMhiINEjGBWJ4oIiCANznz+eaujpqd5muqene+7PdfXV01XVVfdUdfdd9dSzJFRVwbd7vRg75GrRvvWt4Fem0isXuspR6HPitlSSlVeSl9RR0s8lrQRWkdSFraSBkh6RdGChg3QN3XtvS26tHriQMJLcGOBWWseIcoluaRt6//3Q8uDRR2HU+WJAv/Vc2Oc5lvU7kbsvns+1zx5Gx05q8OXu0gWunbxry37r29KvjHPNVOhz4rZUkpVP7fouhM5u9iX0W98O6G1m1dH8DoTa9r82s/8uTrjZtYXa9f36wcKFLbW1OsK99/2A62g9Hd0YVVXRFX1KK4Pa2vClTdffe9oBX+JmQPFqwPvIM87lpNJqwxdaptr1+ST5G4ArgLPN7LeSrgauSiT5aJnpQA8zK9nVfFtI8n37wqJFxd7KRsIQsZ+L/q6mZRJ8bk3ipDAE68iR8O67jecXpCmY/7I412r4OXF6mZJ8PuWuJwGPm9lvMyzzBt7tbNE1fyz5bOqA4cDhwGeEGvStJ8FvtRXMmQNDh4YR9ZpbjJe2UwzvMca5VsPrkDRNPkl+FyBbIfEawpCzrohWroS4+9GFsYHQceEfCS0jty7SdiD+f4hv4pZQUxOu3hMtDJp7aztj05y21M7GOVeR8knynwA7ZFlmD0KFPFdEn3ySXy9vufuM0IPdQ8DNhLszxZT6P4hqLH1bdkGnTg0TeHMr0GS8WG8FNeCdc6458unJ5EXgOEmdzOyT1JmSdiL0Tje9UMG5lnYZ4fD9Cji/yNsyRD2W1P1ubdVGTv/cozy69lDeXdd4qNrtt4f58xsn8OTx2/OV8WL9gbEwZYoPBu+cK1v5XMnfDHQFHpHUO3lG9Pp+QtnuLYULz7Ws8YSOboqd4DfRhdV05hNq2QCEBN+hUw0cdzybOsSPRX/MMSGhF3JgiYwX622pnY1zriLlXLseIKpRfzXhZmodUAt8SKiCLeB7ZvazIsSZs7ZQu14qZHH9J8BEQm92xR7UJhD1zKkeTI9N/+YGfZ95VQfR+8Re/PnJjqxd2/jKOuGss8Iwu4Ws8O4V6J1z5a5Qtesxsx8RRoSbRkjuic7DHwGOKnWCd/laDXyd0FVtzJBkxSJx7z7X0XNAD269aCkvLNuBTt0zJ3gIHdz07w+rVxeuwrtfrDvnKlneo4uY2QxgRhFicS3qA0KCX0C403JQi23ZTMyuOrRBQ/a4e+OpVq2KH163uRXem3NP3znnWrOcr+QlnSmpb5Zl+kg6s/lhueJaRSiQWQj8CTixyNtrfEsoNVnH3RvP9p4Er/DunHPx8imuvxsYlmWZocBdTY7G5ai5beRfBd4E/gIc1/xwMkoz6lvKJy+1vXuuvMK7c86lV+iRRqopXi8tbrOmVrpbFz0fTBhb6OjChJNGVRW0ayeqU+rz1dTA4MENpyXfG99hh8YnAamksJzfQ3fOufQKneT3JlTIc63Om8D+wG+i150KtN6G53Q1NdC1a0jAZ5wBz96/ks41a6kl3HCvrbVGHdokJO6N77Zb+qL5sA7YdtvQZt67t3TOufQyVryTNCll0jBJu8csWg3sCgwGHi5IZK6AlgNHAO8TBhEspIalCmZhuNfaqo1Mu/dTrv3dV1mwaRM3MIZ5DGRA1QLGPvoNevbskXaNAwfCyy83rIhXUwO9e0P79j44hXPO5SpjO3lJyddT2RpnG/AC8G0ze70w4eXP28mneo2Q4NcATwCxTSmLpJ4+LOJhjqMnb4VJ2caBxduuO+dcPjK1k8/WhG6PxDqA14GbCD3fpdoEfGhmnzY5SlcEq4FDgfXAM4Ti+pZUxSL60I8FLKBfSPQ5tHdL3J/3YSWdc655MiZ5M3sj8bekHwEzkqe51q4L8ANCot+vYGuVQrF8bqpYQ0duYCy3cknO7d287bpzzjVfzhXvzOxHZvZsMYNxhbIAmBP9fRGFTPBVVdC5c7hHnqymJiT/uMYVdbRjHgNCgq+q4s2n/snovrMYsP/6Zvc975xzLr18OsM5VdIzkmJrTEnaWdLTkk4qXHgufy8BhwMjyTY2e24aJu2HHoJFi+D888N98169oFs32G47OPlk6LVnHdCwanwtGxjwuddCgt+4E/2W/p7bFx3Miwu24vZf25bx251zzhVUPk3ozgW2NbMVcTPN7G1C+fC5hQjMNcULhJ7sOhM6umnKgDOW8rex9dbQpw/MnQtDh24pSv/LX+Ddd0Nf8u++C1OnwspV7ejSKdSuh/DcsUsNY4cuhfp6btg0hjV0pI52ANRtVLP6nnfOOZdePn3X9yH7WPEvAsc3PRzXdH8FjgF2IFSy27UJ6zDCVbg2P6rZRPv2VTz8cOOKbzfcsKUGPITnjz+Gk0+uonv3qqjSXE2oNHfy41BXxwsM2JzgE5rb97xzzrl4+ST57YB3syzzPtCt6eG4prsT6AE8DezcxHWILYU7oYneJmr45JOQ0FMrwsUNKmMWivSXLUs5KYgavw+sm8fLHNAg0Xvf8845Vxz5FNevAvbKssxewEdND8flL3H/+05gNrkl+ExV4xNX8Vts3Bh/pT1wYKKyXcraLab4PeqcfmzNL+jIGmrZAIQe8LzveeecK458kvxfgaGSvhg3U1Jv4ARCpnEt4hFC5zbvArXA9kXbUtyV9tix8X3M19fHnBREjd97nv8NFvQ7i1F95jCg33pGjZJ3cuOcc0WST3H9BOAk4DlJPwYeA94mXDoeA1xJqOk1odBBujh/Br4J9CX7YUztIU9IoUb8mjWwaVO4Wq+SUW+QeiW/1Vbp+5r/9rfhnnsa9jWftvg9qrHXE/Am8M45V3z5tJN/EbiQUHX7F8AS4OPo+cZo+gVm9kIR4nQN3A+cChwIPEWoLpFJ4+J5M9hzT3j11dAcbsAAOONM0aWLGrSB32ormDUr/ZX2tddCly5bhoj1oV+dc671yOdKHjO7U9JzhGQ/ENiWcA9+LvArM1tS+BBdsqlTpwLfIgwX+wjh3Cq9ajZGdeYb3muvqQmJPbVnuTffzK87We+C1jnnWq+MA9SUo0of1yILlwAAHPlJREFUoGblypXstNN4whACHbMsbUnPVQ2mb7WVePVVT8bOOVfuMg1QU+jx5F2RPP7449TV1dG9e3fCmPDZEjxsuXJPPcyiqipcfXtPc845V7nSJnlJu0aP6pTXWR8tF37bMHHiRIYMGcItt9wSTcm19CX9kLTr1sHtt+NdyjrnXAXLdE9+OSFL9Ab+lfQ6G8uyXpeHG2+8kcsvv5wTTjiBiy++GICdd9zE2/9p3J69sczz6+rY3KWsj/jmnHOVJ1My/i0hYa9Oee1ayPXXX88PfvADTj31VKZMmUJtVIV964618J/CHArvUtY55ypX2iRvZmdneu2K66233uK6665j+PDh3H333dQktWt7+23IfhWfG+9S1jnnKpcXq7dSu+yyC/PmzWPvvfemurrhaHKFahDhbdqdc66yee36VsTMuOKKK7jpppsA6N27d6MEXwhVVbDDDjBqFN6lrHPOVbC0V/KSJjVxnWZmI5r43jbLzLjkkkuYOHEio0ePxsxQ3OgvwN57w6JFTdtO4up9/nxP7s45V+kyFdefnWZ6unZZiekGeJLPQ319PRdccAF33HEHY8aMYcKECWkTPMCdd8JBB2Vfb1VV6FO+tjb8/cUvwuDB3iOdc861FZmS/B4pr6sIfdYPBm4BZgIrge7A4cBo4FlgTMGjrGBmxsiRI5k0aRLjxo3jJz/5ScYED2GI16lT4cQTGw4Mk1BbCx06wLBhsGSJdzXrnHNtVaba9W8kv5Z0GSHBH5gy75/ALEmTgZcIw83eVIRYK5IkDjzwQHr27MnVV1+dNcEnDB0Ky5dv6TO+d+8w3ZO6c865hJz7rpe0BJhpZhdkWOZ2YLCZ7VOg+PJWLn3X19XVsWTJEvr27VvqUJxzzpWxQvVdvzthxLlMPoyWcxmsX7+eU089lUGDBvHOO++UOhznnHMVKp8kvwr4erqZCuXMXwfeb25Qleyzzz7jpJNOYurUqVx//fXstNNOTVrPmy+sYHTfWQzo+Aqj+87izRdWFDhS55xz5S6fJH8/sL+kP0pqUCkvev0HoG/07GKsXbuWoUOH8uijj3L77bdv7os+X2++sIJ+B7fn9kUH8+Kn+3L7ooPpd3B7T/TOOecayCfJXwXMB04B/iVpuaQXJC0nDGBzSjT/mkIHWSkmTpzIU089xaRJkzjvvPOavJ4bRr7KGtuGOtoBUEc71tg23DDy1UKF6pxzrgLk3K2tma2R9FXgCuAcYE8gMazsa8BdwM/NbEPBo6wQY8aM4aCDDuKQQw5p1npeeL3b5gSfUEc75r3erVnrdc45V1ny6tbWzDaY2XVmthfQGegJdDazvc3sek/wjX300UcMHz6cFStWUFNT0+wEDzDw86uopeGurmUDAz6/qtnrds45Vzma3He9ma0xs7fNbE0hA6okH3zwAUcddRT3338/CxcuLNh6x965Fx316eZEX8sGOupTxt65V8G24ZxzrvzlneQlbS/pfEk3S/pNyvQBktoXNsTy9N5773HEEUewePFiHnroIYYMGVKwdfcc2IMFc9Yxqs8cBmzzCqP6zGHBnHX0HNijYNtwzjlX/vIaalbSCEKXtluzpZ/6c6PZOwJzgPOA/ylgjGVn5cqVHHnkkbz++utMmzaNo48+uuDb6DmwB7cu9KTunHMuvZyv5CV9DbiDUJP+ROBXyfPNbDHwCjCskAGWo5qaGjp37swjjzxSlATvnHPO5SKfK/nvAe8Ah5rZx5IOiFlmIXBwQSIrQytWrKBbt25069aN559/Pud+6J1zzrliyOeefH9gupl9nGGZtwij0rU5y5cvZ9CgQYwcORLAE7xzzrmSyyfJtwM+zbLMtsCmpodTnl577TUOOeQQVq9ezejRo0sdjnPOOQfkV1y/HPhSlmUGEoaebTOWLl3KEUccQV1dHTNmzKBfv36lDsk555wD8ruSnwoMlnRq3ExJ5xD6rn+wEIGVg02bNjFs2DDq6+uZOXOmJ3jnnHOtSj5X8jcApwG/k3QK0AVA0sXAYOAk4FXg1kIH2VpVV1dzzz330LlzZ3r16lXqcJxzzrkG8um7/kNJhwK/BZKv5m+JnmcDp5tZtvv2ZW/+/PnMnj2byy67jC9/+culDsc555yLlVdnOGb2b+AwSX0JTeW6AquBuWb2UhHia3XmzJnDkCFD6Nq1KyNGjKBz586lDsk555yLlXOSl3QI8LGZvWxmCwlt4tuU2bNnc+yxx9K9e3eeeeYZT/DOOedatXwq3s0gdFnbJj3zzDMMGTKEXXbZhVmzZtGzZ89Sh+Scc85llE+SXwWsK1Ygrd2yZcvYc889mTlzJj16eJ/xzjnnWr98kvxM4CtFiqPV+vDDDwEYMWIE8+fPZ8cddyxxRM4551xu8knyPwR6SbpWUm2xAmpNHnroIXbffXeef/55ANq1a1fiiJxzzrnc5VO7fhywGPgBMELSAmAlYbjZZGZmIwoUX8n84Q9/YPjw4QwYMIB999231OE455xzecsnyZ+d9Hd30g9EY0Czk7yknwHHAxuA/wXOMbOPmrveXEyZMoUzzzyTQYMG8fDDD9OpU6eW2KxzzjlXUPkk+T2KFkW8J4FxZrZR0k8JJQnfK/ZGn3vuOc444wwOP/xwpk2bxjbbbFPsTTrnnHNFkU+Pd28UM5CY7T2R9HIucEpLbPcrX/kKN954I6NGjaJ9+/YtsUnnnHOuKHKqeCdpV0knSzpJUikaiH8HeDTdTEnnSZovaf57773XpA1MmjSJN954g6qqKi699FJP8M4558pe1iQvaQLwOvBH4H5gWXS/vNkkPSVpcczjhKRlxgMbgSnp1mNmd5hZfzPrv/322+cdx4QJExgxYgQ33nhjk/4P55xzrjXKWFwv6VvAGEJluqWAgF7AGEl/M7PfNWfjZnZUlu2fDRwHHGlmqbX4C+K6665j/PjxfPOb32TChAnF2IRzzjlXEtmu5M8lXEUfZWb7mtk+wNeBegpQgz4TSUOAscBQM1tb6PWbGVdffTXjx49n+PDhTJkyhdraNtH83znnXBuRLcn3Baaa2YzEBDN7CpgK7F/MwICJQCfgSUkvS/p1IVf+2WefMX36dM455xwmT55MTU1eA/I555xzrV62zPY5QjF9qqXAsMKHs4WZfaFI66Wuro727dszY8YMOnbsSFVVPh3/Oeecc+UhW3arAupiptcR7s+Xlfr6ekaPHs2wYcOoq6ujc+fOnuCdc85VrFwyXFEqvLW0+vp6Ro0axW233ca+++7rxfPOOecqXi6Z7hpJ18TNkLQpZrKZWavKoJs2bWLEiBFMnjyZ8ePHc+211yKVXUGEc845l5dcknG+2bDVZc9LLrmEyZMn8+Mf/5grr7yy1OE455xzLSJjkjezirhhPXLkSPbaay8uvfTSUofinHPOtZiKSOJx1q9fz3333QfA/vvv7wneOedcm1ORSX7dunWceOKJDB8+nPnz55c6HOecc64kWlUFuUKor69n6NChPP3009xxxx3079+/1CE555xzJaEidQlfMp06dbK1a9cyadIkzjrrrFKH45xzzhWVpJfMLPaKtuKu5D/99FPuvfdeTj/99FKH4pxzzpVUxV3JS3oPeKMZq+gGrCpQOG2N77um833XNL7fms73XdO1tn23m5nFjrNecUm+uSTNT1fs4TLzfdd0vu+axvdb0/m+a7py2ncVWbveOeecc57knXPOuYrlSb6xO0odQBnzfdd0vu+axvdb0/m+a7qy2Xd+T94555yrUH4l75xzzlUoT/IxJP1M0lJJCyU9JGnbUsfUmkkaIumfkl6T9P1Sx1MuJPWUNEPSPyS9Ium7pY6p3EiqlvR3SdNLHUs5kbStpAei37klkg4udUzlQtJl0fd1saTfSdq61DFl4kk+3pPAfmbWF/gXMK7E8bRakqqB24BjgH2Ab0nap7RRlY2NwOVmtg9wEHCR77u8fRdYUuogytDNwGNm9kWgH74PcyJpZ+ASoL+Z7QdUA6eVNqrMPMnHMLMnzGxj9HIusEsp42nlBgCvmdnrZrYB+D1wQoljKgtm9o6Z/S36+xPCD+3OpY2qfEjaBfgG8JtSx1JOJHUBDgH+B8DMNpjZR6WNqqzUAO0l1QAdgBUljicjT/LZfQd4tNRBtGI7A28mvX4LT1R5k7Q7cADwQmkjKSs3AWOB+lIHUmb2AN4D7opudfxG0jalDqocmNnbwATg38A7wGoze6K0UWXWZpO8pKeieyqpjxOSlhlPKFKdUrpIXaWT1BF4ELjUzD4udTzlQNJxwLtm9lKpYylDNcCBwK/M7ADgU8Dr0uRA0ucIJZV7AD2AbSR9u7RRZVZxA9TkysyOyjRf0tnAccCR5u0MM3kb6Jn0epdomsuBpFpCgp9iZn8qdTxlZBAwVNKxwNZAZ0n3mlmr/sFtJd4C3jKzRKnRA3iSz9VRwDIzew9A0p+ArwD3ljSqDNrslXwmkoYQigGHmtnaUsfTyr0I7CVpD0ntCJVQppU4prIgSYT7okvM7MZSx1NOzGycme1iZrsTPnPPeILPjZmtBN6U1CuadCTwjxKGVE7+DRwkqUP0/T2SVl5psc1eyWcxEdgKeDIcR+aa2fmlDal1MrONki4GHifUNJ1kZq+UOKxyMQg4A1gk6eVo2g/M7JESxuTahtHAlOjE/HXgnBLHUxbM7AVJDwB/I9zK/TutvPc77/HOOeecq1BeXO+cc85VKE/yzjnnXIXyJO+cc85VKE/yzjnnXIXyJO+cc85VKE/yzjWBpAmSTFL/pGkdo2mtYkQ0SRdH8ZxS6lhcdpLmS1pT6jgSJO0m6XRJl0saI+mbknpmf2fLkLRK0uJSx9HaeZIvougHNp/H2aWO2blo+NbTJf1Z0tuS1ktaE3X7/EtJA1KWT5xMTMywzuNyPQGS9Puk70T3POJOnHhdkWGZrLHmuK1WdUJXSJJOkPQSsJzQpfcE4OfAH4A3ouGRD2riuo+V9AtJMyV9GO3DxwoWvGvEO8Mprh/FTLsU6EIY6jF15KeXGy/uysinQG+g1VyN5Su6UvsT0J/w+XwSWEb4regFnAVcIOk7ZnZXEbb/LeC/CPvSB01pQZI6EHpgPA1YAFwIzCD08iZC99VDgPOAv0q63sx+mOdmxhB6iVtL+FxtW5joXTqe5IvIzK5JnRZdrXcBbjKz5S0ckiuiaIyDpaWOo6kkdQaeAL4I3EXMgDnRAB3fpwg/ztFY3bcREs3+wJcKvQ0XT1I18GfgcOAy4OaYMTuWAksl3QaMB66WVG9mV+WxqWsIve39E9gHWNTc2F1mXlzfCiXuzUlqL+n/SXpN0oZEEWPc/eCk9+6XrjgyKmK8StIiSWslfSJptqST8oxvVVR021nSzZLeiop0/yXpuxned4akv0r6ONr+guh+X22GbWwn6VZJ/5a0MVEUm7wPJJ0t6WVJ66JYrlMY6xlJx0h6Ltqf70uapDCedur2jo7mLY32y1pJCyWNi4svzf/XqAg3qZg606N/ynr6SJoSFZVvkPSOpMmSPp9mu70VitY/iv7P2ZIyDsCUxjhCgn8CGBE3Ip6ZfWhm3wN+2YT1pyVJhBOLTwhXey0q6TvXQdINkt6IPtOvSvp+4vMULXtxFCfAN1KOZeLzKUnnSZoqaVn02fxI0ixJp+YZ2zei2N6QtE/KvK9Gx/4/0WflDUkTJe2Q5y64ijD4yklmdlOmQbnMrC66gBkLXCnp0Fw3YmbPmdkSMyvY8MCSukq6I/qefBb9vo2KWS7vY6Isv8XlwK/kW68qYDqhiPRx4H3gjaauTNL2wEzC2fM84E6gHXAM8KCkcWb2//NYZXtCUV4nwoA0Ak4CbpJUY2Y/T9n+LYQz+P8AvwU+A44n3O87UtLxZrYpZRsdgGejOB8G1tFw7HoIieloYCrwTPT/jCOMSjYX+A3wF2AucCihj+5OQOqX+iqgO2E896lAR2AwcB3wVUnHNXE0wn8Rf9tma0IyqyUUXQIg6UTg99HLaYQizd2AbwHHSRpsZv9IWr4PMBvoHP2frxAS9WPAo7kGGSXZc6OXP872v5rZ+lzXnaOLCEnmaDP7OITT4kQ49r0ItywMOBG4nlCycFq03Lxo2jjgVeC+pHU8Hz1XA7dHy84gfO63J4xs+UdJ3zezn2YNSBoJ/ApYDBxrZiuS5l0M3EK4PTQNWEE49hcSPisDzew/OWyjO3AF8HMz+0vS9O8SvrM9CZ/DiYT+2ieYWUcz+5mk4wnfnSOzbadI2hN+12oII8F1IHy3fy3p89EJaUJTj0lBf4tbnJn5owUfhMosBuyeYZn50TLzgG1j5k+I5vePmbdfNG9iyvQHoukXpUzvAMwifHn3zvF/WBWt6wFgq6TpPQn3UlcSjYsQTf9atPyrQNek6e2Ap6J5l6TZxjRg6wz74D1gz5T/ZxlQF61jQNK8auCvQD2wV8r6Pp/mf/1FtJ1vZDsGhBMDA6Zn2X8C7o+WvTZpenfCD/Y7wBdS3vMlwonR7JTpz0frGZEyfXg03YBTcjimvaNl1wDVeX6mL076vF6T5nFfun1D+PFcSxjfPPU70D2POBLH5IocYk39fiS2txDolHJM/x7NOzHXYx0d40afKUJSep5wwto1JoY1Sa9/FG3jSaBzyrL7E76zi4EdUuYdH73vnhz32+XRZ2u7pGlXs+U7ezMwOYr51ZQYTyN8n7rmsq2U7SZ+qx7L973R+xO/EU8AtUnTdyQMp1sPfKkAxyTtb3E5PEoeQFt7kF+SPzLN/LySPGGM93pgRpr1DYrec1WO/8OqaH07xcx7MPX/A34XTTs9Zvl+iR/XmG0YSQk8zT74Xsy8G6J5v4yZd1E07+Qc/9ddo+VvyXYMyD3JJ+KbQsOTofHR9LPSvO/OaP6u0ete0evFyetJWv5Fck/yR0TLvtaEz3QicebymJ7y3proB/R1oGPMd6Clk/yJMe85Lpr3l3yPdZoYzozee1JMDGuifTIpWuYekhJYzGfh0DTbeJKQtNrlEM8TwNSk19sD66N4OiRNH0A4sUhO8rtFcRzehP1QqCR/QIbjfGsBjkna3+JyeHhxfes2r0DrOYhwFlsr6ZqY+YlazL3zWOcKM3snZnqiOP1zhBMagAOj52dSFzazBZI+APaTVGtmdUmzPzCz/80Sx/y42KLnl2LmvR0975I8UaHS2WXACcAXCD/iyWXGO2eJIyfRvcL/JhSxf8eiX5LIwdHzlyXtEfP23aPn3oQaz4n9+mzKehJmEWrJt5TbzOziuBmSjiPcTkg1nhDj4WbWGlolzIqZNjN6PiCfFUnak3Df+nDC5619yiJxn6lqwn4aAvwUGJfm2CY+K1+TdHjM/G0Jt4T2IFRyy2R3QqW7hEMIpWy/MLPNt5LMbJ6kxwm3vRI+jZ47ZdlGsXxiZn+PmT4zem5wzJp4TKBwv8UtzpN867XWzD7JvlhOukbPg6JHOh3zWGdq87+EjdFzddK0LoQr/3T3B98BtiPcV34/afrKHOJYnSGGTPM2V6aTtDXwHNCH0HToviiOOsKP3ThgqxxiyUjSEMJ9zX8Bw6zxfe3Ecbooy6oSxylRgTDdfs1l/yUkTti6S6q2xvUjCk5SX+CHhKvquOSar0RlrkwVihPz4ip+bTSzD1InmtkaSZ+yZX9nFVWQe55wrGYS6kd8DGwC9ibUsYj7TNUAXyEUn09Pk+Bhy2dlfJZQcvlOd6Dhd2X76DnuvvNyGib5ROc47+awnWLI9tnffMyacUwK+Vvc4jzJt17pvtyw5Qcq7vjFNW1KfIGvtfyauxTKasJ9sh2I/1LuRPh/U79ImfZBIZ1GSPCNrkQl7UVI8s0iqR/wR8LJ0bFxyYQtx2lPM3s9h9Umlt8xzfycO5IhNI9aBXQDBrKlAlkx9SV8hkdLGp1mmXeiSnhfM7OnsqwvsT+6ZlimW/Qcd5JaI2m71GMjqSOhtOvtmPekM5aQYE41swdS1jeSkFDirCfUYXkMeEzSUDNrVALGlu9UOzPbGDM/H+8RvoMJq6Ln3Qgnv8l2TXl9MuEWQ9zVdEvI9tlPPnlp6jFpqd+hovAmdOXpw+g5rovJuOLZudHz4OKEk1XiB+Cw1BnR1dx2wGIz29CSQSX5QvT8YMy8Q2Om5UWh/ffDhNKDEzLcgsj3OP0tej5E8dXRc449umL8TfTyymzLS2p2yQbwGqFNfNwjkWjui17nkmAXRM8HZ1gmMW9Bmvlx++yw6Dk5kSVKOqqJ9wXCyfifY+ZlPC5mNo9QR2IdMF3S12MWm0u4nZSpZC5XL9Pwu/ksoRTrUkmbi7MlHUC4jZB4PQi4hFBfpdCtLXLVKYor1WHRc/Ixa/IxKWee5MtT4v7QCEmbj6FCO+pGV50WOt15CDhMoQ/qRsdd0t4qXr/Uk6LnayRtLmlQaH8+IXr5P0Xadi6WR8+HJU+U1Au4tjkrjq4CpwM9CBXqMl0h30G4x3lddOWfuq4aSZtjNLN/AnOAfYHvpCw7nPzvx19PuH87RNKdkhrdZ5XURdJ1hGZazWJmc83s3LgHW4qKL4+mLclhlU8S6mMMVkx//ZKGEpp6vQs8kmYdP0r+vyVtw5bPwF1Jsa8jJOHUK9uE5YTf1wYnbFETydOz/SNmlki8q4GpUVO1ZDcTTjQmxtXfkLR1lIRzMQ3YN7G8mb0L/Izw+Vmg0BfGXYSr+mVAO0kPEerYzCK0niilnyqpLwtJOxI6bDLg7qTlltOMY1KuvLi+PM0gVDj7OjBX0rOE4rYTCFeM34x5z0hCJZyfA+dKep5wtdSDkCQOJDS9SW2H3mxm9oSkXxISwz8kPciWdvK9CLV7S9m5xAOEtr5XKnRMs5hQGel4wg/gfzVj3WMJzZ2WAvukqfh4h5mtMLMVkk4j9BH+N0lPAkvY0qXoIMJ3tlvSe0cRKvHdGSWxRDv54wknF8flGqiF9ulfI7QRPxc4RdIThB/2asJ9yyMI9zTPznW9LcXM1kcnN9MJ7Z6fIVS+NEIFrK8RPndnREk61VrCCcArkpLbye8G/NHM/pSy/NOE9ugPEnpu2wg8ZWZzgVsJn5uHJd0frbcfoS+A+4n/jqb+P68odDTzNKEvi9MTxcxm9ndJFxI6JVoq6VFC87b2hBOPQwglJbmc6E0lfG5ulTQo2jc/JJQYXgicTzjp+j6wgXAyuC2he9t7LI+ObSQdCZwRvUyc8PeRdHf092dmdn6u6yO0yugBLFTohKo9Yd9uD9xgZsmVb5t9TMpSqav3t7UHuTehW5NlPdsTzlLfI/xwvUxoBhLbTj56T6IDlhcIFU4+I3x5nyA0OcmpHSjh5GBxmnmZmvedRbjyXEO4ClpIqGneqJlPpm3ksJ1E85lGTcfY0hzqipTpnyfcM38nim0R8F1CZcC4pl85NaFLWi7To3/KuvcCfg38b3SMPiIk+7tIaa8fLb8P4Yd6dbRvZxN+uNLuhyzHt5pwZTOVUEy+npAA/xHF9eU0+7vRZy5mv+fU5IwmNKFLeu8XojhfjY7lZ9G+vBPolek7R6iE9jNC64X1hEQ5DqiJec/OhOTwHuGqusHninAl/mx0/D4mXPUek+EzGPu9J5ycLyOcRAxPmXcgoROYNwkJ+P3os3sb8NU89tmg6P+dRlKzuUI/yN7cMuPvXsq6VhFOyLtGx/ad6H9YDIxK856CHJNyeij6R5xzrs2SNB/4opnl08KkoigMDjSZ0PrjcjN7PGaZKuBY4ALgfDMreMmfKywvrnfOOYeZ/U7Svwl1aB6L/n6WUM9BhFtYgwk11//MluaorhXzK3nnXJvnV/JbKIxI91/AMODLhGZqmwi39mYBk6zhvW7XinmSd861eZ7kXaXyJO+cc85VKG8n75xzzlUoT/LOOedchfIk75xzzlUoT/LOOedchfIk75xzzlUoT/LOOedchfo/+Zp454RYWWoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation of test error and plotting parity\n",
    "\n",
    "#model = tf.keras.models.load_model('model_checkpoint_bandgap.h5')\n",
    "loss, mae, mse = model.evaluate(test_fp.to_numpy(), test_label.to_numpy(), verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} bg\".format(mae))\n",
    "\n",
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_fp.to_numpy(), train_label.to_numpy(), verbose=2)\n",
    "\n",
    "tr_rmse = math.sqrt(tr_mse)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "test_predictions = model.predict(test_fp.to_numpy()).flatten()\n",
    "\n",
    "\n",
    "train_predictions = model.predict(train_fp.to_numpy()).flatten()\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "ax1.scatter(test_label, test_predictions, c='r',s=30)\n",
    "\n",
    "ax1.scatter(train_label, train_predictions, c='b',s=30)\n",
    "\n",
    "ax1.set_xlabel('True normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_ylabel('Predicted normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_xlim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.set_ylim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.legend()\n",
    "plot_x_min, plot_x_max = plt.xlim()\n",
    "plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.15\n",
    "\n",
    "ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "         \"RMSE train=\" + str(\"%.4f\" % tr_rmse), ha='left', fontsize=16)\n",
    "\n",
    "# ax1.text(text_position_x, text_position_y, \"MAE=\" + str(\"%.4f\" % mae) + ' \\n' + \n",
    "#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./%s_test_parity_%s.png'%(property_used, total_frac),dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions - test_label\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_space(4,9, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "defaults = [10, .8, 3, 100, 'relu', 'mse', 'adam', .2]\n",
    "patience = defaults[0]\n",
    "training_pct = defaults[1]\n",
    "n_layer = defaults[2]\n",
    "n_unit = defaults[3]\n",
    "activation = defaults[4]\n",
    "loss = defaults[5]\n",
    "opt = defaults[6]\n",
    "val_pct = defaults[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 100)               38600     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,901\n",
      "Trainable params: 58,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48884 samples, validate on 12222 samples\n",
      "Epoch 1/5000\n",
      "47616/48884 [============================>.] - ETA: 0s - loss: 0.1944 - mae: 0.2153 - mse: 0.1944\n",
      "Epoch 00001: val_loss improved from inf to 0.14806, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.1950,  mae:0.2147,  mse:0.1950,  val_loss:0.1481,  val_mae:0.2079,  val_mse:0.1481,  \n",
      "48884/48884 [==============================] - 2s 42us/sample - loss: 0.1950 - mae: 0.2147 - mse: 0.1950 - val_loss: 0.1481 - val_mae: 0.2079 - val_mse: 0.1481\n",
      "Epoch 2/5000\n",
      "48064/48884 [============================>.] - ETA: 0s - loss: 0.1474 - mae: 0.1765 - mse: 0.1474\n",
      "Epoch 00002: val_loss improved from 0.14806 to 0.13700, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1465 - mae: 0.1763 - mse: 0.1465 - val_loss: 0.1370 - val_mae: 0.1630 - val_mse: 0.1370\n",
      "Epoch 3/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1347 - mae: 0.1659 - mse: 0.1347\n",
      "Epoch 00003: val_loss improved from 0.13700 to 0.12152, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1365 - mae: 0.1657 - mse: 0.1365 - val_loss: 0.1215 - val_mae: 0.1687 - val_mse: 0.1215\n",
      "Epoch 4/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1314 - mae: 0.1618 - mse: 0.1314\n",
      "Epoch 00004: val_loss did not improve from 0.12152\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1311 - mae: 0.1619 - mse: 0.1311 - val_loss: 0.1239 - val_mae: 0.1733 - val_mse: 0.1239\n",
      "Epoch 5/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1272 - mae: 0.1561 - mse: 0.1272\n",
      "Epoch 00005: val_loss improved from 0.12152 to 0.11592, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1266 - mae: 0.1560 - mse: 0.1266 - val_loss: 0.1159 - val_mae: 0.1458 - val_mse: 0.1159\n",
      "Epoch 6/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.1245 - mae: 0.1544 - mse: 0.1245\n",
      "Epoch 00006: val_loss did not improve from 0.11592\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1241 - mae: 0.1543 - mse: 0.1241 - val_loss: 0.1194 - val_mae: 0.1491 - val_mse: 0.1194\n",
      "Epoch 7/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1209 - mae: 0.1516 - mse: 0.1209\n",
      "Epoch 00007: val_loss did not improve from 0.11592\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.1207 - mae: 0.1517 - mse: 0.1207 - val_loss: 0.1176 - val_mae: 0.1474 - val_mse: 0.1176\n",
      "Epoch 8/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1193 - mae: 0.1483 - mse: 0.1193\n",
      "Epoch 00008: val_loss improved from 0.11592 to 0.11350, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1190 - mae: 0.1483 - mse: 0.1190 - val_loss: 0.1135 - val_mae: 0.1480 - val_mse: 0.1135\n",
      "Epoch 9/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1169 - mae: 0.1468 - mse: 0.1169\n",
      "Epoch 00009: val_loss did not improve from 0.11350\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1165 - mae: 0.1468 - mse: 0.1165 - val_loss: 0.1148 - val_mae: 0.1476 - val_mse: 0.1148\n",
      "Epoch 10/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1144 - mae: 0.1449 - mse: 0.1144\n",
      "Epoch 00010: val_loss improved from 0.11350 to 0.10856, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1138 - mae: 0.1448 - mse: 0.1138 - val_loss: 0.1086 - val_mae: 0.1374 - val_mse: 0.1086\n",
      "Epoch 11/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1144 - mae: 0.1442 - mse: 0.1144\n",
      "Epoch 00011: val_loss did not improve from 0.10856\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1137 - mae: 0.1442 - mse: 0.1137 - val_loss: 0.1142 - val_mae: 0.1441 - val_mse: 0.1142\n",
      "Epoch 12/5000\n",
      "48608/48884 [============================>.] - ETA: 0s - loss: 0.1124 - mae: 0.1423 - mse: 0.1124\n",
      "Epoch 00012: val_loss did not improve from 0.10856\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1121 - mae: 0.1423 - mse: 0.1121 - val_loss: 0.1114 - val_mae: 0.1400 - val_mse: 0.1114\n",
      "Epoch 13/5000\n",
      "48544/48884 [============================>.] - ETA: 0s - loss: 0.1098 - mae: 0.1399 - mse: 0.1098\n",
      "Epoch 00013: val_loss improved from 0.10856 to 0.10710, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1095 - mae: 0.1399 - mse: 0.1095 - val_loss: 0.1071 - val_mae: 0.1354 - val_mse: 0.1071\n",
      "Epoch 14/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1083 - mae: 0.1386 - mse: 0.1083\n",
      "Epoch 00014: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1076 - mae: 0.1385 - mse: 0.1076 - val_loss: 0.1080 - val_mae: 0.1367 - val_mse: 0.1080\n",
      "Epoch 15/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.1093 - mae: 0.1400 - mse: 0.1093\n",
      "Epoch 00015: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1087 - mae: 0.1398 - mse: 0.1087 - val_loss: 0.1120 - val_mae: 0.1407 - val_mse: 0.1120\n",
      "Epoch 16/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1064 - mae: 0.1367 - mse: 0.1064\n",
      "Epoch 00016: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1058 - mae: 0.1367 - mse: 0.1058 - val_loss: 0.1082 - val_mae: 0.1376 - val_mse: 0.1082\n",
      "Epoch 17/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.1057 - mae: 0.1367 - mse: 0.1057\n",
      "Epoch 00017: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.1052 - mae: 0.1366 - mse: 0.1052 - val_loss: 0.1110 - val_mae: 0.1368 - val_mse: 0.1110\n",
      "Epoch 18/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1038 - mae: 0.1350 - mse: 0.1038\n",
      "Epoch 00018: val_loss improved from 0.10710 to 0.10580, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1035 - mae: 0.1349 - mse: 0.1035 - val_loss: 0.1058 - val_mae: 0.1342 - val_mse: 0.1058\n",
      "Epoch 19/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1026 - mae: 0.1333 - mse: 0.1026\n",
      "Epoch 00019: val_loss did not improve from 0.10580\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1017 - mae: 0.1330 - mse: 0.1017 - val_loss: 0.1060 - val_mae: 0.1330 - val_mse: 0.1060\n",
      "Epoch 20/5000\n",
      "48352/48884 [============================>.] - ETA: 0s - loss: 0.1025 - mae: 0.1338 - mse: 0.1025\n",
      "Epoch 00020: val_loss did not improve from 0.10580\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1020 - mae: 0.1338 - mse: 0.1020 - val_loss: 0.1071 - val_mae: 0.1423 - val_mse: 0.1071\n",
      "Epoch 21/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.1018 - mae: 0.1336 - mse: 0.1018\n",
      "Epoch 00021: val_loss improved from 0.10580 to 0.10549, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1013 - mae: 0.1335 - mse: 0.1013 - val_loss: 0.1055 - val_mae: 0.1331 - val_mse: 0.1055\n",
      "Epoch 22/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1001 - mae: 0.1318 - mse: 0.1001\n",
      "Epoch 00022: val_loss improved from 0.10549 to 0.10472, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0995 - mae: 0.1317 - mse: 0.0995 - val_loss: 0.1047 - val_mae: 0.1333 - val_mse: 0.1047\n",
      "Epoch 23/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.0996 - mae: 0.1306 - mse: 0.0996\n",
      "Epoch 00023: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0994 - mae: 0.1308 - mse: 0.0994 - val_loss: 0.1103 - val_mae: 0.1474 - val_mse: 0.1103\n",
      "Epoch 24/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0959 - mae: 0.1292 - mse: 0.0959\n",
      "Epoch 00024: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0972 - mae: 0.1291 - mse: 0.0972 - val_loss: 0.1101 - val_mae: 0.1421 - val_mse: 0.1101\n",
      "Epoch 25/5000\n",
      "48448/48884 [============================>.] - ETA: 0s - loss: 0.0978 - mae: 0.1294 - mse: 0.0978\n",
      "Epoch 00025: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0975 - mae: 0.1293 - mse: 0.0975 - val_loss: 0.1067 - val_mae: 0.1353 - val_mse: 0.1067\n",
      "Epoch 26/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0971 - mae: 0.1284 - mse: 0.0971\n",
      "Epoch 00026: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0967 - mae: 0.1284 - mse: 0.0967 - val_loss: 0.1073 - val_mae: 0.1342 - val_mse: 0.1073\n",
      "Epoch 27/5000\n",
      "48608/48884 [============================>.] - ETA: 0s - loss: 0.0961 - mae: 0.1279 - mse: 0.0961\n",
      "Epoch 00027: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0959 - mae: 0.1279 - mse: 0.0959 - val_loss: 0.1094 - val_mae: 0.1368 - val_mse: 0.1094\n",
      "Epoch 28/5000\n",
      "48352/48884 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.1266 - mse: 0.0932\n",
      "Epoch 00028: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0943 - mae: 0.1267 - mse: 0.0943 - val_loss: 0.1101 - val_mae: 0.1405 - val_mse: 0.1101\n",
      "Epoch 29/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.0891 - mae: 0.1258 - mse: 0.0891\n",
      "Epoch 00029: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0941 - mae: 0.1262 - mse: 0.0941 - val_loss: 0.1117 - val_mae: 0.1515 - val_mse: 0.1117\n",
      "Epoch 30/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.0939 - mae: 0.1268 - mse: 0.0939\n",
      "Epoch 00030: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0935 - mae: 0.1267 - mse: 0.0935 - val_loss: 0.1076 - val_mae: 0.1338 - val_mse: 0.1076\n",
      "Epoch 31/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.1254 - mse: 0.0932\n",
      "Epoch 00031: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0927 - mae: 0.1252 - mse: 0.0927 - val_loss: 0.1068 - val_mae: 0.1314 - val_mse: 0.1068\n",
      "Epoch 32/5000\n",
      "48544/48884 [============================>.] - ETA: 0s - loss: 0.0923 - mae: 0.1245 - mse: 0.0923\n",
      "Epoch 00032: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0920 - mae: 0.1245 - mse: 0.0920 - val_loss: 0.1204 - val_mae: 0.1448 - val_mse: 0.1204\n",
      "15276/15276 - 0s - loss: 0.0947 - mae: 0.1430 - mse: 0.0947\n"
     ]
    }
   ],
   "source": [
    "mse = evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09473248"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = .1\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":7, \"n_unit\":20, \"activation\":'relu', \"loss\":'mean_absolute_error', \n",
    "            \"opt\":'rmsprop', \"val_pct\":.2}\n",
    "all_grid = {\"patience\":[10], \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":eq_space(.2, .5, 5)}\n",
    "\n",
    "\n",
    "init_grid = {\"val_pct\":eq_space(.2, .5, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.2\n",
      "385\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_318 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4888 samples, validate on 1222 samples\n",
      "Epoch 1/2000\n",
      "4672/4888 [===========================>..] - ETA: 0s - loss: 0.3754 - mae: 0.3754 - mse: 0.6080\n",
      "Epoch 00001: val_loss improved from inf to 0.31702, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.3715,  mae:0.3715,  mse:0.5963,  val_loss:0.3170,  val_mae:0.3170,  val_mse:0.4920,  \n",
      "4888/4888 [==============================] - 1s 152us/sample - loss: 0.3715 - mae: 0.3715 - mse: 0.5963 - val_loss: 0.3170 - val_mae: 0.3170 - val_mse: 0.4920\n",
      "Epoch 2/2000\n",
      "3488/4888 [====================>.........] - ETA: 0s - loss: 0.2824 - mae: 0.2824 - mse: 0.3752\n",
      "Epoch 00002: val_loss improved from 0.31702 to 0.25170, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 53us/sample - loss: 0.2725 - mae: 0.2725 - mse: 0.3564 - val_loss: 0.2517 - val_mae: 0.2517 - val_mse: 0.3266\n",
      "Epoch 3/2000\n",
      "4704/4888 [===========================>..] - ETA: 0s - loss: 0.2354 - mae: 0.2354 - mse: 0.2533\n",
      "Epoch 00003: val_loss improved from 0.25170 to 0.23193, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 57us/sample - loss: 0.2363 - mae: 0.2363 - mse: 0.2729 - val_loss: 0.2319 - val_mae: 0.2319 - val_mse: 0.2720\n",
      "Epoch 4/2000\n",
      "3616/4888 [=====================>........] - ETA: 0s - loss: 0.2199 - mae: 0.2199 - mse: 0.2613\n",
      "Epoch 00004: val_loss improved from 0.23193 to 0.22040, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 52us/sample - loss: 0.2151 - mae: 0.2151 - mse: 0.2308 - val_loss: 0.2204 - val_mae: 0.2204 - val_mse: 0.2685\n",
      "Epoch 5/2000\n",
      "4768/4888 [============================>.] - ETA: 0s - loss: 0.2039 - mae: 0.2039 - mse: 0.2161\n",
      "Epoch 00005: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.2036 - mae: 0.2036 - mse: 0.2144 - val_loss: 0.2269 - val_mae: 0.2269 - val_mse: 0.2931\n",
      "Epoch 6/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1925 - mae: 0.1925 - mse: 0.1681\n",
      "Epoch 00006: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1991 - mae: 0.1991 - mse: 0.2090 - val_loss: 0.2592 - val_mae: 0.2592 - val_mse: 0.2942\n",
      "Epoch 7/2000\n",
      "3776/4888 [======================>.......] - ETA: 0s - loss: 0.1893 - mae: 0.1893 - mse: 0.2150\n",
      "Epoch 00007: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1907 - mae: 0.1907 - mse: 0.2011 - val_loss: 0.2314 - val_mae: 0.2314 - val_mse: 0.2927\n",
      "Epoch 8/2000\n",
      "3712/4888 [=====================>........] - ETA: 0s - loss: 0.1879 - mae: 0.1879 - mse: 0.1919\n",
      "Epoch 00008: val_loss improved from 0.22040 to 0.19471, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1859 - mae: 0.1859 - mse: 0.1947 - val_loss: 0.1947 - val_mae: 0.1947 - val_mse: 0.2358\n",
      "Epoch 9/2000\n",
      "3616/4888 [=====================>........] - ETA: 0s - loss: 0.1833 - mae: 0.1833 - mse: 0.1807\n",
      "Epoch 00009: val_loss did not improve from 0.19471\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1838 - mae: 0.1838 - mse: 0.1894 - val_loss: 0.2062 - val_mae: 0.2062 - val_mse: 0.2489\n",
      "Epoch 10/2000\n",
      "3552/4888 [====================>.........] - ETA: 0s - loss: 0.1775 - mae: 0.1775 - mse: 0.2074\n",
      "Epoch 00010: val_loss did not improve from 0.19471\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1803 - mae: 0.1803 - mse: 0.1868 - val_loss: 0.1954 - val_mae: 0.1954 - val_mse: 0.2356\n",
      "Epoch 11/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1803 - mae: 0.1803 - mse: 0.1748\n",
      "Epoch 00011: val_loss improved from 0.19471 to 0.19371, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 49us/sample - loss: 0.1800 - mae: 0.1800 - mse: 0.1864 - val_loss: 0.1937 - val_mae: 0.1937 - val_mse: 0.2288\n",
      "Epoch 12/2000\n",
      "3744/4888 [=====================>........] - ETA: 0s - loss: 0.1765 - mae: 0.1765 - mse: 0.1700\n",
      "Epoch 00012: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1744 - mae: 0.1744 - mse: 0.1787 - val_loss: 0.2072 - val_mae: 0.2072 - val_mse: 0.2398\n",
      "Epoch 13/2000\n",
      "3776/4888 [======================>.......] - ETA: 0s - loss: 0.1775 - mae: 0.1775 - mse: 0.1775\n",
      "Epoch 00013: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1735 - mae: 0.1735 - mse: 0.1812 - val_loss: 0.1965 - val_mae: 0.1965 - val_mse: 0.2347\n",
      "Epoch 14/2000\n",
      "3808/4888 [======================>.......] - ETA: 0s - loss: 0.1688 - mae: 0.1688 - mse: 0.1882\n",
      "Epoch 00014: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1702 - mae: 0.1702 - mse: 0.1746 - val_loss: 0.2289 - val_mae: 0.2289 - val_mse: 0.2735\n",
      "Epoch 15/2000\n",
      "4864/4888 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.1685 - mse: 0.1738\n",
      "Epoch 00015: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 50us/sample - loss: 0.1680 - mae: 0.1680 - mse: 0.1731 - val_loss: 0.1949 - val_mae: 0.1949 - val_mse: 0.2310\n",
      "Epoch 16/2000\n",
      "4096/4888 [========================>.....] - ETA: 0s - loss: 0.1697 - mae: 0.1697 - mse: 0.1609\n",
      "Epoch 00016: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1678 - mae: 0.1678 - mse: 0.1728 - val_loss: 0.2035 - val_mae: 0.2035 - val_mse: 0.2447\n",
      "Epoch 17/2000\n",
      "3552/4888 [====================>.........] - ETA: 0s - loss: 0.1642 - mae: 0.1642 - mse: 0.1888\n",
      "Epoch 00017: val_loss improved from 0.19371 to 0.18904, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1655 - mae: 0.1655 - mse: 0.1696 - val_loss: 0.1890 - val_mae: 0.1890 - val_mse: 0.2237\n",
      "Epoch 18/2000\n",
      "4864/4888 [============================>.] - ETA: 0s - loss: 0.1654 - mae: 0.1654 - mse: 0.1706\n",
      "Epoch 00018: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1650 - mae: 0.1650 - mse: 0.1699 - val_loss: 0.2417 - val_mae: 0.2417 - val_mse: 0.2997\n",
      "Epoch 19/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1563 - mae: 0.1563 - mse: 0.1228\n",
      "Epoch 00019: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1616 - mae: 0.1616 - mse: 0.1660 - val_loss: 0.1988 - val_mae: 0.1988 - val_mse: 0.2276\n",
      "Epoch 20/2000\n",
      "3520/4888 [====================>.........] - ETA: 0s - loss: 0.1645 - mae: 0.1645 - mse: 0.1907\n",
      "Epoch 00020: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1604 - mae: 0.1604 - mse: 0.1638 - val_loss: 0.1998 - val_mae: 0.1998 - val_mse: 0.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/2000\n",
      "4416/4888 [==========================>...] - ETA: 0s - loss: 0.1564 - mae: 0.1564 - mse: 0.1463\n",
      "Epoch 00021: val_loss improved from 0.18904 to 0.18687, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1666 - val_loss: 0.1869 - val_mae: 0.1869 - val_mse: 0.2211\n",
      "Epoch 22/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1544 - mae: 0.1544 - mse: 0.1489\n",
      "Epoch 00022: val_loss did not improve from 0.18687\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1562 - mae: 0.1562 - mse: 0.1604 - val_loss: 0.1928 - val_mae: 0.1928 - val_mse: 0.2369\n",
      "Epoch 23/2000\n",
      "4224/4888 [========================>.....] - ETA: 0s - loss: 0.1560 - mae: 0.1560 - mse: 0.1701\n",
      "Epoch 00023: val_loss did not improve from 0.18687\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1541 - mae: 0.1541 - mse: 0.1568 - val_loss: 0.1910 - val_mae: 0.1910 - val_mse: 0.2350\n",
      "Epoch 24/2000\n",
      "4448/4888 [==========================>...] - ETA: 0s - loss: 0.1541 - mae: 0.1541 - mse: 0.1656\n",
      "Epoch 00024: val_loss improved from 0.18687 to 0.18336, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1550 - mae: 0.1550 - mse: 0.1602 - val_loss: 0.1834 - val_mae: 0.1834 - val_mse: 0.2262\n",
      "Epoch 25/2000\n",
      "3744/4888 [=====================>........] - ETA: 0s - loss: 0.1524 - mae: 0.1524 - mse: 0.1471\n",
      "Epoch 00025: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1540 - mae: 0.1540 - mse: 0.1596 - val_loss: 0.2260 - val_mae: 0.2260 - val_mse: 0.2687\n",
      "Epoch 26/2000\n",
      "4128/4888 [========================>.....] - ETA: 0s - loss: 0.1510 - mae: 0.1510 - mse: 0.1418\n",
      "Epoch 00026: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1543 - mae: 0.1543 - mse: 0.1599 - val_loss: 0.2104 - val_mae: 0.2104 - val_mse: 0.2458\n",
      "Epoch 27/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1531 - mae: 0.1531 - mse: 0.1650\n",
      "Epoch 00027: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1514 - mae: 0.1514 - mse: 0.1550 - val_loss: 0.2011 - val_mae: 0.2011 - val_mse: 0.2425\n",
      "Epoch 28/2000\n",
      "4480/4888 [==========================>...] - ETA: 0s - loss: 0.1504 - mae: 0.1504 - mse: 0.1582\n",
      "Epoch 00028: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1495 - mae: 0.1495 - mse: 0.1524 - val_loss: 0.1899 - val_mae: 0.1899 - val_mse: 0.2270\n",
      "Epoch 29/2000\n",
      "4160/4888 [========================>.....] - ETA: 0s - loss: 0.1478 - mae: 0.1478 - mse: 0.1620\n",
      "Epoch 00029: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1507 - mae: 0.1507 - mse: 0.1556 - val_loss: 0.1998 - val_mae: 0.1998 - val_mse: 0.2426\n",
      "Epoch 30/2000\n",
      "4480/4888 [==========================>...] - ETA: 0s - loss: 0.1493 - mae: 0.1493 - mse: 0.1585\n",
      "Epoch 00030: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1491 - mae: 0.1491 - mse: 0.1527 - val_loss: 0.1930 - val_mae: 0.1930 - val_mse: 0.2249\n",
      "Epoch 31/2000\n",
      "4192/4888 [========================>.....] - ETA: 0s - loss: 0.1489 - mae: 0.1489 - mse: 0.1610\n",
      "Epoch 00031: val_loss improved from 0.18336 to 0.18114, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1474 - mae: 0.1474 - mse: 0.1501 - val_loss: 0.1811 - val_mae: 0.1811 - val_mse: 0.2184\n",
      "Epoch 32/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1482 - mae: 0.1482 - mse: 0.1718\n",
      "Epoch 00032: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1467 - mae: 0.1467 - mse: 0.1520 - val_loss: 0.1818 - val_mae: 0.1818 - val_mse: 0.2133\n",
      "Epoch 33/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1477 - mae: 0.1477 - mse: 0.1586\n",
      "Epoch 00033: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1468 - mae: 0.1468 - mse: 0.1509 - val_loss: 0.2196 - val_mae: 0.2196 - val_mse: 0.2616\n",
      "Epoch 34/2000\n",
      "4288/4888 [=========================>....] - ETA: 0s - loss: 0.1448 - mae: 0.1448 - mse: 0.1575\n",
      "Epoch 00034: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1441 - mae: 0.1441 - mse: 0.1489 - val_loss: 0.1868 - val_mae: 0.1868 - val_mse: 0.2229\n",
      "Epoch 35/2000\n",
      "4512/4888 [==========================>...] - ETA: 0s - loss: 0.1452 - mae: 0.1452 - mse: 0.1544\n",
      "Epoch 00035: val_loss improved from 0.18114 to 0.17427, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1434 - mae: 0.1434 - mse: 0.1474 - val_loss: 0.1743 - val_mae: 0.1743 - val_mse: 0.2114\n",
      "Epoch 36/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1452 - mae: 0.1452 - mse: 0.1649\n",
      "Epoch 00036: val_loss did not improve from 0.17427\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1444 - mae: 0.1444 - mse: 0.1500 - val_loss: 0.1852 - val_mae: 0.1852 - val_mse: 0.2253\n",
      "Epoch 37/2000\n",
      "4256/4888 [=========================>....] - ETA: 0s - loss: 0.1393 - mae: 0.1393 - mse: 0.1294\n",
      "Epoch 00037: val_loss improved from 0.17427 to 0.16954, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1405 - mae: 0.1405 - mse: 0.1442 - val_loss: 0.1695 - val_mae: 0.1695 - val_mse: 0.2086\n",
      "Epoch 38/2000\n",
      "4032/4888 [=======================>......] - ETA: 0s - loss: 0.1443 - mae: 0.1443 - mse: 0.1607\n",
      "Epoch 00038: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1427 - mae: 0.1427 - mse: 0.1452 - val_loss: 0.1786 - val_mae: 0.1786 - val_mse: 0.2139\n",
      "Epoch 39/2000\n",
      "4192/4888 [========================>.....] - ETA: 0s - loss: 0.1435 - mae: 0.1435 - mse: 0.1591\n",
      "Epoch 00039: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1422 - mae: 0.1422 - mse: 0.1474 - val_loss: 0.1895 - val_mae: 0.1895 - val_mse: 0.2216\n",
      "Epoch 40/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1384 - mae: 0.1384 - mse: 0.1275\n",
      "Epoch 00040: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1409 - mae: 0.1409 - mse: 0.1451 - val_loss: 0.1764 - val_mae: 0.1764 - val_mse: 0.2085\n",
      "Epoch 41/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1391 - mae: 0.1391 - mse: 0.1265\n",
      "Epoch 00041: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1398 - mae: 0.1398 - mse: 0.1426 - val_loss: 0.1698 - val_mae: 0.1698 - val_mse: 0.2068\n",
      "Epoch 42/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1398 - mae: 0.1398 - mse: 0.1519\n",
      "Epoch 00042: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1386 - mae: 0.1386 - mse: 0.1425 - val_loss: 0.1731 - val_mae: 0.1731 - val_mse: 0.2070\n",
      "Epoch 43/2000\n",
      "4064/4888 [=======================>......] - ETA: 0s - loss: 0.1315 - mae: 0.1315 - mse: 0.1227\n",
      "Epoch 00043: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1364 - mae: 0.1364 - mse: 0.1405 - val_loss: 0.1778 - val_mae: 0.1778 - val_mse: 0.2128\n",
      "Epoch 44/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1380 - mae: 0.1380 - mse: 0.1498\n",
      "Epoch 00044: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1385 - mae: 0.1385 - mse: 0.1444 - val_loss: 0.1915 - val_mae: 0.1915 - val_mse: 0.2318\n",
      "Epoch 45/2000\n",
      "4288/4888 [=========================>....] - ETA: 0s - loss: 0.1390 - mae: 0.1390 - mse: 0.1497\n",
      "Epoch 00045: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1378 - mae: 0.1378 - mse: 0.1410 - val_loss: 0.1778 - val_mae: 0.1778 - val_mse: 0.2049\n",
      "Epoch 46/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/4888 [==========================>...] - ETA: 0s - loss: 0.1374 - mae: 0.1374 - mse: 0.1448\n",
      "Epoch 00046: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 41us/sample - loss: 0.1364 - mae: 0.1364 - mse: 0.1380 - val_loss: 0.1732 - val_mae: 0.1732 - val_mse: 0.2062\n",
      "Epoch 47/2000\n",
      "4544/4888 [==========================>...] - ETA: 0s - loss: 0.1384 - mae: 0.1384 - mse: 0.1468\n",
      "Epoch 00047: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 41us/sample - loss: 0.1360 - mae: 0.1360 - mse: 0.1394 - val_loss: 0.1772 - val_mae: 0.1772 - val_mse: 0.2112\n",
      "1528/1528 - 0s - loss: 0.1615 - mae: 0.1615 - mse: 0.1068\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.275\n",
      "385\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_326 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4429 samples, validate on 1681 samples\n",
      "Epoch 1/2000\n",
      "4416/4429 [============================>.] - ETA: 0s - loss: 0.4281 - mae: 0.4281 - mse: 0.8085\n",
      "Epoch 00001: val_loss improved from inf to 0.35309, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4272,  mae:0.4272,  mse:0.8063,  val_loss:0.3531,  val_mae:0.3531,  val_mse:0.6535,  \n",
      "4429/4429 [==============================] - 1s 159us/sample - loss: 0.4272 - mae: 0.4272 - mse: 0.8063 - val_loss: 0.3531 - val_mae: 0.3531 - val_mse: 0.6535\n",
      "Epoch 2/2000\n",
      "3936/4429 [=========================>....] - ETA: 0s - loss: 0.2987 - mae: 0.2987 - mse: 0.4605\n",
      "Epoch 00002: val_loss improved from 0.35309 to 0.24095, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 52us/sample - loss: 0.2904 - mae: 0.2904 - mse: 0.4287 - val_loss: 0.2409 - val_mae: 0.2409 - val_mse: 0.3043\n",
      "Epoch 3/2000\n",
      "4288/4429 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.2218 - mse: 0.2073\n",
      "Epoch 00003: val_loss improved from 0.24095 to 0.21329, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 50us/sample - loss: 0.2276 - mae: 0.2276 - mse: 0.2580 - val_loss: 0.2133 - val_mae: 0.2133 - val_mse: 0.2370\n",
      "Epoch 4/2000\n",
      "3648/4429 [=======================>......] - ETA: 0s - loss: 0.2095 - mae: 0.2095 - mse: 0.2373\n",
      "Epoch 00004: val_loss did not improve from 0.21329\n",
      "4429/4429 [==============================] - 0s 51us/sample - loss: 0.2084 - mae: 0.2084 - mse: 0.2223 - val_loss: 0.2465 - val_mae: 0.2465 - val_mse: 0.2743\n",
      "Epoch 5/2000\n",
      "4224/4429 [===========================>..] - ETA: 0s - loss: 0.1996 - mae: 0.1996 - mse: 0.2124\n",
      "Epoch 00005: val_loss did not improve from 0.21329\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.2001 - mae: 0.2001 - mse: 0.2090 - val_loss: 0.2324 - val_mae: 0.2324 - val_mse: 0.2673\n",
      "Epoch 6/2000\n",
      "4128/4429 [==========================>...] - ETA: 0s - loss: 0.1981 - mae: 0.1981 - mse: 0.2157\n",
      "Epoch 00006: val_loss improved from 0.21329 to 0.19794, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 51us/sample - loss: 0.1968 - mae: 0.1968 - mse: 0.2093 - val_loss: 0.1979 - val_mae: 0.1979 - val_mse: 0.2190\n",
      "Epoch 7/2000\n",
      "3776/4429 [========================>.....] - ETA: 0s - loss: 0.1965 - mae: 0.1965 - mse: 0.2242\n",
      "Epoch 00007: val_loss improved from 0.19794 to 0.19042, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 53us/sample - loss: 0.1945 - mae: 0.1945 - mse: 0.2091 - val_loss: 0.1904 - val_mae: 0.1904 - val_mse: 0.2118\n",
      "Epoch 8/2000\n",
      "4192/4429 [===========================>..] - ETA: 0s - loss: 0.1887 - mae: 0.1887 - mse: 0.2035\n",
      "Epoch 00008: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1908 - mae: 0.1908 - mse: 0.2016 - val_loss: 0.1963 - val_mae: 0.1963 - val_mse: 0.2125\n",
      "Epoch 9/2000\n",
      "4128/4429 [==========================>...] - ETA: 0s - loss: 0.1891 - mae: 0.1891 - mse: 0.2062\n",
      "Epoch 00009: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1860 - mae: 0.1860 - mse: 0.1974 - val_loss: 0.2073 - val_mae: 0.2073 - val_mse: 0.2504\n",
      "Epoch 10/2000\n",
      "4352/4429 [============================>.] - ETA: 0s - loss: 0.1823 - mae: 0.1823 - mse: 0.1959\n",
      "Epoch 00010: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1825 - mae: 0.1825 - mse: 0.1944 - val_loss: 0.1982 - val_mae: 0.1982 - val_mse: 0.2114\n",
      "Epoch 11/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1813 - mae: 0.1813 - mse: 0.1930\n",
      "Epoch 00011: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1824 - mae: 0.1824 - mse: 0.1938 - val_loss: 0.2079 - val_mae: 0.2079 - val_mse: 0.2147\n",
      "Epoch 12/2000\n",
      "2976/4429 [===================>..........] - ETA: 0s - loss: 0.1814 - mae: 0.1814 - mse: 0.2013\n",
      "Epoch 00012: val_loss improved from 0.19042 to 0.17982, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 49us/sample - loss: 0.1764 - mae: 0.1764 - mse: 0.1888 - val_loss: 0.1798 - val_mae: 0.1798 - val_mse: 0.1900\n",
      "Epoch 13/2000\n",
      "3904/4429 [=========================>....] - ETA: 0s - loss: 0.1776 - mae: 0.1776 - mse: 0.1927\n",
      "Epoch 00013: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 48us/sample - loss: 0.1772 - mae: 0.1772 - mse: 0.1850 - val_loss: 0.1915 - val_mae: 0.1915 - val_mse: 0.2217\n",
      "Epoch 14/2000\n",
      "4032/4429 [==========================>...] - ETA: 0s - loss: 0.1750 - mae: 0.1750 - mse: 0.1691\n",
      "Epoch 00014: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 48us/sample - loss: 0.1745 - mae: 0.1745 - mse: 0.1853 - val_loss: 0.2057 - val_mae: 0.2057 - val_mse: 0.2210\n",
      "Epoch 15/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.1714 - mse: 0.1862\n",
      "Epoch 00015: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1711 - mae: 0.1711 - mse: 0.1841 - val_loss: 0.1921 - val_mae: 0.1921 - val_mse: 0.2105\n",
      "Epoch 16/2000\n",
      "4352/4429 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.1676 - mse: 0.1780\n",
      "Epoch 00016: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1685 - mae: 0.1685 - mse: 0.1792 - val_loss: 0.2682 - val_mae: 0.2682 - val_mse: 0.3046\n",
      "Epoch 17/2000\n",
      "4000/4429 [==========================>...] - ETA: 0s - loss: 0.1692 - mae: 0.1692 - mse: 0.1908\n",
      "Epoch 00017: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1686 - mae: 0.1686 - mse: 0.1820 - val_loss: 0.1847 - val_mae: 0.1847 - val_mse: 0.1992\n",
      "Epoch 18/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1674 - mae: 0.1674 - mse: 0.1799\n",
      "Epoch 00018: val_loss did not improve from 0.17982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1665 - mae: 0.1665 - mse: 0.1767 - val_loss: 0.1888 - val_mae: 0.1888 - val_mse: 0.2113\n",
      "Epoch 19/2000\n",
      "3392/4429 [=====================>........] - ETA: 0s - loss: 0.1657 - mae: 0.1657 - mse: 0.1410\n",
      "Epoch 00019: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 55us/sample - loss: 0.1681 - mae: 0.1681 - mse: 0.1812 - val_loss: 0.1996 - val_mae: 0.1996 - val_mse: 0.2086\n",
      "Epoch 20/2000\n",
      "4064/4429 [==========================>...] - ETA: 0s - loss: 0.1652 - mae: 0.1652 - mse: 0.1792\n",
      "Epoch 00020: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 59us/sample - loss: 0.1655 - mae: 0.1655 - mse: 0.1724 - val_loss: 0.2297 - val_mae: 0.2297 - val_mse: 0.2568\n",
      "Epoch 21/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1631 - mae: 0.1631 - mse: 0.1775\n",
      "Epoch 00021: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1632 - mae: 0.1632 - mse: 0.1766 - val_loss: 0.1812 - val_mae: 0.1812 - val_mse: 0.1902\n",
      "Epoch 22/2000\n",
      "4160/4429 [===========================>..] - ETA: 0s - loss: 0.1629 - mae: 0.1629 - mse: 0.1805\n",
      "Epoch 00022: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 50us/sample - loss: 0.1624 - mae: 0.1624 - mse: 0.1758 - val_loss: 0.1893 - val_mae: 0.1893 - val_mse: 0.2136\n",
      "1528/1528 - 0s - loss: 0.1855 - mae: 0.1855 - mse: 0.1430\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.35\n",
      "385\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_334 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3971 samples, validate on 2139 samples\n",
      "Epoch 1/2000\n",
      "3552/3971 [=========================>....] - ETA: 0s - loss: 0.4320 - mae: 0.4320 - mse: 0.7491\n",
      "Epoch 00001: val_loss improved from inf to 0.33272, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4160,  mae:0.4160,  mse:0.7015,  val_loss:0.3327,  val_mae:0.3327,  val_mse:0.4836,  \n",
      "3971/3971 [==============================] - 1s 250us/sample - loss: 0.4160 - mae: 0.4160 - mse: 0.7015 - val_loss: 0.3327 - val_mae: 0.3327 - val_mse: 0.4836\n",
      "Epoch 2/2000\n",
      "2496/3971 [=================>............] - ETA: 0s - loss: 0.2879 - mae: 0.2879 - mse: 0.4316\n",
      "Epoch 00002: val_loss improved from 0.33272 to 0.26711, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.2670 - mae: 0.2670 - mse: 0.3439 - val_loss: 0.2671 - val_mae: 0.2671 - val_mse: 0.3334\n",
      "Epoch 3/2000\n",
      "3744/3971 [===========================>..] - ETA: 0s - loss: 0.2224 - mae: 0.2224 - mse: 0.2618\n",
      "Epoch 00003: val_loss improved from 0.26711 to 0.25610, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 59us/sample - loss: 0.2235 - mae: 0.2235 - mse: 0.2598 - val_loss: 0.2561 - val_mae: 0.2561 - val_mse: 0.2599\n",
      "Epoch 4/2000\n",
      "3968/3971 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.2038 - mse: 0.2295\n",
      "Epoch 00004: val_loss improved from 0.25610 to 0.23298, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.2038 - mae: 0.2038 - mse: 0.2293 - val_loss: 0.2330 - val_mae: 0.2330 - val_mse: 0.2296\n",
      "Epoch 5/2000\n",
      "2400/3971 [=================>............] - ETA: 0s - loss: 0.1919 - mae: 0.1919 - mse: 0.2660\n",
      "Epoch 00005: val_loss did not improve from 0.23298\n",
      "3971/3971 [==============================] - 0s 53us/sample - loss: 0.1906 - mae: 0.1906 - mse: 0.2129 - val_loss: 0.2430 - val_mae: 0.2430 - val_mse: 0.3033\n",
      "Epoch 6/2000\n",
      "2624/3971 [==================>...........] - ETA: 0s - loss: 0.1907 - mae: 0.1907 - mse: 0.2487\n",
      "Epoch 00006: val_loss improved from 0.23298 to 0.21105, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1880 - mae: 0.1880 - mse: 0.2109 - val_loss: 0.2111 - val_mae: 0.2111 - val_mse: 0.2259\n",
      "Epoch 7/2000\n",
      "2752/3971 [===================>..........] - ETA: 0s - loss: 0.1867 - mae: 0.1867 - mse: 0.2052\n",
      "Epoch 00007: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1815 - mae: 0.1815 - mse: 0.1989 - val_loss: 0.2206 - val_mae: 0.2206 - val_mse: 0.2155\n",
      "Epoch 8/2000\n",
      "2368/3971 [================>.............] - ETA: 0s - loss: 0.1809 - mae: 0.1809 - mse: 0.2146\n",
      "Epoch 00008: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1785 - mae: 0.1785 - mse: 0.1976 - val_loss: 0.2249 - val_mae: 0.2249 - val_mse: 0.2221\n",
      "Epoch 9/2000\n",
      "3712/3971 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.1737 - mse: 0.1926\n",
      "Epoch 00009: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1752 - mae: 0.1752 - mse: 0.1902 - val_loss: 0.2682 - val_mae: 0.2682 - val_mse: 0.2631\n",
      "Epoch 10/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1717 - mae: 0.1717 - mse: 0.1859\n",
      "Epoch 00010: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1705 - mae: 0.1705 - mse: 0.1906 - val_loss: 0.2582 - val_mae: 0.2582 - val_mse: 0.2842\n",
      "Epoch 11/2000\n",
      "3936/3971 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.1714 - mse: 0.1907\n",
      "Epoch 00011: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1714 - mae: 0.1714 - mse: 0.1899 - val_loss: 0.2610 - val_mae: 0.2610 - val_mse: 0.2255\n",
      "Epoch 12/2000\n",
      "2880/3971 [====================>.........] - ETA: 0s - loss: 0.1671 - mae: 0.1671 - mse: 0.1764\n",
      "Epoch 00012: val_loss improved from 0.21105 to 0.20880, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1694 - mae: 0.1694 - mse: 0.1837 - val_loss: 0.2088 - val_mae: 0.2088 - val_mse: 0.2155\n",
      "Epoch 13/2000\n",
      "2464/3971 [=================>............] - ETA: 0s - loss: 0.1660 - mae: 0.1660 - mse: 0.1870\n",
      "Epoch 00013: val_loss improved from 0.20880 to 0.19382, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1683 - mae: 0.1683 - mse: 0.1838 - val_loss: 0.1938 - val_mae: 0.1938 - val_mse: 0.1975\n",
      "Epoch 14/2000\n",
      "3776/3971 [===========================>..] - ETA: 0s - loss: 0.1626 - mae: 0.1626 - mse: 0.1800\n",
      "Epoch 00014: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1624 - mae: 0.1624 - mse: 0.1754 - val_loss: 0.2003 - val_mae: 0.2003 - val_mse: 0.1999\n",
      "Epoch 15/2000\n",
      "3840/3971 [============================>.] - ETA: 0s - loss: 0.1627 - mae: 0.1627 - mse: 0.1794\n",
      "Epoch 00015: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 53us/sample - loss: 0.1616 - mae: 0.1616 - mse: 0.1752 - val_loss: 0.2115 - val_mae: 0.2115 - val_mse: 0.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/2000\n",
      "2624/3971 [==================>...........] - ETA: 0s - loss: 0.1592 - mae: 0.1592 - mse: 0.1330\n",
      "Epoch 00016: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1630 - mae: 0.1630 - mse: 0.1786 - val_loss: 0.2224 - val_mae: 0.2224 - val_mse: 0.2115\n",
      "Epoch 17/2000\n",
      "2336/3971 [================>.............] - ETA: 0s - loss: 0.1524 - mae: 0.1524 - mse: 0.2161\n",
      "Epoch 00017: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1586 - mae: 0.1586 - mse: 0.1732 - val_loss: 0.2274 - val_mae: 0.2274 - val_mse: 0.2038\n",
      "Epoch 18/2000\n",
      "3840/3971 [============================>.] - ETA: 0s - loss: 0.1576 - mae: 0.1576 - mse: 0.1752\n",
      "Epoch 00018: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1739 - val_loss: 0.3448 - val_mae: 0.3448 - val_mse: 0.3433\n",
      "Epoch 19/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1599 - mae: 0.1599 - mse: 0.1707\n",
      "Epoch 00019: val_loss improved from 0.19382 to 0.19274, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1588 - mae: 0.1588 - mse: 0.1752 - val_loss: 0.1927 - val_mae: 0.1927 - val_mse: 0.2017\n",
      "Epoch 20/2000\n",
      "3904/3971 [============================>.] - ETA: 0s - loss: 0.1506 - mae: 0.1506 - mse: 0.1402\n",
      "Epoch 00020: val_loss improved from 0.19274 to 0.18348, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 57us/sample - loss: 0.1529 - mae: 0.1529 - mse: 0.1685 - val_loss: 0.1835 - val_mae: 0.1835 - val_mse: 0.1827\n",
      "Epoch 21/2000\n",
      "3648/3971 [==========================>...] - ETA: 0s - loss: 0.1525 - mae: 0.1525 - mse: 0.1484\n",
      "Epoch 00021: val_loss did not improve from 0.18348\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1541 - mae: 0.1541 - mse: 0.1719 - val_loss: 0.2174 - val_mae: 0.2174 - val_mse: 0.2038\n",
      "Epoch 22/2000\n",
      "2560/3971 [==================>...........] - ETA: 0s - loss: 0.1500 - mae: 0.1500 - mse: 0.2123\n",
      "Epoch 00022: val_loss improved from 0.18348 to 0.18239, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1530 - mae: 0.1530 - mse: 0.1722 - val_loss: 0.1824 - val_mae: 0.1824 - val_mse: 0.1874\n",
      "Epoch 23/2000\n",
      "3776/3971 [===========================>..] - ETA: 0s - loss: 0.1504 - mae: 0.1504 - mse: 0.1446\n",
      "Epoch 00023: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1517 - mae: 0.1517 - mse: 0.1685 - val_loss: 0.2322 - val_mae: 0.2322 - val_mse: 0.2635\n",
      "Epoch 24/2000\n",
      "2496/3971 [=================>............] - ETA: 0s - loss: 0.1572 - mae: 0.1572 - mse: 0.2194\n",
      "Epoch 00024: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1532 - mae: 0.1532 - mse: 0.1689 - val_loss: 0.2030 - val_mae: 0.2030 - val_mse: 0.1952\n",
      "Epoch 25/2000\n",
      "2688/3971 [===================>..........] - ETA: 0s - loss: 0.1469 - mae: 0.1469 - mse: 0.1577\n",
      "Epoch 00025: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1489 - mae: 0.1489 - mse: 0.1643 - val_loss: 0.2167 - val_mae: 0.2167 - val_mse: 0.2544\n",
      "Epoch 26/2000\n",
      "2592/3971 [==================>...........] - ETA: 0s - loss: 0.1457 - mae: 0.1457 - mse: 0.1275\n",
      "Epoch 00026: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1493 - mae: 0.1493 - mse: 0.1662 - val_loss: 0.1866 - val_mae: 0.1866 - val_mse: 0.1955\n",
      "Epoch 27/2000\n",
      "2592/3971 [==================>...........] - ETA: 0s - loss: 0.1434 - mae: 0.1434 - mse: 0.1245\n",
      "Epoch 00027: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1485 - mae: 0.1485 - mse: 0.1654 - val_loss: 0.2202 - val_mae: 0.2202 - val_mse: 0.2389\n",
      "Epoch 28/2000\n",
      "2688/3971 [===================>..........] - ETA: 0s - loss: 0.1375 - mae: 0.1375 - mse: 0.1180\n",
      "Epoch 00028: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1485 - mae: 0.1485 - mse: 0.1672 - val_loss: 0.2082 - val_mae: 0.2082 - val_mse: 0.2156\n",
      "Epoch 29/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1469 - mae: 0.1469 - mse: 0.1591\n",
      "Epoch 00029: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1462 - mae: 0.1462 - mse: 0.1647 - val_loss: 0.2113 - val_mae: 0.2113 - val_mse: 0.2017\n",
      "Epoch 30/2000\n",
      "2816/3971 [====================>.........] - ETA: 0s - loss: 0.1483 - mae: 0.1483 - mse: 0.2016\n",
      "Epoch 00030: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1465 - mae: 0.1465 - mse: 0.1659 - val_loss: 0.2127 - val_mae: 0.2127 - val_mse: 0.1948\n",
      "Epoch 31/2000\n",
      "3712/3971 [===========================>..] - ETA: 0s - loss: 0.1495 - mae: 0.1495 - mse: 0.1712\n",
      "Epoch 00031: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1463 - mae: 0.1463 - mse: 0.1621 - val_loss: 0.1843 - val_mae: 0.1843 - val_mse: 0.1843\n",
      "Epoch 32/2000\n",
      "2656/3971 [===================>..........] - ETA: 0s - loss: 0.1446 - mae: 0.1446 - mse: 0.1226\n",
      "Epoch 00032: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1463 - mae: 0.1463 - mse: 0.1620 - val_loss: 0.2203 - val_mae: 0.2203 - val_mse: 0.2097\n",
      "1528/1528 - 0s - loss: 0.2050 - mae: 0.2050 - mse: 0.1352\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.425\n",
      "385\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_342 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3513 samples, validate on 2597 samples\n",
      "Epoch 1/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.4148 - mae: 0.4148 - mse: 0.6017\n",
      "Epoch 00001: val_loss improved from inf to 0.30770, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.3882,  mae:0.3882,  mse:0.5935,  val_loss:0.3077,  val_mae:0.3077,  val_mse:0.4319,  \n",
      "3513/3513 [==============================] - 1s 202us/sample - loss: 0.3882 - mae: 0.3882 - mse: 0.5935 - val_loss: 0.3077 - val_mae: 0.3077 - val_mse: 0.4319\n",
      "Epoch 2/2000\n",
      "2176/3513 [=================>............] - ETA: 0s - loss: 0.2968 - mae: 0.2968 - mse: 0.4197\n",
      "Epoch 00002: val_loss improved from 0.30770 to 0.27396, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 66us/sample - loss: 0.2765 - mae: 0.2765 - mse: 0.3758 - val_loss: 0.2740 - val_mae: 0.2740 - val_mse: 0.3754\n",
      "Epoch 3/2000\n",
      "2240/3513 [==================>...........] - ETA: 0s - loss: 0.2435 - mae: 0.2435 - mse: 0.3555\n",
      "Epoch 00003: val_loss improved from 0.27396 to 0.22949, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 65us/sample - loss: 0.2372 - mae: 0.2372 - mse: 0.2902 - val_loss: 0.2295 - val_mae: 0.2295 - val_mse: 0.2534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/2000\n",
      "2432/3513 [===================>..........] - ETA: 0s - loss: 0.2120 - mae: 0.2120 - mse: 0.2422\n",
      "Epoch 00004: val_loss improved from 0.22949 to 0.21983, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 64us/sample - loss: 0.2162 - mae: 0.2162 - mse: 0.2532 - val_loss: 0.2198 - val_mae: 0.2198 - val_mse: 0.2476\n",
      "Epoch 5/2000\n",
      "2336/3513 [==================>...........] - ETA: 0s - loss: 0.2069 - mae: 0.2069 - mse: 0.2829\n",
      "Epoch 00005: val_loss improved from 0.21983 to 0.20917, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 64us/sample - loss: 0.2053 - mae: 0.2053 - mse: 0.2406 - val_loss: 0.2092 - val_mae: 0.2092 - val_mse: 0.2179\n",
      "Epoch 6/2000\n",
      "2304/3513 [==================>...........] - ETA: 0s - loss: 0.2048 - mae: 0.2048 - mse: 0.2467\n",
      "Epoch 00006: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1999 - mae: 0.1999 - mse: 0.2336 - val_loss: 0.2130 - val_mae: 0.2130 - val_mse: 0.2109\n",
      "Epoch 7/2000\n",
      "2144/3513 [=================>............] - ETA: 0s - loss: 0.1975 - mae: 0.1975 - mse: 0.2368\n",
      "Epoch 00007: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 61us/sample - loss: 0.1957 - mae: 0.1957 - mse: 0.2285 - val_loss: 0.2360 - val_mae: 0.2360 - val_mse: 0.2275\n",
      "Epoch 8/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1936 - mae: 0.1936 - mse: 0.1805\n",
      "Epoch 00008: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1909 - mae: 0.1909 - mse: 0.2184 - val_loss: 0.2548 - val_mae: 0.2548 - val_mse: 0.2968\n",
      "Epoch 9/2000\n",
      "3488/3513 [============================>.] - ETA: 0s - loss: 0.1866 - mae: 0.1866 - mse: 0.2168\n",
      "Epoch 00009: val_loss improved from 0.20917 to 0.19422, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 68us/sample - loss: 0.1869 - mae: 0.1869 - mse: 0.2163 - val_loss: 0.1942 - val_mae: 0.1942 - val_mse: 0.1871\n",
      "Epoch 10/2000\n",
      "2144/3513 [=================>............] - ETA: 0s - loss: 0.1841 - mae: 0.1841 - mse: 0.2199\n",
      "Epoch 00010: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 63us/sample - loss: 0.1836 - mae: 0.1836 - mse: 0.2162 - val_loss: 0.2094 - val_mae: 0.2094 - val_mse: 0.1834\n",
      "Epoch 11/2000\n",
      "2784/3513 [======================>.......] - ETA: 0s - loss: 0.1847 - mae: 0.1847 - mse: 0.2356\n",
      "Epoch 00011: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1774 - mae: 0.1774 - mse: 0.2055 - val_loss: 0.2675 - val_mae: 0.2675 - val_mse: 0.2884\n",
      "Epoch 12/2000\n",
      "2656/3513 [=====================>........] - ETA: 0s - loss: 0.1831 - mae: 0.1831 - mse: 0.2363\n",
      "Epoch 00012: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 56us/sample - loss: 0.1800 - mae: 0.1800 - mse: 0.2061 - val_loss: 0.2002 - val_mae: 0.2002 - val_mse: 0.2034\n",
      "Epoch 13/2000\n",
      "1920/3513 [===============>..............] - ETA: 0s - loss: 0.1731 - mae: 0.1731 - mse: 0.2227\n",
      "Epoch 00013: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1753 - mae: 0.1753 - mse: 0.2033 - val_loss: 0.2137 - val_mae: 0.2137 - val_mse: 0.2090\n",
      "Epoch 14/2000\n",
      "2720/3513 [======================>.......] - ETA: 0s - loss: 0.1725 - mae: 0.1725 - mse: 0.1909\n",
      "Epoch 00014: val_loss improved from 0.19422 to 0.18293, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 62us/sample - loss: 0.1747 - mae: 0.1747 - mse: 0.2008 - val_loss: 0.1829 - val_mae: 0.1829 - val_mse: 0.1824\n",
      "Epoch 15/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1696 - mae: 0.1696 - mse: 0.1127\n",
      "Epoch 00015: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1763 - mae: 0.1763 - mse: 0.2041 - val_loss: 0.2334 - val_mae: 0.2334 - val_mse: 0.2186\n",
      "Epoch 16/2000\n",
      "2432/3513 [===================>..........] - ETA: 0s - loss: 0.1699 - mae: 0.1699 - mse: 0.1547\n",
      "Epoch 00016: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1721 - mae: 0.1721 - mse: 0.2000 - val_loss: 0.1864 - val_mae: 0.1864 - val_mse: 0.1782\n",
      "Epoch 17/2000\n",
      "2752/3513 [======================>.......] - ETA: 0s - loss: 0.1730 - mae: 0.1730 - mse: 0.2250\n",
      "Epoch 00017: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 54us/sample - loss: 0.1692 - mae: 0.1692 - mse: 0.1945 - val_loss: 0.2193 - val_mae: 0.2193 - val_mse: 0.2096\n",
      "Epoch 18/2000\n",
      "2624/3513 [=====================>........] - ETA: 0s - loss: 0.1781 - mae: 0.1781 - mse: 0.2396\n",
      "Epoch 00018: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1700 - mae: 0.1700 - mse: 0.1987 - val_loss: 0.1835 - val_mae: 0.1835 - val_mse: 0.1785\n",
      "Epoch 19/2000\n",
      "2816/3513 [=======================>......] - ETA: 0s - loss: 0.1707 - mae: 0.1707 - mse: 0.2130\n",
      "Epoch 00019: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1672 - mae: 0.1672 - mse: 0.1911 - val_loss: 0.1984 - val_mae: 0.1984 - val_mse: 0.1844\n",
      "Epoch 20/2000\n",
      "2752/3513 [======================>.......] - ETA: 0s - loss: 0.1646 - mae: 0.1646 - mse: 0.1814\n",
      "Epoch 00020: val_loss improved from 0.18293 to 0.17534, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1670 - mae: 0.1670 - mse: 0.1951 - val_loss: 0.1753 - val_mae: 0.1753 - val_mse: 0.1664\n",
      "Epoch 21/2000\n",
      "2208/3513 [=================>............] - ETA: 0s - loss: 0.1619 - mae: 0.1619 - mse: 0.1484\n",
      "Epoch 00021: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1649 - mae: 0.1649 - mse: 0.1901 - val_loss: 0.1795 - val_mae: 0.1795 - val_mse: 0.1678\n",
      "Epoch 22/2000\n",
      "2496/3513 [====================>.........] - ETA: 0s - loss: 0.1626 - mae: 0.1626 - mse: 0.2238\n",
      "Epoch 00022: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1610 - mae: 0.1610 - mse: 0.1890 - val_loss: 0.1860 - val_mae: 0.1860 - val_mse: 0.1743\n",
      "Epoch 23/2000\n",
      "2496/3513 [====================>.........] - ETA: 0s - loss: 0.1633 - mae: 0.1633 - mse: 0.2213\n",
      "Epoch 00023: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1618 - mae: 0.1618 - mse: 0.1849 - val_loss: 0.1762 - val_mae: 0.1762 - val_mse: 0.1736\n",
      "Epoch 24/2000\n",
      "2400/3513 [===================>..........] - ETA: 0s - loss: 0.1585 - mae: 0.1585 - mse: 0.1811\n",
      "Epoch 00024: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1622 - mae: 0.1622 - mse: 0.1873 - val_loss: 0.1816 - val_mae: 0.1816 - val_mse: 0.1710\n",
      "Epoch 25/2000\n",
      "2400/3513 [===================>..........] - ETA: 0s - loss: 0.1618 - mae: 0.1618 - mse: 0.2308\n",
      "Epoch 00025: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1619 - mae: 0.1619 - mse: 0.1866 - val_loss: 0.1925 - val_mae: 0.1925 - val_mse: 0.1866\n",
      "Epoch 26/2000\n",
      "2336/3513 [==================>...........] - ETA: 0s - loss: 0.1571 - mae: 0.1571 - mse: 0.1795\n",
      "Epoch 00026: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1569 - mae: 0.1569 - mse: 0.1809 - val_loss: 0.1816 - val_mae: 0.1816 - val_mse: 0.1749\n",
      "Epoch 27/2000\n",
      "2592/3513 [=====================>........] - ETA: 0s - loss: 0.1634 - mae: 0.1634 - mse: 0.2145\n",
      "Epoch 00027: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 56us/sample - loss: 0.1597 - mae: 0.1597 - mse: 0.1839 - val_loss: 0.1768 - val_mae: 0.1768 - val_mse: 0.1709\n",
      "Epoch 28/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1564 - mae: 0.1564 - mse: 0.1739\n",
      "Epoch 00028: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1577 - mae: 0.1577 - mse: 0.1808 - val_loss: 0.1836 - val_mae: 0.1836 - val_mse: 0.1851\n",
      "Epoch 29/2000\n",
      "2464/3513 [====================>.........] - ETA: 0s - loss: 0.1550 - mae: 0.1550 - mse: 0.2184\n",
      "Epoch 00029: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 57us/sample - loss: 0.1540 - mae: 0.1540 - mse: 0.1804 - val_loss: 0.1842 - val_mae: 0.1842 - val_mse: 0.1832\n",
      "Epoch 30/2000\n",
      "2784/3513 [======================>.......] - ETA: 0s - loss: 0.1572 - mae: 0.1572 - mse: 0.1723\n",
      "Epoch 00030: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 54us/sample - loss: 0.1547 - mae: 0.1547 - mse: 0.1812 - val_loss: 0.1770 - val_mae: 0.1770 - val_mse: 0.1812\n",
      "1528/1528 - 0s - loss: 0.1756 - mae: 0.1756 - mse: 0.1300\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.5\n",
      "385\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_350 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3055 samples, validate on 3055 samples\n",
      "Epoch 1/2000\n",
      "2848/3055 [==========================>...] - ETA: 0s - loss: 0.4273 - mae: 0.4273 - mse: 0.6682\n",
      "Epoch 00001: val_loss improved from inf to 0.33730, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4159,  mae:0.4159,  mse:0.6402,  val_loss:0.3373,  val_mae:0.3373,  val_mse:0.5570,  \n",
      "3055/3055 [==============================] - 1s 224us/sample - loss: 0.4159 - mae: 0.4159 - mse: 0.6402 - val_loss: 0.3373 - val_mae: 0.3373 - val_mse: 0.5570\n",
      "Epoch 2/2000\n",
      "1984/3055 [==================>...........] - ETA: 0s - loss: 0.2919 - mae: 0.2919 - mse: 0.3765\n",
      "Epoch 00002: val_loss improved from 0.33730 to 0.27766, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.2771 - mae: 0.2771 - mse: 0.3589 - val_loss: 0.2777 - val_mae: 0.2777 - val_mse: 0.4036\n",
      "Epoch 3/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.2372 - mae: 0.2372 - mse: 0.2966\n",
      "Epoch 00003: val_loss did not improve from 0.27766\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.2372 - mae: 0.2372 - mse: 0.2754 - val_loss: 0.2832 - val_mae: 0.2832 - val_mse: 0.4028\n",
      "Epoch 4/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.2012 - mae: 0.2012 - mse: 0.1474\n",
      "Epoch 00004: val_loss improved from 0.27766 to 0.24788, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 74us/sample - loss: 0.2141 - mae: 0.2141 - mse: 0.2258 - val_loss: 0.2479 - val_mae: 0.2479 - val_mse: 0.3209\n",
      "Epoch 5/2000\n",
      "2336/3055 [=====================>........] - ETA: 0s - loss: 0.2055 - mae: 0.2055 - mse: 0.2269\n",
      "Epoch 00005: val_loss improved from 0.24788 to 0.22283, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.2012 - mae: 0.2012 - mse: 0.2089 - val_loss: 0.2228 - val_mae: 0.2228 - val_mse: 0.2749\n",
      "Epoch 6/2000\n",
      "2368/3055 [======================>.......] - ETA: 0s - loss: 0.1968 - mae: 0.1968 - mse: 0.2199\n",
      "Epoch 00006: val_loss did not improve from 0.22283\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1938 - mae: 0.1938 - mse: 0.1977 - val_loss: 0.2484 - val_mae: 0.2484 - val_mse: 0.2729\n",
      "Epoch 7/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.1908 - mae: 0.1908 - mse: 0.2077\n",
      "Epoch 00007: val_loss did not improve from 0.22283\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1915 - mae: 0.1915 - mse: 0.1948 - val_loss: 0.2359 - val_mae: 0.2359 - val_mse: 0.2721\n",
      "Epoch 8/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1876 - mae: 0.1876 - mse: 0.1633\n",
      "Epoch 00008: val_loss improved from 0.22283 to 0.20990, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 68us/sample - loss: 0.1867 - mae: 0.1867 - mse: 0.1886 - val_loss: 0.2099 - val_mae: 0.2099 - val_mse: 0.2282\n",
      "Epoch 9/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1855 - mae: 0.1855 - mse: 0.1997\n",
      "Epoch 00009: val_loss did not improve from 0.20990\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1824 - mae: 0.1824 - mse: 0.1846 - val_loss: 0.2110 - val_mae: 0.2110 - val_mse: 0.2568\n",
      "Epoch 10/2000\n",
      "2272/3055 [=====================>........] - ETA: 0s - loss: 0.1727 - mae: 0.1727 - mse: 0.1500\n",
      "Epoch 00010: val_loss improved from 0.20990 to 0.19393, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 72us/sample - loss: 0.1777 - mae: 0.1777 - mse: 0.1825 - val_loss: 0.1939 - val_mae: 0.1939 - val_mse: 0.2131\n",
      "Epoch 11/2000\n",
      "2304/3055 [=====================>........] - ETA: 0s - loss: 0.1857 - mae: 0.1857 - mse: 0.2110\n",
      "Epoch 00011: val_loss did not improve from 0.19393\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1776 - mae: 0.1776 - mse: 0.1789 - val_loss: 0.1990 - val_mae: 0.1990 - val_mse: 0.2172\n",
      "Epoch 12/2000\n",
      "2176/3055 [====================>.........] - ETA: 0s - loss: 0.1758 - mae: 0.1758 - mse: 0.1959\n",
      "Epoch 00012: val_loss improved from 0.19393 to 0.19333, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.1773 - mae: 0.1773 - mse: 0.1761 - val_loss: 0.1933 - val_mae: 0.1933 - val_mse: 0.2148\n",
      "Epoch 13/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1700 - mae: 0.1700 - mse: 0.1116\n",
      "Epoch 00013: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 67us/sample - loss: 0.1748 - mae: 0.1748 - mse: 0.1790 - val_loss: 0.2030 - val_mae: 0.2030 - val_mse: 0.2317\n",
      "Epoch 14/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1753 - mae: 0.1753 - mse: 0.1981\n",
      "Epoch 00014: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1697 - mae: 0.1697 - mse: 0.1725 - val_loss: 0.1956 - val_mae: 0.1956 - val_mse: 0.2191\n",
      "Epoch 15/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1667 - mae: 0.1667 - mse: 0.1454\n",
      "Epoch 00015: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1686 - mae: 0.1686 - mse: 0.1695 - val_loss: 0.2227 - val_mae: 0.2227 - val_mse: 0.2741\n",
      "Epoch 16/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1727 - mae: 0.1727 - mse: 0.1979\n",
      "Epoch 00016: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 67us/sample - loss: 0.1677 - mae: 0.1677 - mse: 0.1691 - val_loss: 0.1951 - val_mae: 0.1951 - val_mse: 0.2332\n",
      "Epoch 17/2000\n",
      "2208/3055 [====================>.........] - ETA: 0s - loss: 0.1665 - mae: 0.1665 - mse: 0.1964\n",
      "Epoch 00017: val_loss improved from 0.19333 to 0.18515, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 72us/sample - loss: 0.1641 - mae: 0.1641 - mse: 0.1654 - val_loss: 0.1852 - val_mae: 0.1852 - val_mse: 0.2031\n",
      "Epoch 18/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/3055 [=====================>........] - ETA: 0s - loss: 0.1640 - mae: 0.1640 - mse: 0.1863\n",
      "Epoch 00018: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1639 - mae: 0.1639 - mse: 0.1671 - val_loss: 0.2228 - val_mae: 0.2228 - val_mse: 0.2724\n",
      "Epoch 19/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1656 - mae: 0.1656 - mse: 0.1758\n",
      "Epoch 00019: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1650 - mae: 0.1650 - mse: 0.1660 - val_loss: 0.2010 - val_mae: 0.2010 - val_mse: 0.2104\n",
      "Epoch 20/2000\n",
      "2656/3055 [=========================>....] - ETA: 0s - loss: 0.1590 - mae: 0.1590 - mse: 0.1334\n",
      "Epoch 00020: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1632 - mae: 0.1632 - mse: 0.1652 - val_loss: 0.2712 - val_mae: 0.2712 - val_mse: 0.3200\n",
      "Epoch 21/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1570 - mae: 0.1570 - mse: 0.1335\n",
      "Epoch 00021: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1581 - mae: 0.1581 - mse: 0.1619 - val_loss: 0.2015 - val_mae: 0.2015 - val_mse: 0.2066\n",
      "Epoch 22/2000\n",
      "2752/3055 [==========================>...] - ETA: 0s - loss: 0.1561 - mae: 0.1561 - mse: 0.1277\n",
      "Epoch 00022: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1573 - mae: 0.1573 - mse: 0.1575 - val_loss: 0.1976 - val_mae: 0.1976 - val_mse: 0.2077\n",
      "Epoch 23/2000\n",
      "2688/3055 [=========================>....] - ETA: 0s - loss: 0.1600 - mae: 0.1600 - mse: 0.1728\n",
      "Epoch 00023: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1615 - val_loss: 0.2351 - val_mae: 0.2351 - val_mse: 0.2278\n",
      "Epoch 24/2000\n",
      "2400/3055 [======================>.......] - ETA: 0s - loss: 0.1568 - mae: 0.1568 - mse: 0.1407\n",
      "Epoch 00024: val_loss improved from 0.18515 to 0.18482, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 71us/sample - loss: 0.1558 - mae: 0.1558 - mse: 0.1615 - val_loss: 0.1848 - val_mae: 0.1848 - val_mse: 0.1948\n",
      "Epoch 25/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1614 - mae: 0.1614 - mse: 0.1810\n",
      "Epoch 00025: val_loss did not improve from 0.18482\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1600 - mae: 0.1600 - mse: 0.1641 - val_loss: 0.1965 - val_mae: 0.1965 - val_mse: 0.2021\n",
      "Epoch 26/2000\n",
      "2528/3055 [=======================>......] - ETA: 0s - loss: 0.1527 - mae: 0.1527 - mse: 0.1652\n",
      "Epoch 00026: val_loss improved from 0.18482 to 0.18444, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 70us/sample - loss: 0.1536 - mae: 0.1536 - mse: 0.1526 - val_loss: 0.1844 - val_mae: 0.1844 - val_mse: 0.2016\n",
      "Epoch 27/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1499 - mae: 0.1499 - mse: 0.1250\n",
      "Epoch 00027: val_loss did not improve from 0.18444\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1549 - mae: 0.1549 - mse: 0.1559 - val_loss: 0.1994 - val_mae: 0.1994 - val_mse: 0.2367\n",
      "Epoch 28/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1480 - mae: 0.1480 - mse: 0.1247\n",
      "Epoch 00028: val_loss did not improve from 0.18444\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1538 - mae: 0.1538 - mse: 0.1594 - val_loss: 0.1884 - val_mae: 0.1884 - val_mse: 0.2100\n",
      "Epoch 29/2000\n",
      "2912/3055 [===========================>..] - ETA: 0s - loss: 0.1506 - mae: 0.1506 - mse: 0.1573\n",
      "Epoch 00029: val_loss improved from 0.18444 to 0.17581, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1493 - mae: 0.1493 - mse: 0.1524 - val_loss: 0.1758 - val_mae: 0.1758 - val_mse: 0.1948\n",
      "Epoch 30/2000\n",
      "2112/3055 [===================>..........] - ETA: 0s - loss: 0.1569 - mae: 0.1569 - mse: 0.1885\n",
      "Epoch 00030: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 68us/sample - loss: 0.1530 - mae: 0.1530 - mse: 0.1551 - val_loss: 0.1766 - val_mae: 0.1766 - val_mse: 0.2017\n",
      "Epoch 31/2000\n",
      "2112/3055 [===================>..........] - ETA: 0s - loss: 0.1445 - mae: 0.1445 - mse: 0.0826\n",
      "Epoch 00031: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1466 - mae: 0.1466 - mse: 0.1490 - val_loss: 0.2106 - val_mae: 0.2106 - val_mse: 0.2580\n",
      "Epoch 32/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1422 - mae: 0.1422 - mse: 0.1191\n",
      "Epoch 00032: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1489 - mae: 0.1489 - mse: 0.1502 - val_loss: 0.2200 - val_mae: 0.2200 - val_mse: 0.2167\n",
      "Epoch 33/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1484 - mae: 0.1484 - mse: 0.1572\n",
      "Epoch 00033: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1483 - mae: 0.1483 - mse: 0.1498 - val_loss: 0.2273 - val_mae: 0.2273 - val_mse: 0.2766\n",
      "Epoch 34/2000\n",
      "2752/3055 [==========================>...] - ETA: 0s - loss: 0.1493 - mae: 0.1493 - mse: 0.1595\n",
      "Epoch 00034: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1482 - mae: 0.1482 - mse: 0.1507 - val_loss: 0.1807 - val_mae: 0.1807 - val_mse: 0.2066\n",
      "Epoch 35/2000\n",
      "2848/3055 [==========================>...] - ETA: 0s - loss: 0.1494 - mae: 0.1494 - mse: 0.1559\n",
      "Epoch 00035: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 60us/sample - loss: 0.1473 - mae: 0.1473 - mse: 0.1485 - val_loss: 0.2192 - val_mae: 0.2192 - val_mse: 0.2659\n",
      "Epoch 36/2000\n",
      "2528/3055 [=======================>......] - ETA: 0s - loss: 0.1497 - mae: 0.1497 - mse: 0.1661\n",
      "Epoch 00036: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1465 - mae: 0.1465 - mse: 0.1491 - val_loss: 0.1900 - val_mae: 0.1900 - val_mse: 0.1981\n",
      "Epoch 37/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1467 - mae: 0.1467 - mse: 0.1638\n",
      "Epoch 00037: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1452 - mae: 0.1452 - mse: 0.1490 - val_loss: 0.1777 - val_mae: 0.1777 - val_mse: 0.1892\n",
      "Epoch 38/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.1490 - mae: 0.1490 - mse: 0.1677\n",
      "Epoch 00038: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1459 - mae: 0.1459 - mse: 0.1480 - val_loss: 0.2146 - val_mae: 0.2146 - val_mse: 0.2284\n",
      "Epoch 39/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1500 - mae: 0.1500 - mse: 0.1669\n",
      "Epoch 00039: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1453 - mae: 0.1453 - mse: 0.1472 - val_loss: 0.1829 - val_mae: 0.1829 - val_mse: 0.2019\n",
      "1528/1528 - 0s - loss: 0.1805 - mae: 0.1805 - mse: 0.1341\n"
     ]
    }
   ],
   "source": [
    "#patience_d, training_pct_d, n_layer_d, n_unit_d, activation_d, loss_d, opt_d, val_pct_d \n",
    "r = varyParams(ml_data, defaults, init_grid, total_frac, start_str, end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.2: 0.106797,\n",
       " 0.275: 0.14296195,\n",
       " 0.35: 0.13522059,\n",
       " 0.425: 0.1300026,\n",
       " 0.5: 0.13410667}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  3\n",
      "n_unit:  100\n",
      "activation:  relu\n",
      "loss:  mse\n",
      "opt:  adam\n",
      "val_pct:  0.2\n",
      "385\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 100)               38600     \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,901\n",
      "Trainable params: 58,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 6111 steps, validate for 6111 steps\n",
      "Epoch 1/2000\n",
      "6076/6111 [============================>.] - ETA: 0s - loss: 0.1852 - mae: 0.2081 - mse: 0.1852\n",
      "Epoch 00001: val_loss improved from inf to 0.15277, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.1846,  mae:0.2078,  mse:0.1846,  val_loss:0.1528,  val_mae:0.1835,  val_mse:0.1528,  \n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1846 - mae: 0.2078 - mse: 0.1846 - val_loss: 0.1528 - val_mae: 0.1835 - val_mse: 0.1528\n",
      "Epoch 2/2000\n",
      "6066/6111 [============================>.] - ETA: 0s - loss: 0.1472 - mae: 0.1765 - mse: 0.1472\n",
      "Epoch 00002: val_loss improved from 0.15277 to 0.13289, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1466 - mae: 0.1763 - mse: 0.1466 - val_loss: 0.1329 - val_mae: 0.1611 - val_mse: 0.1329\n",
      "Epoch 3/2000\n",
      "6097/6111 [============================>.] - ETA: 0s - loss: 0.1356 - mae: 0.1654 - mse: 0.1356\n",
      "Epoch 00003: val_loss improved from 0.13289 to 0.12437, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1354 - mae: 0.1654 - mse: 0.1354 - val_loss: 0.1244 - val_mae: 0.1523 - val_mse: 0.1244\n",
      "Epoch 4/2000\n",
      "6094/6111 [============================>.] - ETA: 0s - loss: 0.1295 - mae: 0.1596 - mse: 0.1295\n",
      "Epoch 00004: val_loss improved from 0.12437 to 0.12210, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1293 - mae: 0.1595 - mse: 0.1293 - val_loss: 0.1221 - val_mae: 0.1513 - val_mse: 0.1221\n",
      "Epoch 5/2000\n",
      "6095/6111 [============================>.] - ETA: 0s - loss: 0.1252 - mae: 0.1553 - mse: 0.1252\n",
      "Epoch 00005: val_loss improved from 0.12210 to 0.11988, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1251 - mae: 0.1552 - mse: 0.1251 - val_loss: 0.1199 - val_mae: 0.1490 - val_mse: 0.1199\n",
      "Epoch 6/2000\n",
      "6082/6111 [============================>.] - ETA: 0s - loss: 0.1223 - mae: 0.1522 - mse: 0.1223\n",
      "Epoch 00006: val_loss improved from 0.11988 to 0.11705, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1219 - mae: 0.1521 - mse: 0.1220 - val_loss: 0.1171 - val_mae: 0.1470 - val_mse: 0.1171\n",
      "Epoch 7/2000\n",
      "6073/6111 [============================>.] - ETA: 0s - loss: 0.1198 - mae: 0.1497 - mse: 0.1198\n",
      "Epoch 00007: val_loss did not improve from 0.11705\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1194 - mae: 0.1496 - mse: 0.1194 - val_loss: 0.1176 - val_mae: 0.1490 - val_mse: 0.1176\n",
      "Epoch 8/2000\n",
      "6109/6111 [============================>.] - ETA: 0s - loss: 0.1171 - mae: 0.1473 - mse: 0.1171\n",
      "Epoch 00008: val_loss did not improve from 0.11705\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1171 - mae: 0.1473 - mse: 0.1171 - val_loss: 0.1178 - val_mae: 0.1504 - val_mse: 0.1178\n",
      "Epoch 9/2000\n",
      "6095/6111 [============================>.] - ETA: 0s - loss: 0.1154 - mae: 0.1455 - mse: 0.1154\n",
      "Epoch 00009: val_loss improved from 0.11705 to 0.11128, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1152 - mae: 0.1454 - mse: 0.1152 - val_loss: 0.1113 - val_mae: 0.1411 - val_mse: 0.1113\n",
      "Epoch 10/2000\n",
      "6095/6111 [============================>.] - ETA: 0s - loss: 0.1138 - mae: 0.1440 - mse: 0.1138\n",
      "Epoch 00010: val_loss improved from 0.11128 to 0.11102, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1136 - mae: 0.1439 - mse: 0.1136 - val_loss: 0.1110 - val_mae: 0.1416 - val_mse: 0.1110\n",
      "Epoch 11/2000\n",
      "6085/6111 [============================>.] - ETA: 0s - loss: 0.1121 - mae: 0.1421 - mse: 0.1121\n",
      "Epoch 00011: val_loss did not improve from 0.11102\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1118 - mae: 0.1421 - mse: 0.1118 - val_loss: 0.1112 - val_mae: 0.1408 - val_mse: 0.1112\n",
      "Epoch 12/2000\n",
      "6089/6111 [============================>.] - ETA: 0s - loss: 0.1106 - mae: 0.1407 - mse: 0.1106\n",
      "Epoch 00012: val_loss improved from 0.11102 to 0.11051, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1103 - mae: 0.1406 - mse: 0.1103 - val_loss: 0.1105 - val_mae: 0.1412 - val_mse: 0.1105\n",
      "Epoch 13/2000\n",
      "6081/6111 [============================>.] - ETA: 0s - loss: 0.1094 - mae: 0.1392 - mse: 0.1094\n",
      "Epoch 00013: val_loss improved from 0.11051 to 0.10907, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1091 - mae: 0.1392 - mse: 0.1091 - val_loss: 0.1091 - val_mae: 0.1401 - val_mse: 0.1091\n",
      "Epoch 14/2000\n",
      "6081/6111 [============================>.] - ETA: 0s - loss: 0.1085 - mae: 0.1387 - mse: 0.1085\n",
      "Epoch 00014: val_loss did not improve from 0.10907\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1082 - mae: 0.1387 - mse: 0.1082 - val_loss: 0.1129 - val_mae: 0.1420 - val_mse: 0.1129\n",
      "Epoch 15/2000\n",
      "6098/6111 [============================>.] - ETA: 0s - loss: 0.1074 - mae: 0.1379 - mse: 0.1074\n",
      "Epoch 00015: val_loss improved from 0.10907 to 0.10587, saving model to model_checkpoint.h5\n",
      "6111/6111 [==============================] - 9s 2ms/step - loss: 0.1072 - mae: 0.1378 - mse: 0.1072 - val_loss: 0.1059 - val_mae: 0.1369 - val_mse: 0.1059\n",
      "Epoch 16/2000\n",
      "6096/6111 [============================>.] - ETA: 0s - loss: 0.1056 - mae: 0.1363 - mse: 0.1056\n",
      "Epoch 00016: val_loss did not improve from 0.10587\n",
      "6111/6111 [==============================] - 10s 2ms/step - loss: 0.1054 - mae: 0.1363 - mse: 0.1055 - val_loss: 0.1101 - val_mae: 0.1416 - val_mse: 0.1101\n",
      "Epoch 17/2000\n",
      "1259/6111 [=====>........................] - ETA: 4s - loss: 0.1192 - mae: 0.1386 - mse: 0.1192"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_frac = 1\n",
    "mse = evaluate_model(ml_data, total_frac, start_str, end_str, 10, .8, 3, 100, 'relu', \n",
    "                   'mse', 'adam', .2, batch_size=10, norm=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08649507"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29410044483059083"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
