{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_core = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if stacked:\n",
    "    ml_data = pd.read_csv('/home/rgur/efrc/prep_data/all_no_norm/stacked.csv')\n",
    "else:\n",
    "    ml_data = pd.read_csv('/home/rgur/efrc/prep_data/all_no_norm/ml_data.csv')\n",
    "    #ml_data = pd.read_csv('/home/rgur/efrc/prep_data/all_v1/ml_data.csv')\n",
    "    #ml_data = pd.read_csv('./all_mofs/stacked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = ml_data.drop([col for col in ml_data.keys() if 'Smiles' in col] + [col for col in \n",
    "                                                                             ml_data.keys() if 'Unnamed' in col] + ['#_of_Linkers'] + ['Metal_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ml_data.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution when running below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_data = ml_data.drop(['norm_Dom._Pore_(ang.)', 'norm_Max._Pore_(ang.)',\n",
    "# 'norm_Void_Fraction',\n",
    "# 'norm_Surf._Area_(m2/g)',\n",
    "# 'norm_Vol._Surf._Area',\n",
    "# 'norm_Density'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_col(df, col_name):\n",
    "    \n",
    "    nrow = len(df)\n",
    "    mean = df[col_name].mean()\n",
    "    std = df[col_name].std()\n",
    "    \n",
    "    df['norm_' + col_name] = (df[col_name] - mean) / std\n",
    "    #df['mean_' + col_name] = [mean for i in range(nrow)]\n",
    "    #df['std_' + col_name] = [std for i in range(nrow)]\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default is below \n",
    "\n",
    "other_props = ['norm_Dom._Pore_(ang.)',\n",
    " 'norm_Max._Pore_(ang.)',\n",
    " 'norm_Void_Fraction',\n",
    " 'norm_Surf._Area_(m2/g)',\n",
    " 'norm_Vol._Surf._Area',\n",
    " 'norm_Density',\n",
    "  'norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "\n",
    "# other_props = ['norm_valence_pa',\n",
    "#    'norm_atomic_rad_pa_(angstroms)',\n",
    "#      'norm_affinity_pa_(eV)',\n",
    "#        'norm_ionization_potential_pa_(eV)',\n",
    "#            'norm_electronegativity_pa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge gravimetric uptake\n",
    "if not stacked:\n",
    "    y_data = pd.read_excel('/home/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx')\n",
    "    ml_data = ml_data.join(y_data[['Crystal ID#', 'CH4 cm3/g 35 bar']].set_index('Crystal ID#'), on='Crystal_ID#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all spaced cols with _\n",
    "if not stacked:\n",
    "    for key in ml_data.keys():\n",
    "        if ' ' in key:\n",
    "            new_key = key.replace(' ', '_')\n",
    "            ml_data[new_key] = ml_data[key]\n",
    "            ml_data = ml_data.drop(key, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge gravimetric uptake\n",
    "if not stacked:\n",
    "    y_data = pd.read_excel('/home/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx')\n",
    "    ml_data = ml_data.join(y_data[['Crystal ID#', 'CH4 cm3/g 35 bar']].set_index('Crystal ID#'), on='Crystal_ID#')\n",
    "\n",
    "#replace all spaced cols with _\n",
    "if not stacked:\n",
    "    for key in ml_data.keys():\n",
    "        if ' ' in key:\n",
    "            new_key = key.replace(' ', '_')\n",
    "            ml_data[new_key] = ml_data[key]\n",
    "            ml_data = ml_data.drop(key, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat(s):\n",
    "    '''\n",
    "    Returns interpenetration\n",
    "    '''\n",
    "    if 'cat' in s:\n",
    "        return int(s.split('_')[-1][0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to create weighted interpenetration feature and norm it\n",
    "# weighted_cat = [get_cat(i) for i in ml_data['filename'].tolist()]\n",
    "# ml_data['weighted_cat'] = weighted_cat\n",
    "# norm_col(ml_data, 'weighted_cat')\n",
    "# other_props.append('norm_weighted_cat')\n",
    "# ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to create one-hot interpenetration feature and norm it\n",
    "one_hot = [[0]*4 for i in range(len(ml_data))]\n",
    "for i, f in enumerate(ml_data['filename'].tolist()):\n",
    "    one_hot[i][get_cat(f)] = 1\n",
    "oh_1 = []\n",
    "oh_2 = []\n",
    "oh_3 = []\n",
    "oh_4 = []\n",
    "for i in one_hot:\n",
    "    oh_1.append(i[0])\n",
    "    oh_2.append(i[1])\n",
    "    oh_3.append(i[2])\n",
    "    oh_4.append(i[3])\n",
    "ml_data['oh_1'] = oh_1\n",
    "ml_data['oh_2'] = oh_2\n",
    "ml_data['oh_3'] = oh_3\n",
    "ml_data['oh_4'] = oh_4\n",
    "other_props.extend(['oh_1','oh_2','oh_3','oh_4'])\n",
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only do if using grav. uptake as property\n",
    "#norm gravimetric uptake\n",
    "if not stacked:\n",
    "    target_mean, target_std = norm_col(ml_data, 'CH4_cm3/g_35_bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked:\n",
    "    target_mean, target_std = norm_col(ml_data, 'vol_uptake')\n",
    "    property_used = 'norm_vol_uptake'\n",
    "    #target_mean = float(ml_data[property_used.replace('norm', 'mean')][0])\n",
    "    #target_std = float(ml_data[property_used.replace('norm', 'std')][0])\n",
    "else:\n",
    "    property_used = 'norm_CH4_cm3/g_35_bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ml_data.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjustable parameters\n",
    "total_frac = 1\n",
    "start_str = 'Density'\n",
    "end_str = 'norm_Dom._Pore_(ang.)'\n",
    "training_pct = .7 #.9 gives best results w/ random state = 2\n",
    "patience = 10 #10 is Deepak default\n",
    "batch_size = 1 #1 is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove MOFs w/ excessive linkers or too little linkers\n",
    "#ml_data = ml_data[(ml_data['#_of_Linkers'] < 7) & (ml_data['#_of_Linkers'] > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_space(x, y, n, force_int=False):\n",
    "    step = (y - x) / (n - 1)\n",
    "    if force_int:\n",
    "        return [int(x + step * i) for i in range(n)]\n",
    "    return [x + step * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define default params\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":2, \"n_unit\":10, \"activation\":'relu', \"loss\":'mse', \n",
    "            \"opt\":'adam', \"val_pct\":.2} #patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial grid\n",
    "init_grid = {\"patience\":eq_space(20, 1000, 5, True), \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":[.3, .5, 5]}\n",
    "#patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_frac != 1:\n",
    "    ml_data = ml_data.sample(frac=total_frac, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##uncomment below when dealing with stacked\n",
    "if stacked:\n",
    "    \n",
    "    ml_data['log_pressure'] = np.log(ml_data['pressure'].tolist())\n",
    "    log_p_mean, log_p_std = norm_col(ml_data, 'log_pressure')\n",
    "\n",
    "    max_p = max(ml_data['norm_log_pressure'].tolist())\n",
    "    min_p = min(ml_data['norm_log_pressure'].tolist())\n",
    "\n",
    "    other_props.append('norm_log_pressure')\n",
    "    \n",
    "    for ind, col in enumerate(ml_data.columns):\n",
    "        if start_str == col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "    ml_data = ml_data.drop(ml_data.columns[ml_data.isna().all()].tolist(), axis=1) #drop na cols\n",
    "    features = list(ml_data.columns[start_col:end_col])\n",
    "    features = features + other_props \n",
    "    \n",
    "    \n",
    "    c = ml_data['filename'].unique().tolist()\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed = 2\n",
    "\n",
    "    #c = list(b.groups.keys())\n",
    "    random.shuffle(c)\n",
    "\n",
    "    n_groups = len(c)\n",
    "\n",
    "    train_ind = round(n_groups*training_pct)\n",
    "\n",
    "    train_groups = c[0:train_ind]\n",
    "    test_groups = c[train_ind:]\n",
    "\n",
    "    train_dataset = ml_data.loc[ml_data['filename'].isin(train_groups)]\n",
    "\n",
    "    test_dataset = ml_data.loc[ml_data['filename'].isin(test_groups)]\n",
    "\n",
    "    isotherm_inds = [0, 11, 89, 170, 367, 600, 1100, 3021, 5321, 7621]\n",
    "    isotherm_files = [test_dataset.iloc[i]['filename'] for i in isotherm_inds]\n",
    "    isotherm_df = test_dataset.loc[test_dataset['filename'].isin(isotherm_files)]\n",
    "\n",
    "    isotherm_pressures = []\n",
    "    isotherm_uptakes = []\n",
    "    for i in isotherm_files:\n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        for row in isotherm_df.iterrows():\n",
    "            if row[1]['filename'] == i:\n",
    "                l1.append(row[1]['pressure'])\n",
    "                l2.append(row[1]['vol_uptake'])\n",
    "        isotherm_pressures.append(l1)\n",
    "        isotherm_uptakes.append(l2)\n",
    "\n",
    "    #del isotherm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stacked:\n",
    "    for ind, col in enumerate(ml_data.columns):\n",
    "        if start_str == col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "    ml_data = ml_data.drop(ml_data.columns[ml_data.isna().all()].tolist(), axis=1) #drop na cols\n",
    "    features = list(ml_data.columns[start_col:end_col])\n",
    "    features = features + other_props    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_dataset = fp_dat.sample(frac=training_pct,random_state=2) #default is 2\n",
    "#test_dataset = fp_dat.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split according to most isolated points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_remote = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./srt/two_pca_pgnorm.pickle', 'rb') as handle:\n",
    "    srt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([x[0] for x in srt]) == max(ml_data.index)\n",
    "\n",
    "n_train = round(training_pct*len(srt))\n",
    "n_train\n",
    "\n",
    "train_inds = [x[0] for x in srt[:n_remote]]\n",
    "train_inds\n",
    "\n",
    "remaining = srt[n_remote:]\n",
    "\n",
    "remaining\n",
    "\n",
    "random.shuffle(remaining)\n",
    "\n",
    "train_ind = n_train - n_remote\n",
    "\n",
    "train_inds += [x[0] for x in remaining[0:train_ind]]\n",
    "test_inds = [x[0] for x in remaining[train_ind:]]\n",
    "\n",
    "len(train_inds) + len(test_inds)\n",
    "\n",
    "remaining\n",
    "\n",
    "print(len(train_inds))\n",
    "\n",
    "train_dataset = ml_data.iloc[train_inds, :]\n",
    "\n",
    "test_dataset = ml_data.iloc[test_inds, :]\n",
    "\n",
    "len(train_inds) + len(test_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_dataset[property_used]\n",
    "test_label = test_dataset[property_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = train_dataset[features].to_numpy().astype('float32')\n",
    "#del train_dataset\n",
    "test_fp = test_dataset[features].to_numpy().astype('float32')\n",
    "#del test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = tf.data.Dataset.from_tensor_slices((train_fp.to_numpy().astype(np.float32), \n",
    "#                                                      train_label.to_numpy().astype(np.float32))).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = tf.data.Dataset.from_tensor_slices((test_fp.to_numpy().astype(np.float32), \n",
    "#                                                     test_label.to_numpy().astype(np.float32))).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear space\n",
    "#del train_fp\n",
    "#del test_fp\n",
    "#del train_label\n",
    "#del test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of test and training sets\n",
    "fig,ax = plt.subplots(figsize = (8,5))\n",
    "n_bins=30\n",
    "n, bins, patches = plt.hist(train_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='#FDA65F', alpha=1,label = 'Training set')\n",
    "n, bins, patches = plt.hist(test_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='green', alpha=1, label = 'Test set')\n",
    "plt.xlabel('y_val',fontsize=labelfontsize)\n",
    "plt.ylabel('Count',fontsize=labelfontsize)\n",
    "#ax.set_xlim(2,12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig('%s.png'%property_used,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of training ( and test)\n",
    "# train_stats = train_fp.describe()\n",
    "# train_stats = train_stats.transpose()\n",
    "# train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stats = test_fp.describe()\n",
    "# test_stats = test_stats.transpose()\n",
    "# test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(100, activation='relu', input_shape=[len(features)]), #default is 100\n",
    "        #layers.Dense(100, activation='relu'), #default is not exist\n",
    "        #layers.Dropout(.1), #default is not exist\n",
    "        #layers.Dense(100, activation='relu'), #default is not exist\n",
    "        #layers.Dropout(.1), #default is not exist\n",
    "        layers.Dense(100, activation='relu'), #default is 100\n",
    "        #layers.Dropout(.1), #default is not exist\n",
    "        layers.Dense(100, activation='relu'), #default is 100\n",
    "        #layers.Dropout(.1), #default is not exist\n",
    "        #layers.Dense(1)\n",
    "        layers.Dense(1, activation='linear') #default activation is None\n",
    "    ])\n",
    "\n",
    "#     model = keras.Sequential([\n",
    "#         layers.Dense(400, activation='relu', input_shape=[len(train_fp.keys())]),\n",
    "#         layers.Dense(400, activation='relu'),\n",
    "#         layers.Dense(100, activation='relu'),\n",
    "#         #layers.Dense(1)\n",
    "#         layers.Dense(1, activation='linear')\n",
    "#     ])\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=.001) #default is .001\n",
    "    \n",
    "    model.compile(loss='mse',\n",
    "        optimizer=opt,\n",
    "        metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model training\n",
    "start = time.time()\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                      verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# early_history = model.fit(train_fp.to_numpy(), train_label.to_numpy(), batch_size=1000,\n",
    "#                     epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "#                           callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "\n",
    "#default below\n",
    "\n",
    "early_history = model.fit(train_fp, train_label.to_numpy(), batch_size=32,\n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "                          callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "##############################################\n",
    "\n",
    "\n",
    "# early_history = model.fit(train_data,\n",
    "#                         epochs=EPOCHS, validation_data = train_data, verbose=1,\\\n",
    "#                               callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if run converged\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "#plt.ylim([0, 0.15])\n",
    "plt.ylabel('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del early_history\n",
    "del features, train_groups, test_groups, n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(a, b):\n",
    "    '''\n",
    "    Compute rmse between a and b\n",
    "    '''\n",
    "    return math.sqrt(np.mean(np.square(np.subtract(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_mean = 121.689253\n",
    "#target_std = 88.163575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale(property_name, test_predictions, train_predictions, test_label, train_label):\n",
    "    '''\n",
    "    Undo the scaling on predictions of test set, labels of test set, labels of training set\n",
    "    '''\n",
    "    mean = target_mean\n",
    "    std = target_std\n",
    "    res_test_predictions = (test_predictions * std) + mean\n",
    "    res_test_label = (test_label * std) + mean\n",
    "    res_train_label = (train_label * std) + mean    \n",
    "    res_train_predictions = (train_predictions * std) + mean   \n",
    "    return res_test_predictions, res_test_label, res_train_label, res_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_fp).flatten()\n",
    "train_predictions = model.predict(train_fp).flatten()\n",
    "\n",
    "# #normalize test values\n",
    "# mean = float(ml_data['mean_CH4_v/v_1_bar'][0])\n",
    "# std = float(ml_data['std_CH4_v/v_1_bar'][0])\n",
    "# res_test_predictions = (test_predictions * std) + mean\n",
    "# res_test_label = (test_label * std) + mean\n",
    "# res_train_label = (train_label * std) + mean\n",
    "# ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_predictions, res_test_label, res_train_label, res_train_predictions = unscale(property_used, test_predictions, train_predictions, test_label, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of test error and plotting parity\n",
    "\n",
    "#model = tf.keras.models.load_model('model_checkpoint_bandgap.h5')\n",
    "\n",
    "\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "\n",
    "#plot un-normed\n",
    "#loss, mae, mse = model.evaluate(test_fp.to_numpy(), res_test_label.to_numpy(), verbose=2)\n",
    "#print(\"Testing set Mean Abs Error: {:5.2f} bg\".format(mae))\n",
    "#tr_loss, tr_mae, tr_mse = model.evaluate(train_fp.to_numpy(), res_train_label.to_numpy(), verbose=2)\n",
    "#tr_rmse = math.sqrt(tr_mse)\n",
    "#rmse = math.sqrt(mse)\n",
    "rmse = get_rmse(res_test_label, res_test_predictions)\n",
    "\n",
    "tr_rmse = get_rmse(res_train_label, res_train_predictions)\n",
    "\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "r2_val = r2(y_true=res_test_label, y_pred=res_test_predictions)\n",
    "\n",
    "print(\"This is Test RMSE: \", rmse)\n",
    "print(\"This is Train RMSE: \", tr_rmse)\n",
    "\n",
    "ax1.scatter(res_test_label, res_test_predictions, c='r',s=10, label='Test')\n",
    "ax1.set_xlabel('True Volumetric Uptake',fontsize=labelfontsize)\n",
    "ax1.set_ylabel('Predicted Volumetric Uptake',fontsize=labelfontsize)\n",
    "max_val = max([max(res_test_label),max(res_test_predictions)])+1\n",
    "ax1.set_xlim(0, max_val)\n",
    "ax1.set_ylim(0, max_val)\n",
    "\n",
    "##############################\n",
    "\n",
    "#default\n",
    "# loss, mae, mse = model.evaluate(test_fp.to_numpy(), test_label.to_numpy(), verbose=2)\n",
    "# print(\"Testing set Mean Abs Error: {:5.2f} bg\".format(mae))\n",
    "\n",
    "# tr_loss, tr_mae, tr_mse = model.evaluate(train_fp.to_numpy(), train_label.to_numpy(), verbose=2)\n",
    "\n",
    "# tr_rmse = math.sqrt(tr_mse)\n",
    "# rmse = math.sqrt(mse)\n",
    "#ax1.scatter(test_label, test_predictions, c='r',s=10) \n",
    "#ax1.scatter(train_label, train_predictions, c='b',s=10)\n",
    "# ax1.set_xlabel('True normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "# ax1.set_ylabel('Predicted normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "#ax1.set_xlim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "#ax1.set_ylim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "###############################################\n",
    "ax1.legend()\n",
    "plot_x_min, plot_x_max = plt.xlim()\n",
    "plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.15\n",
    "\n",
    "#ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse), ha='left', fontsize=16)\n",
    "\n",
    "ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "         \"RMSE train=\" + str(\"%.4f\" % tr_rmse) + '\\n' +\n",
    "         \"R2 test=\" + str(\"%.4f\" % r2_val), ha='left', fontsize=16)\n",
    "\n",
    "# ax1.text(text_position_x, text_position_y, \"MAE=\" + str(\"%.4f\" % mae) + ' \\n' + \n",
    "#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./%s_test_parity_%s.png'%(property_used, total_frac),dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./isotherm_model_landgscaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for effect of # of remote points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [0, 500, 2000, 10000, 25000, 50000]:\n",
    "    n_remote = i\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open('./srt/two_pca_pgnorm.pickle', 'rb') as handle:\n",
    "        srt = pickle.load(handle)\n",
    "\n",
    "    max([x[0] for x in srt]) == max(ml_data.index)\n",
    "\n",
    "    n_train = round(training_pct*len(srt))\n",
    "\n",
    "\n",
    "    train_inds = [x[0] for x in srt[:n_remote]]\n",
    "\n",
    "    random.seed = 2\n",
    "    remaining = srt[n_remote:]\n",
    "\n",
    "\n",
    "    random.shuffle(remaining)\n",
    "\n",
    "    train_ind = n_train - n_remote\n",
    "\n",
    "    train_inds += [x[0] for x in remaining[0:train_ind]]\n",
    "    test_inds = [x[0] for x in remaining[train_ind:]]\n",
    "\n",
    "    len(train_inds) + len(test_inds)\n",
    "\n",
    "    print(len(train_inds))\n",
    "\n",
    "    train_dataset = ml_data.iloc[train_inds, :]\n",
    "\n",
    "    test_dataset = ml_data.iloc[test_inds, :]\n",
    "\n",
    "    len(train_inds) + len(test_inds)\n",
    "    \n",
    "    train_label = train_dataset[property_used]\n",
    "    test_label = test_dataset[property_used]\n",
    "\n",
    "    train_fp = train_dataset[features].to_numpy().astype('float32')\n",
    "    del train_dataset\n",
    "    test_fp = test_dataset[features].to_numpy().astype('float32')\n",
    "    del test_dataset\n",
    "\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "\n",
    "    # NN model training\n",
    "    start = time.time()\n",
    "\n",
    "    EPOCHS = 1000\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                          verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # early_history = model.fit(train_fp.to_numpy(), train_label.to_numpy(), batch_size=1000,\n",
    "    #                     epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "    #                           callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "\n",
    "    #default below\n",
    "\n",
    "    early_history = model.fit(train_fp, train_label.to_numpy(), batch_size=32,\n",
    "                        epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "                              callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "    ##############################################\n",
    "\n",
    "\n",
    "    # early_history = model.fit(train_data,\n",
    "    #                         epochs=EPOCHS, validation_data = train_data, verbose=1,\\\n",
    "    #                               callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Time elapsed: \", end-start)\n",
    "\n",
    "    def get_rmse(a, b):\n",
    "        '''\n",
    "        Compute rmse between a and b\n",
    "        '''\n",
    "        return math.sqrt(np.mean(np.square(np.subtract(a, b))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #target_mean = 121.689253\n",
    "    #target_std = 88.163575\n",
    "\n",
    "    def unscale(property_name, test_predictions, train_predictions, test_label, train_label):\n",
    "        '''\n",
    "        Undo the scaling on predictions of test set, labels of test set, labels of training set\n",
    "        '''\n",
    "        mean = target_mean\n",
    "        std = target_std\n",
    "        res_test_predictions = (test_predictions * std) + mean\n",
    "        res_test_label = (test_label * std) + mean\n",
    "        res_train_label = (train_label * std) + mean    \n",
    "        res_train_predictions = (train_predictions * std) + mean   \n",
    "        return res_test_predictions, res_test_label, res_train_label, res_train_predictions\n",
    "\n",
    "    test_predictions = model.predict(test_fp).flatten()\n",
    "    train_predictions = model.predict(train_fp).flatten()\n",
    "\n",
    "    # #normalize test values\n",
    "    # mean = float(ml_data['mean_CH4_v/v_1_bar'][0])\n",
    "    # std = float(ml_data['std_CH4_v/v_1_bar'][0])\n",
    "    # res_test_predictions = (test_predictions * std) + mean\n",
    "    # res_test_label = (test_label * std) + mean\n",
    "    # res_train_label = (train_label * std) + mean\n",
    "    # ################\n",
    "\n",
    "    res_test_predictions, res_test_label, res_train_label, res_train_predictions = unscale(property_used, test_predictions, train_predictions, test_label, train_label)\n",
    "\n",
    "    rmse = get_rmse(res_test_label, res_test_predictions)\n",
    "    \n",
    "    rmse_tracker.append((i, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isotherm example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotherm_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_fp = test_fp[isotherm_inds[i], :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_pressures = eq_space(min_p, max_p, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_arr = np.array([np.concatenate([trial_fp, [p]]) for p in trial_pressures])\n",
    "#trial_fps = [trial_fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(trial_arr).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = target_mean\n",
    "std = target_std\n",
    "res_test_predictions = (test_predictions * std) + mean\n",
    "res_preds = (preds * std) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pressures = np.exp([(i * log_p_std) + log_p_mean for i in trial_pressures]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1,ax1 = plt.subplots(figsize = (4,4))\n",
    "plt.scatter(res_pressures, res_preds, c='b', label='Predicted')\n",
    "plt.scatter(isotherm_pressures[i], isotherm_uptakes[i], c='r', label='Truth')\n",
    "plt.legend()\n",
    "ax1.set_ylabel('Volumetric uptake',fontsize=labelfontsize)\n",
    "ax1.set_xlabel('Pressure (bar)',fontsize=labelfontsize)\n",
    "ax1.set_ylim(ymax=300)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(y_true=res_test_label, y_pred=res_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - test_label\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_space(4,9, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(np.array([1,1]), np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "defaults = [10, .8, 3, 100, 'relu', 'mse', 'adam', .2]\n",
    "patience = defaults[0]\n",
    "training_pct = defaults[1]\n",
    "n_layer = defaults[2]\n",
    "n_unit = defaults[3]\n",
    "activation = defaults[4]\n",
    "loss = defaults[5]\n",
    "opt = defaults[6]\n",
    "val_pct = defaults[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = .1\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":7, \"n_unit\":20, \"activation\":'relu', \"loss\":'mean_absolute_error', \n",
    "            \"opt\":'rmsprop', \"val_pct\":.2}\n",
    "all_grid = {\"patience\":[10], \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":eq_space(.2, .5, 5)}\n",
    "\n",
    "\n",
    "init_grid = {\"val_pct\":eq_space(.2, .5, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#patience_d, training_pct_d, n_layer_d, n_unit_d, activation_d, loss_d, opt_d, val_pct_d \n",
    "r = varyParams(ml_data, defaults, init_grid, total_frac, start_str, end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "training_pct = .9\n",
    "total_frac = 1\n",
    "mse, model = evaluate_model(ml_data, total_frac, start_str, end_str, 10, training_pct, 3, 200, 'relu', \n",
    "                   'mse', 'adam', .2, batch_size=5000, norm=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_data, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds, test_label, c='b', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
