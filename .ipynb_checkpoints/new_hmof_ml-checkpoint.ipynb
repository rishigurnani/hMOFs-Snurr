{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "from sklearn.metrics import r2_score as r2\n",
    "#from rdkit import Chem\n",
    "from sklearn.decomposition import PCA\n",
    "import rishi_utils as ru\n",
    "\n",
    "import importlib\n",
    "import efrc_ml_production as ml\n",
    "importlib.reload(ml)\n",
    "importlib.reload(ru)\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut,KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmofMLdataset:\n",
    "    def __init__(self, results_dir, now, SI_grav_data_path='/data/rgur/efrc/prep_data/all_v1/ml_data.csv', \n",
    "                 SD_grav_data_path='/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv',SI_stacked_path=\n",
    "                '/data/rgur/efrc/prep_data/all_v1/stacked.csv',\n",
    "                 SD_stacked_path='/data/rgur/efrc/prep_data/all_no_norm/stacked.csv',\n",
    "                 Y_DATA_PATH='/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx', n_core=15, skip=None, \n",
    "                 do=None, nn_space=None, grav_algo='xgb'):\n",
    "        self.results_dir = results_dir \n",
    "        os.chdir(self.results_dir)\n",
    "        self.SI_grav_data_path = SI_grav_data_path\n",
    "        self.SD_grav_data_path = SD_grav_data_path\n",
    "        self.SI_stacked_path = SI_stacked_path\n",
    "        self.SD_stacked_path = SD_stacked_path\n",
    "        self.n_core = n_core\n",
    "        self.Y_DATA_PATH = Y_DATA_PATH\n",
    "        self.del_defective_mofs = False\n",
    "        self.cat_si_sd = True\n",
    "        self.add_size_fp = True #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "        self.srt_size_fp = True\n",
    "        self.iso_start_str_sd = 'Density'\n",
    "        self.iso_end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "        self.grav_start_str_sd = 'CH4_v/v_248_bar'\n",
    "        self.grav_end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "        self.start_str_si = 'filename'\n",
    "        self.end_str_si = 'valence_pa'\n",
    "        self.cat_col_names = ['cat_1', 'cat_2', 'cat_3', 'cat_4']\n",
    "        self.skip = skip\n",
    "        self.feature_codes = ['10000', '11000', '01000', '10100', '11100', '01100',\n",
    "                             '10010', '11010', '01010', '10110', '11110', '01110',\n",
    "                             '10001', '11001', '01001', '10101', '11101', '01101',\n",
    "                             '10011', '11011', '01011', '10111', '11111', '01111']\n",
    "        self.do = do\n",
    "        if self.do != None:\n",
    "            self.feature_codes = self.do\n",
    "        elif skip != None:\n",
    "            self.feature_codes = [i for i in self.feature_codes if i not in self.skip]   \n",
    "        print(\"There are %s unique feature codes\" %len(set(self.feature_codes)))\n",
    "        self.any_stacked = any([item[-1]=='1' for item in self.feature_codes]) #are any codes for stacked models?\n",
    "        self.now = now\n",
    "        print(\"now is %s\" %self.now)\n",
    "        self.nn_space = nn_space\n",
    "        self.grav_algo = grav_algo\n",
    "    def makeMasterDFs(self):\n",
    "        #gravimetric\n",
    "        self.grav, self.grav_prop, self.grav_target_mean, self.grav_target_std, self.grav_all_features = \\\n",
    "                                            ml.prepToSplit(\n",
    "                                            self.grav_algo, self.cat_si_sd, self.SD_grav_data_path, self.SI_grav_data_path, \n",
    "                                            self.grav_start_str_sd, self.grav_end_str_sd, self.start_str_si, \n",
    "                                            self.end_str_si, 1, self.del_defective_mofs, self.add_size_fp, \n",
    "                                            self.srt_size_fp, None, stacked=False, n_core=self.n_core, \n",
    "                                            del_geometric_fp=False, cat_col_names=self.cat_col_names, \n",
    "                                            Y_DATA_PATH=self.Y_DATA_PATH\n",
    "                                            )\n",
    "        size_cols = [\"size_%s\" %s for s in range(20)]\n",
    "        self.LS_dict = {row[1]['filename']:row[1][size_cols] for row in self.grav.iterrows()} # map from filename \n",
    "                                                                                        #to linkersize-vector\n",
    "        #stacked\n",
    "        if self.any_stacked:\n",
    "            self.iso, self.iso_prop, self.iso_target_mean, self.iso_target_std, self.iso_all_features, self.pinfo = \\\n",
    "                                                ml.prepToSplit(\n",
    "                                                'nn', self.cat_si_sd, self.SD_stacked_path, self.SI_stacked_path, \n",
    "                                                self.iso_start_str_sd, self.iso_end_str_sd, self.start_str_si, \n",
    "                                                self.end_str_si, 1, self.del_defective_mofs, self.add_size_fp, \n",
    "                                                self.srt_size_fp, None, True, self.n_core, False, self.cat_col_names, \n",
    "                                                self.Y_DATA_PATH, self.LS_dict) \n",
    "    def select_features(self, code, stacked):\n",
    "        '''\n",
    "        Should only be called after makeMasterDFs\n",
    "        '''\n",
    "        si = bool(int(code[0])) #True (=1) if size-independent features are included\n",
    "        sd = bool(int(code[1])) #True (=1) if size-dependent features are included\n",
    "        size_fp = bool(int(code[2])) #True (=1) if linker size features are included\n",
    "        geo_fp = bool(int(code[3])) #True (=1) if geometric features are included\n",
    "        non_pg = ml.getNonPGcolNames(size_fp, stacked, not geo_fp, self.cat_col_names)\n",
    "        pg = []\n",
    "        if si:\n",
    "            try:\n",
    "                si_df = pd.read_csv(self.SI_grav_data_path)\n",
    "            except:\n",
    "                si_df = pd.read_csv(self.SI_grav_data_path, compression='gzip')\n",
    "            self.all_pg = [s for s in ml.getPGcolNames(si_df, start_str=self.start_str_si, end_str=self.end_str_si)]\n",
    "            pg += [s+'_si' for s in ml.getPGcolNames(si_df, start_str=self.start_str_si, end_str=self.end_str_si) \n",
    "                   if s+'_si' in self.grav_all_features]\n",
    "            del si_df\n",
    "        if sd:\n",
    "            try:\n",
    "                sd_df = pd.read_csv(self.SD_grav_data_path)\n",
    "            except:\n",
    "                sd_df = pd.read_csv(self.SD_grav_data_path, compression='gzip')\n",
    "            pg += [s for s in ml.getPGcolNames(sd_df, self.grav_start_str_sd, self.grav_end_str_sd) if s in\n",
    "                  self.grav_all_features]\n",
    "        return non_pg + pg\n",
    "    def makeAllResults(self):\n",
    "        self.makeMasterDFs()\n",
    "        print('\\n')\n",
    "        #Parallel(n_jobs=self.n_core)(delayed(self.makeResult)(j) for j in self.feature_codes)\n",
    "        for i in self.feature_codes: #True if stacked\n",
    "                STACKED = bool(int(i[-1])) #True (=1) if stacked\n",
    "                CODE = i[:-1]\n",
    "                run_features = self.select_features(code=CODE, stacked=STACKED)\n",
    "                if STACKED:\n",
    "                    print(\"Running code %s for isotherm model\" %CODE)\n",
    "                    drop_features = [s for s in self.iso_all_features if s not in run_features]\n",
    "                    algo = 'nn'\n",
    "                else:\n",
    "                    print(\"Running code %s for gravimetric uptake model\" %CODE)\n",
    "                    algo = self.grav_algo\n",
    "                    drop_features = [s for s in self.grav_all_features if s not in run_features]\n",
    "                    #l.append(self.iso.drop(drop_features, axis=1))\n",
    "                if algo == 'nn':\n",
    "                    N_CORE=1\n",
    "                else:\n",
    "                    N_CORE=self.n_core\n",
    "                if STACKED:\n",
    "                    FpDataSet(self.iso.drop(drop_features, axis=1), run_features, self.iso_prop, \n",
    "                              self.iso_target_mean, self.iso_target_std, now=self.now, nn_space=self.nn_space, \n",
    "                              stacked=STACKED, fp_code=CODE, n_core=N_CORE, grav_algo=self.grav_algo).train()\n",
    "                else:\n",
    "                    FpDataSet(self.grav.drop(drop_features, axis=1), run_features, self.grav_prop, \n",
    "                              self.grav_target_mean, self.grav_target_std, now=self.now, nn_space=self.nn_space,\n",
    "                              stacked=STACKED, fp_code=CODE, n_core=N_CORE, grav_algo=self.grav_algo).train()\n",
    "class FpDataSet:\n",
    "    def __init__(self, df, features, property_used, target_mean, target_std, stacked, now, nn_space, fp_code='0', \n",
    "                    n_core=15, grav_algo='xgb', track=True, chkpt_name='model_checkpoint',n_folds=15):\n",
    "        self.n_folds = n_folds #for master run\n",
    "        self.now = now\n",
    "        self.df = df\n",
    "        self.fp_code = fp_code\n",
    "        self.property_used = property_used\n",
    "        self.target_mean = target_mean\n",
    "        self.target_std = target_std\n",
    "        self.n_samples = len(self.df)\n",
    "        self.features = features\n",
    "        self.stacked = stacked\n",
    "        self.n_core = n_core\n",
    "        self.grav_algo = grav_algo\n",
    "        self.track=track\n",
    "        self.chkpt_name = chkpt_name\n",
    "        self.file_tracker = {'train':[],'test':[]}\n",
    "        self.pressure_tracker = {'train':[],'test':[]}\n",
    "        if self.stacked:\n",
    "            self.algo = 'nn'\n",
    "            self.model_tag = 'iso'\n",
    "        else:\n",
    "            self.algo = self.grav_algo\n",
    "            self.model_tag = 'grav'\n",
    "        self.nn_space = nn_space\n",
    "        self.hp_frac = .05\n",
    "        self.fn = self.df['filename'].unique() #filenames\n",
    "        \n",
    "    def make_splits(self):\n",
    "        def gen():\n",
    "            for train_index, test_index in KFold(self.n_folds).split(self.fn):\n",
    "                train_fn = self.fn[train_index]\n",
    "                test_fn = self.fn[test_index]\n",
    "                train_df = self.df[self.df['filename'].isin(train_fn)].reset_index().drop('index', axis=1)\n",
    "                test_df = self.df[self.df['filename'].isin(test_fn)].reset_index().drop('index', axis=1)\n",
    "                if self.track:\n",
    "                    self.file_tracker['train'].append(train_df['filename'].tolist())\n",
    "                    self.file_tracker['test'].append(test_df['filename'].tolist())\n",
    "                    try:\n",
    "                        self.pressure_tracker['train'].append(train_df['pressure'].tolist())\n",
    "                        self.pressure_tracker['test'].append(test_df['pressure'].tolist())\n",
    "                    except:\n",
    "                        self.pressure_tracker['train'].append(['na']*len(train_df))\n",
    "                        self.pressure_tracker['test'].append(['na']*len(test_df))\n",
    "                X_train, X_test = train_df[self.features].to_numpy(), test_df[self.features].to_numpy()\n",
    "                y_train, y_test = train_df[self.property_used].to_numpy(), test_df[self.property_used].to_numpy()\n",
    "                yield X_train,y_train,X_test,y_test\n",
    "\n",
    "        return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))    \n",
    "    def CV_objective(self, params):\n",
    "        try:\n",
    "            lr = params[1]\n",
    "        except:\n",
    "            lr = .001 #default\n",
    "        try:\n",
    "            h_units = params[0]\n",
    "        except:\n",
    "            h_units = 100 #default\n",
    "        patience = 15 #default\n",
    "        try:\n",
    "            BS = params[2]\n",
    "        except:\n",
    "            BS = 32 #default\n",
    "        dataset = self.make_splits()\n",
    "        start = time.time()\n",
    "        import datetime\n",
    "        rmses = []\n",
    "        os.system('rm %s.h5' %self.chkpt_name)\n",
    "        try:\n",
    "            os.system('rm -rf logs')\n",
    "        except:\n",
    "            pass\n",
    "        fold = 0\n",
    "        for X_train,y_train,X_test,y_test in dataset:\n",
    "            early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "            checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='%s.h5' %self.chkpt_name, monitor='val_loss',\\\n",
    "                                                                  verbose=1, save_best_only=True, mode='min')\n",
    "            model = ml.build_model(X_train.shape[1], lr, h_units, 'relu')\n",
    "\n",
    "            model.fit(X_train, y_train, batch_size=BS,epochs=1000, verbose=1, \n",
    "                      callbacks=[checkpoint_callbacks, early_stop],\n",
    "                        validation_split=None, validation_data=(X_test,y_test), shuffle=True, class_weight=None,\n",
    "                        sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "                        validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,\n",
    "                        use_multiprocessing=False\n",
    "                     ) \n",
    "            model.load_weights(filepath='%s.h5' %self.chkpt_name)\n",
    "            preds = model.predict(X_test).flatten()\n",
    "            rmse = ml.get_rmse(preds, y_test)\n",
    "            print(\"RMSE of fold %s is %s\" %( fold,rmse ))\n",
    "            rmses.append(rmse)\n",
    "            if self.track:\n",
    "                n_train = len(self.file_tracker['train'][fold])\n",
    "                n_test = len(self.file_tracker['test'][fold])\n",
    "                res_test_predictions, res_test_label, res_train_label, res_train_predictions = \\\n",
    "                    ml.unscale(preds, model.predict(X_train).flatten(), y_test.numpy(), \n",
    "                           y_train.numpy(), self.target_mean, self.target_std)\n",
    "                results_df = pd.DataFrame({\"Filename\": self.file_tracker['train'][fold]+ \\\n",
    "                                       self.file_tracker['test'][fold], \n",
    "                                       \"Pressure\": self.pressure_tracker['train'][fold]+ \\\n",
    "                                       self.pressure_tracker['test'][fold], \n",
    "                                       \"Class\": ['Train']*n_train+['Test']*n_test,\n",
    "                                    \"Prediction\": res_train_predictions.tolist()+res_test_predictions.tolist(),\n",
    "                                      \"Truth\": res_train_label.tolist()+res_test_label.tolist()}\n",
    "                                     )\n",
    "                save_fragment = '%s_code_%s_fold_%s_%s' %(self.model_tag, self.fp_code, fold, self.now)\n",
    "                results_df.to_csv('results_%s.csv' %save_fragment, compression='gzip')\n",
    "                print(\"Save Results using Fragment %s\" %save_fragment)\n",
    "                try:\n",
    "                    model.save_model('%s.xgb' %save_fragment)\n",
    "                except:\n",
    "                    model.save('%s.h5' %save_fragment,save_format='h5')\n",
    "            \n",
    "            fold+=1\n",
    "        print(\"\\nBest fold is %s\" %np.array(rmses).argmax())\n",
    "        print(\"Average RMSEs of best epochs in each fold: %s\" %np.mean(rmses))\n",
    "        end = time.time()\n",
    "        print('Set of Folds Done in %s' %(end-start))        \n",
    "        if self.track:\n",
    "            save_fragment = '%s_code_%s_%s' %(self.model_tag, self.fp_code, self.now)\n",
    "            with open('file_tracker_%s.pkl' %save_fragment, 'wb') as f:\n",
    "                pickle.dump(self.file_tracker, f)\n",
    "        return np.mean(rmses)\n",
    "    def train(self):\n",
    "        hp_files = np.random.choice(self.fn, size=round(len(self.fn)*self.hp_frac), replace=False)\n",
    "        hp_df = self.df[self.df['filename'].isin(hp_files)].reset_index().drop('index', axis=1)\n",
    "        hp_df.to_csv('hp_df.csv', compression='gzip')\n",
    "        print(\"Saved hp_df to disk\")\n",
    "        params = HPOpt(hp_df, self.features, self.property_used, self.target_mean, self.target_std, \n",
    "                      self.stacked, now=self.now, space=self.nn_space, grav_algo=self.grav_algo).get_params()\n",
    "        print(\"Optimzed Hyperparameters found: %s\" %params)\n",
    "        mean_rmse = self.CV_objective(params)\n",
    "\n",
    "class HPOpt:\n",
    "    def __init__(self, df, features, property_used, target_mean, target_std, stacked, \n",
    "                 space, now, n_trees=50, grav_algo='xgb'):\n",
    "        self.df = df\n",
    "        self.space = space\n",
    "        self.stacked = stacked\n",
    "        self.property_used = property_used\n",
    "        self.features = features\n",
    "        self.target_mean = target_mean\n",
    "        self.target_std = target_std\n",
    "        self.n_trees = n_trees\n",
    "        self.grav_algo = grav_algo\n",
    "        self.now = now\n",
    "        if stacked:\n",
    "            self.N_CALLS = 20\n",
    "            self.algo = 'nn'\n",
    "        else:\n",
    "            self.N_CALLS = 30\n",
    "            self.algo = self.grav_algo\n",
    "        \n",
    "        print(\"Using %s calls for HPOpt\" %self.N_CALLS)\n",
    "    def get_params(self):\n",
    "        HP_Inst = FpDataSet(self.df, self.features, self.property_used, self.target_mean, self.target_std, \n",
    "                            self.stacked, self.now, self.space, '0',1, self.grav_algo, track=False, \n",
    "                            chkpt_name='hp_model_checkpoint', n_folds=5)\n",
    "        self.start = time.time()\n",
    "        r = gp_minimize(HP_Inst.CV_objective, self.space, n_calls=self.N_CALLS)\n",
    "        self.end = time.time()\n",
    "        print(\"Finished HPOpt in %s\" %(self.end-self.start))\n",
    "        self.params = r.x\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 unique feature codes\n",
      "now is 10_31AM_on_May_08_2020\n",
      "\n",
      "Starting to Construct Gravimetric Uptake Data Frame\n",
      "Using start_str_sd CH4_v/v_248_bar\n",
      "Using end_str_sd norm_Dom._Pore_(ang.)\n",
      "Using start_str_si filename\n",
      "Using end_str_si valence_pa\n",
      "Total frac equals 1\n",
      "\n",
      "\n",
      "Starting To Make Linker Size Columns\n",
      "Starting to sort Linker Size Columns\n",
      "Finished Making Linker Size Columns\n",
      "The following columns have been dropped: ['norm_Mafp_C1_N2_N3', 'norm_Mafp_N2_O2_N3', 'norm_Mmfp_MQNs22', 'norm_Mmfp_MQNs23', 'norm_Mmfp_MQNs24', 'norm_Mmfp_MQNs25']\n",
      "\n",
      "Starting to Construct Isotherm Stacked Data Frame\n",
      "Using start_str_sd Density\n",
      "Using end_str_sd norm_Dom._Pore_(ang.)\n",
      "Using start_str_si filename\n",
      "Using end_str_si valence_pa\n",
      "Total frac equals 1\n",
      "\n",
      "\n",
      "Starting To Make Linker Size Columns\n",
      "Finished Making Linker Size Columns\n",
      "The following columns have been dropped: ['norm_Mafp_C1_N2_N3', 'norm_Mafp_N2_O2_N3', 'norm_Mmfp_MQNs22', 'norm_Mmfp_MQNs23', 'norm_Mmfp_MQNs24', 'norm_Mmfp_MQNs25']\n",
      "\n",
      "\n",
      "Running code 1011 for isotherm model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a955d467403d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \u001b[0mSI_stacked_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/data/rgur/efrc/prep_data/all_v1/stacked_head.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mSD_stacked_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/data/rgur/efrc/prep_data/all_no_norm/stacked_head.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m              do=DO, nn_space=NN_SPACE).makeAllResults()\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-75194fe2cd10>\u001b[0m in \u001b[0;36mmakeAllResults\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     FpDataSet(self.iso.drop(drop_features, axis=1), run_features, self.iso_prop, \n\u001b[1;32m    116\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miso_target_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miso_target_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                               stacked=STACKED, fp_code=CODE, n_core=N_CORE, grav_algo=self.grav_algo).train()\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     FpDataSet(self.grav.drop(drop_features, axis=1), run_features, self.grav_prop, \n",
      "\u001b[0;32m<ipython-input-7-75194fe2cd10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, features, property_used, target_mean, target_std, stacked, now, nn_space, fp_code, n_core, grav_algo, track, chkpt_name, n_folds)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_frac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "PATH = '/data/rgur/efrc/paper_data/isotherm_models/test'\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%I_%M%p_on_%B_%d_%Y\")\n",
    "\n",
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx'\n",
    "\n",
    "NN_SPACE = [(250, 600), #n_units\n",
    "                (.0002, .001),#learning rate\n",
    "                (8, 512)] #batch size\n",
    "GRAV_ALGO = 'nn'\n",
    "DO = ['10111']\n",
    "out = hmofMLdataset(PATH, now, Y_DATA_PATH=Y_DATA_PATH,\n",
    "                   SI_grav_data_path='/data/rgur/efrc/prep_data/all_v1/ml_data_head.csv',\n",
    "                 SD_grav_data_path='/data/rgur/efrc/prep_data/all_no_norm/ml_data_head.csv',\n",
    "                   SI_stacked_path='/data/rgur/efrc/prep_data/all_v1/stacked_head.csv',\n",
    "                 SD_stacked_path='/data/rgur/efrc/prep_data/all_no_norm/stacked_head.csv',\n",
    "             do=DO, nn_space=NN_SPACE).makeAllResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'/file_tracker_iso_code_1011_06_28PM_on_May_07_2020.pkl', 'rb') as f:\n",
    "    file_tracker = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_tracker['train'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files, train_files = file_tracker['test'][7],file_tracker['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ru.pd_load('/data/rgur/efrc/prep_data/all_v1/stacked_head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['filename'].isin(test_files)].reset_index().drop('index', axis=1) \n",
    "train_df = df[df['filename'].isin(train_files)].reset_index().drop('index', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_train = test_df['vol_uptake'], train_df['vol_uptake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH+'/iso_code_1011_fold_7_06_28PM_on_May_07_2020.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ru.pd_load(PATH+'/results_iso_code_1011_fold_10_08_59PM_on_May_07_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Class</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hypotheticalMOF_5053773_i_1_j_27_k_27_m_7_cat_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train</td>\n",
       "      <td>19.184036</td>\n",
       "      <td>12.409546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hypotheticalMOF_5051526_i_1_j_27_k_5_m_9_cat_1...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train</td>\n",
       "      <td>28.117630</td>\n",
       "      <td>14.970042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hypotheticalMOF_5004666_i_0_j_20_k_12_m_1_cat_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train</td>\n",
       "      <td>23.642342</td>\n",
       "      <td>11.924002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hypotheticalMOF_36515_i_2_j_17_k_6_m_2_cat_1.cif</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train</td>\n",
       "      <td>36.059769</td>\n",
       "      <td>49.924683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hypotheticalMOF_5032463_i_0_j_29_k_21_m_4_cat_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train</td>\n",
       "      <td>-35.059898</td>\n",
       "      <td>22.993454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Filename  Pressure  \\\n",
       "0           0  hypotheticalMOF_5053773_i_1_j_27_k_27_m_7_cat_...       1.0   \n",
       "1           1  hypotheticalMOF_5051526_i_1_j_27_k_5_m_9_cat_1...       1.0   \n",
       "2           2  hypotheticalMOF_5004666_i_0_j_20_k_12_m_1_cat_...       1.0   \n",
       "3           3   hypotheticalMOF_36515_i_2_j_17_k_6_m_2_cat_1.cif       1.0   \n",
       "4           4  hypotheticalMOF_5032463_i_0_j_29_k_21_m_4_cat_...       1.0   \n",
       "\n",
       "   Class  Prediction      Truth  \n",
       "0  Train   19.184036  12.409546  \n",
       "1  Train   28.117630  14.970042  \n",
       "2  Train   23.642342  11.924002  \n",
       "3  Train   36.059769  49.924683  \n",
       "4  Train  -35.059898  22.993454  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Class'] == 'Test']\n",
    "true = test['Truth'].tolist()\n",
    "pred = test['Prediction'].tolist()\n",
    "pred = [max(i, 0) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f37bc286be0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, pred_test = model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {'a':[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['a'].append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [1, 2]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'a':['hi','me']}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, len(a), size=, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(a, size=2, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
