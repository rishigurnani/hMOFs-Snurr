{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/modules/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize\n",
    "\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables which will be inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'xgb' #am I using XGBoost (xgb) or Neural Nets (nn)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1 #total fraction of data set to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = False #make True if the training data should be chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_remote = 2000 #the n_remote most remote points will be added to training set if random_split = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature by number of atoms\n",
    "stacked = False #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 1 #number of cores to use\n",
    "ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_str = 'filename'\n",
    "end_str = 'valence_pa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_geometric_fp = False #make True if you want to ignore the geometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 30 #number of weak learners. Bigger is better until 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pp = False #make True if you want to save the parity plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_data():\n",
    "    return pd.read_csv(ML_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_size(df):\n",
    "    '''\n",
    "    Remove unnecessary columns from df to decrease size\n",
    "    '''\n",
    "    return df.drop([col for col in df.keys() if 'Smiles' in col] + [col for col in \n",
    "                                                                             df.keys() if 'Unnamed' in col] + ['#_of_Linkers'] + ['Metal_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim(df):\n",
    "    '''\n",
    "    Return data set which contains total_frac of original data set\n",
    "    '''\n",
    "    if total_frac != 1:\n",
    "        df = df.sample(frac=total_frac, random_state=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPGcolNames(df):\n",
    "    '''\n",
    "    Return list of PG-relevant column names\n",
    "    '''\n",
    "    for ind, col in enumerate(df.columns):\n",
    "        if start_str == col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "    return list(df.columns[start_col:end_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonPGcolNames():\n",
    "    features = ['norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "    \n",
    "    if not del_geometric_fp:\n",
    "        features += ['norm_Dom._Pore_(ang.)',\n",
    "                     'norm_Max._Pore_(ang.)',\n",
    "                     'norm_Void_Fraction',\n",
    "                     'norm_Surf._Area_(m2/g)',\n",
    "                     'norm_Vol._Surf._Area',\n",
    "                     'norm_Density']\n",
    "    features += cat_col_names\n",
    "    \n",
    "    if stacked:\n",
    "        features += ['norm_log_pressure']\n",
    "        \n",
    "    if add_size_fp:\n",
    "        features += ['size_%s' %n for n in range(20)]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df):\n",
    "    '''\n",
    "    Merge gravimetric uptake\n",
    "    '''\n",
    "    if not stacked:\n",
    "        y_data = pd.read_excel(Y_DATA_PATH)\n",
    "        df = df.join(y_data[['Crystal ID#', 'CH4 cm3/g 35 bar']].set_index('Crystal ID#'), on='Crystal_ID#')\n",
    "        for key in df.keys():\n",
    "            if ' ' in key:\n",
    "                new_key = key.replace(' ', '_')\n",
    "                df[new_key] = df[key]\n",
    "                df = df.drop(key, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_col(df, col_name):\n",
    "    \n",
    "    mean = df[col_name].mean()\n",
    "    std = df[col_name].std()\n",
    "    \n",
    "    property_used = 'norm_' + col_name\n",
    "    df[property_used] = (df[col_name] - mean) / std\n",
    "\n",
    "    return mean, std, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(df):\n",
    "    if stacked:\n",
    "        to_norm = 'vol_uptake'\n",
    "    else:\n",
    "        to_norm = 'CH4_cm3/g_35_bar'\n",
    "    mean, std, property_used = norm_col(df, to_norm)\n",
    "    return mean, std, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delMissing(df, property_used, reset=True):\n",
    "    '''\n",
    "    Remove rows from df with missing values in target\n",
    "    '''\n",
    "    df = df.iloc[df[property_used].dropna().index]\n",
    "    if reset:\n",
    "        df = df.reset_index().drop('index', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normPressure(df):\n",
    "    '''\n",
    "    Normalize pressure feature and globally save important metrics. Only use if stacked == True!\n",
    "    '''\n",
    "    df['log_pressure'] = np.log(df['pressure'].tolist())\n",
    "    \n",
    "    #global log_p_mean, log_p_std, max_p, min_p #save these variables globally\n",
    "    \n",
    "    log_p_mean, log_p_std, property_used = norm_col(df, 'log_pressure')\n",
    "\n",
    "    max_p = max(df[property_used].tolist())\n",
    "    min_p = min(df[property_used].tolist())\n",
    "    \n",
    "    return df, log_p_mean, log_p_std, max_p, min_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat(s):\n",
    "    '''\n",
    "    Returns interpenetration from filename\n",
    "    '''\n",
    "    if 'cat' in s:\n",
    "        return int(s.split('_')[-1][0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCat(df):\n",
    "    '''\n",
    "    Add normed one-hot encoded features for catenation (interpenetration) of MOF\n",
    "    '''\n",
    "    one_hot = [[0]*4 for i in range(len(df))]\n",
    "    for i, f in enumerate(df['filename'].tolist()):\n",
    "        one_hot[i][get_cat(f)] = 1\n",
    "    oh_1 = []\n",
    "    oh_2 = []\n",
    "    oh_3 = []\n",
    "    oh_4 = []\n",
    "    for i in one_hot:\n",
    "        oh_1.append(i[0])\n",
    "        oh_2.append(i[1])\n",
    "        oh_3.append(i[2])\n",
    "        oh_4.append(i[3])\n",
    "    df[cat_col_names[0]] = oh_1\n",
    "    df[cat_col_names[1]] = oh_2\n",
    "    df[cat_col_names[2]] = oh_3\n",
    "    df[cat_col_names[3]] = oh_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepToSplit():\n",
    "    '''\n",
    "    Carry out a series of tasks to prepare the data to be split into test and train sets\n",
    "    '''\n",
    "    ml_data = load_ml_data()\n",
    "    global pg_cols\n",
    "    pg_cols = getPGcolNames(ml_data)\n",
    "    ml_data = reduce_size(ml_data)\n",
    "    ml_data = slim(ml_data)\n",
    "    global non_pg_cols\n",
    "    non_pg_cols = getNonPGcolNames()\n",
    "    ml_data = merge_data(ml_data)\n",
    "    prepare_target(ml_data)\n",
    "    global target_mean, target_std\n",
    "    target_mean, target_std, property_used = prepare_target(ml_data)\n",
    "    ml_data = delMissing(ml_data, property_used)\n",
    "\n",
    "    if stacked:\n",
    "        global log_p_mean, log_p_std, max_p, min_p\n",
    "        ml_data, log_p_mean, log_p_std, max_p, min_p = normPressure(ml_data)\n",
    "    addCat(ml_data)\n",
    "    return ml_data, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNtree(df):\n",
    "    if stacked:\n",
    "        non_p_features = [col for col in features if col != 'norm_log_pressure']\n",
    "        mat = df[non_p_features].drop_duplicates().to_numpy()\n",
    "    else:\n",
    "        non_p_features = features\n",
    "        mat = df[non_p_features].to_numpy()\n",
    "\n",
    "    sys.setrecursionlimit(10000)\n",
    "\n",
    "    tree = cKDTree(mat)\n",
    "    \n",
    "    if n_core < 4:\n",
    "        N_JOBS = 18\n",
    "    else:\n",
    "        N_JOBS = n_core\n",
    "    \n",
    "    nn_tree = tree.query(mat, k=2, n_jobs=N_JOBS)\n",
    "\n",
    "    sys.setrecursionlimit(3000)\n",
    "    \n",
    "    return nn_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df, property_used, rand=False):\n",
    "    global features\n",
    "    features = non_pg_cols + pg_cols\n",
    "    if rand:\n",
    "        filenames = df['filename'].unique().tolist()\n",
    "        random.shuffle(filenames)\n",
    "        n_files = len(filenames)\n",
    "        n_train = round(n_files*training_pct)\n",
    "        train_fn = filenames[0:n_train]\n",
    "        test_fn = filenames[n_train:]\n",
    "    else:\n",
    "        nn_tree = NNtree(df)\n",
    "        dt = nn_tree[0] #distance tree\n",
    "        nt = nn_tree[1] #neighbor tree\n",
    "\n",
    "        to_sort = zip(nt[:, 0],dt[:, 1]) \n",
    "        srt = sorted(to_sort, key=lambda x: x[1], reverse=True)\n",
    "        srt_inds = [x[0] for x in srt]\n",
    "        train_inds = srt_inds[:n_remote]\n",
    "        remaining = srt_inds[n_remote:]\n",
    "        \n",
    "        seed = 0\n",
    "        random.Random(seed).shuffle(remaining)\n",
    "        n_train_left = round(len(srt_inds)*training_pct - n_remote)\n",
    "\n",
    "        train_inds += remaining[:n_train_left]\n",
    "        train_fn = [df['filename'].iloc[ind] for ind in train_inds]\n",
    "        \n",
    "        test_inds = remaining[n_train_left:]\n",
    "        test_fn = [df['filename'].iloc[ind] for ind in test_inds]   \n",
    "        \n",
    "    train_df = df[df['filename'].isin(train_fn)].reset_index().drop('index', axis=1)\n",
    "    test_df = df[df['filename'].isin(test_fn)].reset_index().drop('index', axis=1)\n",
    "    return train_df, test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dtype(train_df, test_df):\n",
    "    global train_label, test_label\n",
    "    train_fp = train_df[features].to_numpy().astype('float32')\n",
    "    train_label = train_df[property_used]\n",
    "    test_fp = test_df[features].to_numpy().astype('float32')\n",
    "    test_label = test_df[property_used]\n",
    "    \n",
    "    if algo == 'xgb':\n",
    "        global train_d, test_d\n",
    "        train_d = xgb.DMatrix(data=train_fp, label=train_label)\n",
    "        test_d = xgb.DMatrix(data=test_fp, label=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(params=None):\n",
    "    start = time.time()\n",
    "    if algo == 'xgb':\n",
    "        if params==None:\n",
    "            params = default_params\n",
    "        \n",
    "        model = xgb.train(params, train_d, n_trees)\n",
    "    end = time.time()\n",
    "    print(\"Elapsed time during model training: \", end-start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(a, b):\n",
    "    '''\n",
    "    Compute rmse between a and b\n",
    "    '''\n",
    "    return math.sqrt(np.mean(np.square(np.subtract(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale(property_name, test_predictions, train_predictions, test_label, train_label):\n",
    "    '''\n",
    "    Undo the scaling on predictions of test set, labels of test set, labels of training set\n",
    "    '''\n",
    "    mean = target_mean\n",
    "    std = target_std\n",
    "    res_test_predictions = (test_predictions * std) + mean\n",
    "    res_test_label = (test_label * std) + mean\n",
    "    res_train_label = (train_label * std) + mean    \n",
    "    res_train_predictions = (train_predictions * std) + mean   \n",
    "    return res_test_predictions, res_test_label, res_train_label, res_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_plot(model, save=False, fname=None):\n",
    "    '''\n",
    "    Make parity plot and save with name equal to fname\n",
    "    '''\n",
    "    if algo=='xgb':\n",
    "        test_predictions = model.predict(test_d)\n",
    "        train_predictions = model.predict(train_d)\n",
    "    res_test_predictions, res_test_label, res_train_label, res_train_predictions = unscale(property_used, \n",
    "                                                                                           test_predictions, \n",
    "                                                                                           train_predictions, \n",
    "                                                                                           test_label, train_label)\n",
    "    fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "\n",
    "    rmse = get_rmse(res_test_label, res_test_predictions)\n",
    "\n",
    "    tr_rmse = get_rmse(res_train_label, res_train_predictions)\n",
    "\n",
    "    from sklearn.metrics import r2_score as r2\n",
    "\n",
    "    r2_val = r2(y_true=res_test_label, y_pred=res_test_predictions)\n",
    "    r2_tr = r2(y_true=res_train_label, y_pred=res_train_predictions)\n",
    "\n",
    "    print(\"This is Test RMSE: \", rmse)\n",
    "    print(\"This is Train RMSE: \", tr_rmse)\n",
    "\n",
    "    ax1.scatter(res_test_label, res_test_predictions, c='r',s=10, label='Test')\n",
    "    ax1.set_xlabel('True Volumetric Uptake',fontsize=labelfontsize)\n",
    "    ax1.set_ylabel('Predicted Volumetric Uptake',fontsize=labelfontsize)\n",
    "    max_val = max([max(res_test_label),max(res_test_predictions)])+1\n",
    "    ax1.set_xlim(0, max_val)\n",
    "    ax1.set_ylim(0, max_val)\n",
    "\n",
    "    plot_x_min, plot_x_max = plt.xlim()\n",
    "    plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "    ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "    text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "    text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.25\n",
    "\n",
    "    ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "             \"RMSE train=\" + str(\"%.4f\" % tr_rmse) + '\\n' +\n",
    "             \"R2 test=\" + str(\"%.4f\" % r2_val) + '\\n' +\n",
    "             \"R2 train=\" + str(\"%.4f\" % r2_tr), ha='left', fontsize=16)\n",
    "    fig1.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(fname,dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "ml_data, property_used = prepToSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = trainTestSplit(ml_data, property_used, rand=random_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_dtype(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:58:21] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:58:21] WARNING: /workspace/src/learner.cc:328: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Elapsed time during model training:  12.565236806869507\n"
     ]
    }
   ],
   "source": [
    "MODEL = run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-24bdb01f9859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparity_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-173-11f31182a0d6>\u001b[0m in \u001b[0;36mparity_plot\u001b[0;34m(model, save, fname)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                                                            \u001b[0mtest_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                                            \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                                                            test_label, train_label)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfig1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-da82b4562aff>\u001b[0m in \u001b[0;36munscale\u001b[0;34m(property_name, test_predictions, train_predictions, test_label, train_label)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mUndo\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscaling\u001b[0m \u001b[0mon\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mres_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_predictions\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_mean' is not defined"
     ]
    }
   ],
   "source": [
    "parity_plot(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
