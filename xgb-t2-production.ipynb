{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/modules/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize\n",
    "\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables which will be inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'xgb' #am I using XGBoost (xgb) or Neural Nets (nn)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1 #total fraction of data set to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = False #make True if the training data should be chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_remote = 2000 #the n_remote most remote points will be added to training set if random_split = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature by number of atoms\n",
    "stacked = False #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 1 #number of cores to use\n",
    "ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_str = 'filename'\n",
    "end_str = 'valence_pa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_geometric_fp = False #make True if you want to ignore the geometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 30 #number of weak learners. Bigger is better until 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pp = False #make True if you want to save the parity plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_data():\n",
    "    return pd.read_csv(ML_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_size(df):\n",
    "    '''\n",
    "    Remove unnecessary columns from df to decrease size\n",
    "    '''\n",
    "    return df.drop([col for col in df.keys() if 'Smiles' in col] + [col for col in \n",
    "                                                                             df.keys() if 'Unnamed' in col] + ['#_of_Linkers'] + ['Metal_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim(df):\n",
    "    '''\n",
    "    Return data set which contains total_frac of original data set\n",
    "    '''\n",
    "    if total_frac != 1:\n",
    "        df = df.sample(frac=total_frac, random_state=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPGcolNames(df):\n",
    "    '''\n",
    "    Return list of PG-relevant column names\n",
    "    '''\n",
    "    for ind, col in enumerate(df.columns):\n",
    "        if start_str == col:\n",
    "            start_col = ind + 1\n",
    "        elif end_str == col:\n",
    "            end_col = ind\n",
    "    return list(df.columns[start_col:end_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonPGcolNames():\n",
    "    features = ['norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "    \n",
    "    if not del_geometric_fp:\n",
    "        features += ['norm_Dom._Pore_(ang.)',\n",
    "                     'norm_Max._Pore_(ang.)',\n",
    "                     'norm_Void_Fraction',\n",
    "                     'norm_Surf._Area_(m2/g)',\n",
    "                     'norm_Vol._Surf._Area',\n",
    "                     'norm_Density']\n",
    "    features += cat_col_names\n",
    "    \n",
    "    if stacked:\n",
    "        features += ['norm_log_pressure']\n",
    "        \n",
    "    if add_size_fp:\n",
    "        features += ['size_%s' %n for n in range(20)]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df):\n",
    "    '''\n",
    "    Merge gravimetric uptake\n",
    "    '''\n",
    "    if not stacked:\n",
    "        y_data = pd.read_excel(Y_DATA_PATH)\n",
    "        df = df.join(y_data[['Crystal ID#', 'CH4 cm3/g 35 bar']].set_index('Crystal ID#'), on='Crystal_ID#')\n",
    "        for key in df.keys():\n",
    "            if ' ' in key:\n",
    "                new_key = key.replace(' ', '_')\n",
    "                df[new_key] = df[key]\n",
    "                df = df.drop(key, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_col(df, col_name):\n",
    "    \n",
    "    mean = df[col_name].mean()\n",
    "    std = df[col_name].std()\n",
    "    \n",
    "    property_used = 'norm_' + col_name\n",
    "    df[property_used] = (df[col_name] - mean) / std\n",
    "\n",
    "    return mean, std, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(df):\n",
    "    if stacked:\n",
    "        to_norm = 'vol_uptake'\n",
    "    else:\n",
    "        to_norm = 'CH4_cm3/g_35_bar'\n",
    "    mean, std, property_used = norm_col(df, to_norm)\n",
    "    return mean, std, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delMissing(df, property_used, reset=True):\n",
    "    '''\n",
    "    Remove rows from df with missing values in target\n",
    "    '''\n",
    "    df = df.iloc[df[property_used].dropna().index]\n",
    "    if reset:\n",
    "        df = df.reset_index().drop('index', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normPressure(df):\n",
    "    '''\n",
    "    Normalize pressure feature and globally save important metrics. Only use if stacked == True!\n",
    "    '''\n",
    "    df['log_pressure'] = np.log(df['pressure'].tolist())\n",
    "    \n",
    "    #global log_p_mean, log_p_std, max_p, min_p #save these variables globally\n",
    "    \n",
    "    log_p_mean, log_p_std, property_used = norm_col(df, 'log_pressure')\n",
    "\n",
    "    max_p = max(df[property_used].tolist())\n",
    "    min_p = min(df[property_used].tolist())\n",
    "    \n",
    "    return df, log_p_mean, log_p_std, max_p, min_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat(s):\n",
    "    '''\n",
    "    Returns interpenetration from filename\n",
    "    '''\n",
    "    if 'cat' in s:\n",
    "        return int(s.split('_')[-1][0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCat(df):\n",
    "    '''\n",
    "    Add normed one-hot encoded features for catenation (interpenetration) of MOF\n",
    "    '''\n",
    "    one_hot = [[0]*4 for i in range(len(df))]\n",
    "    for i, f in enumerate(df['filename'].tolist()):\n",
    "        one_hot[i][get_cat(f)] = 1\n",
    "    oh_1 = []\n",
    "    oh_2 = []\n",
    "    oh_3 = []\n",
    "    oh_4 = []\n",
    "    for i in one_hot:\n",
    "        oh_1.append(i[0])\n",
    "        oh_2.append(i[1])\n",
    "        oh_3.append(i[2])\n",
    "        oh_4.append(i[3])\n",
    "    df[cat_col_names[0]] = oh_1\n",
    "    df[cat_col_names[1]] = oh_2\n",
    "    df[cat_col_names[2]] = oh_3\n",
    "    df[cat_col_names[3]] = oh_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepToSplit():\n",
    "    '''\n",
    "    Carry out a series of tasks to prepare the data to be split into test and train sets\n",
    "    '''\n",
    "    ml_data = load_ml_data()\n",
    "    global pg_cols\n",
    "    pg_cols = getPGcolNames(ml_data)\n",
    "    ml_data = reduce_size(ml_data)\n",
    "    ml_data = slim(ml_data)\n",
    "    global non_pg_cols\n",
    "    non_pg_cols = getNonPGcolNames()\n",
    "    ml_data = merge_data(ml_data)\n",
    "    prepare_target(ml_data)\n",
    "    global target_mean, target_std\n",
    "    target_mean, target_std, property_used = prepare_target(ml_data)\n",
    "    ml_data = delMissing(ml_data, property_used)\n",
    "\n",
    "    if stacked:\n",
    "        global log_p_mean, log_p_std, max_p, min_p\n",
    "        ml_data, log_p_mean, log_p_std, max_p, min_p = normPressure(ml_data)\n",
    "    addCat(ml_data)\n",
    "    return ml_data, property_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNtree(df, stacked, n_core):\n",
    "    if stacked:\n",
    "        non_p_features = [col for col in features if col != 'norm_log_pressure']\n",
    "        filenames = df[non_p_features].drop_duplicates()['filename'].tolist()\n",
    "        mat = df[non_p_features].drop_duplicates().to_numpy()\n",
    "    else:\n",
    "        non_p_features = features\n",
    "        mat = df[non_p_features].to_numpy()\n",
    "        filenames = df['filename'].tolist()\n",
    "        \n",
    "    sys.setrecursionlimit(10000)\n",
    "\n",
    "    tree = cKDTree(mat)\n",
    "    \n",
    "    if n_core < 4:\n",
    "        N_JOBS = 18\n",
    "    else:\n",
    "        N_JOBS = n_core\n",
    "    \n",
    "    nn_tree = tree.query(mat, k=2, n_jobs=N_JOBS)\n",
    "\n",
    "    sys.setrecursionlimit(3000)\n",
    "    \n",
    "    return nn_tree, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df, property_used, rand=False):\n",
    "    global features\n",
    "    features = non_pg_cols + pg_cols\n",
    "    print(\"Using following %s features\" %len(features))\n",
    "    for i in features:\n",
    "        print(i)\n",
    "    if rand:\n",
    "        filenames = df['filename'].unique().tolist()\n",
    "        random.shuffle(filenames)\n",
    "        n_files = len(filenames)\n",
    "        n_train = round(n_files*training_pct)\n",
    "        train_fn = filenames[0:n_train]\n",
    "        test_fn = filenames[n_train:]\n",
    "    else:\n",
    "        nn_tree = NNtree(df)\n",
    "        dt = nn_tree[0] #distance tree\n",
    "        nt = nn_tree[1] #neighbor tree\n",
    "\n",
    "        to_sort = zip(nt[:, 0],dt[:, 1]) \n",
    "        srt = sorted(to_sort, key=lambda x: x[1], reverse=True)\n",
    "        srt_inds = [x[0] for x in srt]\n",
    "        train_inds = srt_inds[:n_remote]\n",
    "        remaining = srt_inds[n_remote:]\n",
    "        \n",
    "        seed = 0\n",
    "        random.Random(seed).shuffle(remaining)\n",
    "        n_train_left = round(len(srt_inds)*training_pct - n_remote)\n",
    "\n",
    "        train_inds += remaining[:n_train_left]\n",
    "        train_fn = [df['filename'].iloc[ind] for ind in train_inds]\n",
    "        \n",
    "        test_inds = remaining[n_train_left:]\n",
    "        test_fn = [df['filename'].iloc[ind] for ind in test_inds]   \n",
    "        \n",
    "    train_df = df[df['filename'].isin(train_fn)].reset_index().drop('index', axis=1)\n",
    "    test_df = df[df['filename'].isin(test_fn)].reset_index().drop('index', axis=1)\n",
    "    return train_df, test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dtype(train_df, test_df):\n",
    "    global train_label, test_label\n",
    "    train_fp = train_df[features].to_numpy().astype('float32')\n",
    "    train_label = train_df[property_used]\n",
    "    test_fp = test_df[features].to_numpy().astype('float32')\n",
    "    test_label = test_df[property_used]\n",
    "    \n",
    "    if algo == 'xgb':\n",
    "        global train_d, test_d\n",
    "        train_d = xgb.DMatrix(data=train_fp, label=train_label)\n",
    "        test_d = xgb.DMatrix(data=test_fp, label=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(params=None):\n",
    "    start = time.time()\n",
    "    if algo == 'xgb':\n",
    "        if params==None:\n",
    "            params = default_params\n",
    "        \n",
    "        model = xgb.train(params, train_d, n_trees)\n",
    "    end = time.time()\n",
    "    print(\"Elapsed time during model training: \", end-start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(a, b):\n",
    "    '''\n",
    "    Compute rmse between a and b\n",
    "    '''\n",
    "    return math.sqrt(np.mean(np.square(np.subtract(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale(property_name, test_predictions, train_predictions, test_label, train_label):\n",
    "    '''\n",
    "    Undo the scaling on predictions of test set, labels of test set, labels of training set\n",
    "    '''\n",
    "    mean = target_mean\n",
    "    std = target_std\n",
    "    res_test_predictions = (test_predictions * std) + mean\n",
    "    res_test_label = (test_label * std) + mean\n",
    "    res_train_label = (train_label * std) + mean    \n",
    "    res_train_predictions = (train_predictions * std) + mean   \n",
    "    return res_test_predictions, res_test_label, res_train_label, res_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_plot(model, save=False, fname=None):\n",
    "    '''\n",
    "    Make parity plot and save with name equal to fname\n",
    "    '''\n",
    "    if algo=='xgb':\n",
    "        test_predictions = model.predict(test_d)\n",
    "        train_predictions = model.predict(train_d)\n",
    "    res_test_predictions, res_test_label, res_train_label, res_train_predictions = unscale(property_used, \n",
    "                                                                                           test_predictions, \n",
    "                                                                                           train_predictions, \n",
    "                                                                                           test_label, train_label)\n",
    "    fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "\n",
    "    rmse = get_rmse(res_test_label, res_test_predictions)\n",
    "\n",
    "    tr_rmse = get_rmse(res_train_label, res_train_predictions)\n",
    "\n",
    "    from sklearn.metrics import r2_score as r2\n",
    "\n",
    "    r2_val = r2(y_true=res_test_label, y_pred=res_test_predictions)\n",
    "    r2_tr = r2(y_true=res_train_label, y_pred=res_train_predictions)\n",
    "\n",
    "    print(\"This is Test RMSE: \", rmse)\n",
    "    print(\"This is Train RMSE: \", tr_rmse)\n",
    "\n",
    "    ax1.scatter(res_test_label, res_test_predictions, c='r',s=10, label='Test')\n",
    "    ax1.set_xlabel('True Volumetric Uptake',fontsize=labelfontsize)\n",
    "    ax1.set_ylabel('Predicted Volumetric Uptake',fontsize=labelfontsize)\n",
    "    max_val = max([max(res_test_label),max(res_test_predictions)])+1\n",
    "    ax1.set_xlim(0, max_val)\n",
    "    ax1.set_ylim(0, max_val)\n",
    "\n",
    "    plot_x_min, plot_x_max = plt.xlim()\n",
    "    plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "    ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "    text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "    text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.25\n",
    "\n",
    "    ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "             \"RMSE train=\" + str(\"%.4f\" % tr_rmse) + '\\n' +\n",
    "             \"R2 test=\" + str(\"%.4f\" % r2_val) + '\\n' +\n",
    "             \"R2 train=\" + str(\"%.4f\" % r2_tr), ha='left', fontsize=16)\n",
    "    fig1.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(fname,dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following 419 features\n",
      "norm_valence_pa\n",
      "norm_atomic_rad_pa_(angstroms)\n",
      "norm_affinity_pa_(eV)\n",
      "norm_ionization_potential_pa_(eV)\n",
      "norm_electronegativity_pa\n",
      "norm_Dom._Pore_(ang.)\n",
      "norm_Max._Pore_(ang.)\n",
      "norm_Void_Fraction\n",
      "norm_Surf._Area_(m2/g)\n",
      "norm_Vol._Surf._Area\n",
      "norm_Density\n",
      "oh_1\n",
      "oh_2\n",
      "oh_3\n",
      "oh_4\n",
      "Mafp_Br1_C2_C1\n",
      "Mafp_Br1_C2_C2\n",
      "Mafp_Br1_C2_C3\n",
      "Mafp_Br1_C3_Br1\n",
      "Mafp_Br1_C3_C1\n",
      "Mafp_Br1_C3_C2\n",
      "Mafp_Br1_C3_C3\n",
      "Mafp_Br1_C3_C4\n",
      "Mafp_Br1_C3_N1\n",
      "Mafp_Br1_C3_N2\n",
      "Mafp_Br1_C3_N3\n",
      "Mafp_Br1_C3_O1\n",
      "Mafp_Br1_C4_Br1\n",
      "Mafp_Br1_C4_C2\n",
      "Mafp_Br1_C4_C3\n",
      "Mafp_Br1_C4_C4\n",
      "Mafp_Br1_C4_H1\n",
      "Mafp_Br1_C4_N1\n",
      "Mafp_Br1_C4_N2\n",
      "Mafp_Br1_C4_N3\n",
      "Mafp_Br1_C4_O1\n",
      "Mafp_Br1_C4_O2\n",
      "Mafp_Br1_N2_C2\n",
      "Mafp_Br1_N2_C3\n",
      "Mafp_Br1_N2_C4\n",
      "Mafp_Br1_N2_N1\n",
      "Mafp_Br1_N2_N2\n",
      "Mafp_Br1_N2_N3\n",
      "Mafp_Br1_N3_Br1\n",
      "Mafp_Br1_N3_C2\n",
      "Mafp_Br1_N3_C3\n",
      "Mafp_Br1_N3_H1\n",
      "Mafp_Br1_N3_N2\n",
      "Mafp_Br1_N3_O2\n",
      "Mafp_Br1_O2_C2\n",
      "Mafp_Br1_O2_C3\n",
      "Mafp_Br1_O2_C4\n",
      "Mafp_C1_C2_C2\n",
      "Mafp_C1_C2_C3\n",
      "Mafp_C1_C2_C4\n",
      "Mafp_C1_C2_F1\n",
      "Mafp_C1_C2_H1\n",
      "Mafp_C1_C2_O1\n",
      "Mafp_C1_C2_O2\n",
      "Mafp_C1_C3_C2\n",
      "Mafp_C1_C3_C3\n",
      "Mafp_C1_C3_C4\n",
      "Mafp_C1_C3_Cl1\n",
      "Mafp_C1_C3_F1\n",
      "Mafp_C1_C3_H1\n",
      "Mafp_C1_C3_N2\n",
      "Mafp_C1_C3_N3\n",
      "Mafp_C1_C3_O1\n",
      "Mafp_C1_C3_O2\n",
      "Mafp_C1_C4_C2\n",
      "Mafp_C1_C4_C3\n",
      "Mafp_C1_C4_C4\n",
      "Mafp_C1_C4_H1\n",
      "Mafp_C1_C4_O1\n",
      "Mafp_C1_C4_O2\n",
      "Mafp_C1_N2_C2\n",
      "Mafp_C1_N2_C3\n",
      "Mafp_C1_N2_N2\n",
      "Mafp_C1_N2_N3\n",
      "Mafp_C1_N3_C3\n",
      "Mafp_C1_N3_C4\n",
      "Mafp_C1_N3_N2\n",
      "Mafp_C1_N3_N3\n",
      "Mafp_C1_N3_O1\n",
      "Mafp_C1_O2_C3\n",
      "Mafp_C1_O2_C4\n",
      "Mafp_C2_C2_C2\n",
      "Mafp_C2_C2_C3\n",
      "Mafp_C2_C2_C4\n",
      "Mafp_C2_C2_Cl1\n",
      "Mafp_C2_C2_F1\n",
      "Mafp_C2_C2_H1\n",
      "Mafp_C2_C2_N1\n",
      "Mafp_C2_C2_N2\n",
      "Mafp_C2_C2_N3\n",
      "Mafp_C2_C2_O1\n",
      "Mafp_C2_C2_O2\n",
      "Mafp_C2_C3_C2\n",
      "Mafp_C2_C3_C3\n",
      "Mafp_C2_C3_C4\n",
      "Mafp_C2_C3_Cl1\n",
      "Mafp_C2_C3_F1\n",
      "Mafp_C2_C3_H1\n",
      "Mafp_C2_C3_N1\n",
      "Mafp_C2_C3_N2\n",
      "Mafp_C2_C3_N3\n",
      "Mafp_C2_C3_O1\n",
      "Mafp_C2_C3_O2\n",
      "Mafp_C2_C4_C2\n",
      "Mafp_C2_C4_C3\n",
      "Mafp_C2_C4_C4\n",
      "Mafp_C2_C4_Cl1\n",
      "Mafp_C2_C4_F1\n",
      "Mafp_C2_C4_H1\n",
      "Mafp_C2_C4_N2\n",
      "Mafp_C2_C4_N3\n",
      "Mafp_C2_C4_O1\n",
      "Mafp_C2_C4_O2\n",
      "Mafp_C2_N2_C2\n",
      "Mafp_C2_N2_C3\n",
      "Mafp_C2_N2_C4\n",
      "Mafp_C2_N2_Cl1\n",
      "Mafp_C2_N2_F1\n",
      "Mafp_C2_N2_H1\n",
      "Mafp_C2_N2_N1\n",
      "Mafp_C2_N2_N2\n",
      "Mafp_C2_N2_N3\n",
      "Mafp_C2_N2_O1\n",
      "Mafp_C2_N2_O2\n",
      "Mafp_C2_N3_C2\n",
      "Mafp_C2_N3_C3\n",
      "Mafp_C2_N3_C4\n",
      "Mafp_C2_N3_F1\n",
      "Mafp_C2_N3_H1\n",
      "Mafp_C2_N3_N2\n",
      "Mafp_C2_N3_N3\n",
      "Mafp_C2_N3_O1\n",
      "Mafp_C2_N3_O2\n",
      "Mafp_C2_O2_C2\n",
      "Mafp_C2_O2_C3\n",
      "Mafp_C2_O2_C4\n",
      "Mafp_C2_O2_Cl1\n",
      "Mafp_C2_O2_H1\n",
      "Mafp_C2_O2_N2\n",
      "Mafp_C2_O2_N3\n",
      "Mafp_C3_C2_C3\n",
      "Mafp_C3_C2_C4\n",
      "Mafp_C3_C2_Cl1\n",
      "Mafp_C3_C2_F1\n",
      "Mafp_C3_C2_H1\n",
      "Mafp_C3_C2_N1\n",
      "Mafp_C3_C2_N2\n",
      "Mafp_C3_C2_N3\n",
      "Mafp_C3_C2_O1\n",
      "Mafp_C3_C2_O2\n",
      "Mafp_C3_C3_C3\n",
      "Mafp_C3_C3_C4\n",
      "Mafp_C3_C3_Cl1\n",
      "Mafp_C3_C3_F1\n",
      "Mafp_C3_C3_H1\n",
      "Mafp_C3_C3_N1\n",
      "Mafp_C3_C3_N2\n",
      "Mafp_C3_C3_N3\n",
      "Mafp_C3_C3_O1\n",
      "Mafp_C3_C3_O2\n",
      "Mafp_C3_C4_C3\n",
      "Mafp_C3_C4_C4\n",
      "Mafp_C3_C4_Cl1\n",
      "Mafp_C3_C4_F1\n",
      "Mafp_C3_C4_H1\n",
      "Mafp_C3_C4_N1\n",
      "Mafp_C3_C4_N2\n",
      "Mafp_C3_C4_N3\n",
      "Mafp_C3_C4_O1\n",
      "Mafp_C3_C4_O2\n",
      "Mafp_C3_N2_C3\n",
      "Mafp_C3_N2_C4\n",
      "Mafp_C3_N2_Cl1\n",
      "Mafp_C3_N2_F1\n",
      "Mafp_C3_N2_H1\n",
      "Mafp_C3_N2_N1\n",
      "Mafp_C3_N2_N2\n",
      "Mafp_C3_N2_N3\n",
      "Mafp_C3_N2_O1\n",
      "Mafp_C3_N2_O2\n",
      "Mafp_C3_N3_C3\n",
      "Mafp_C3_N3_C4\n",
      "Mafp_C3_N3_Cl1\n",
      "Mafp_C3_N3_F1\n",
      "Mafp_C3_N3_H1\n",
      "Mafp_C3_N3_N1\n",
      "Mafp_C3_N3_N2\n",
      "Mafp_C3_N3_N3\n",
      "Mafp_C3_N3_O2\n",
      "Mafp_C3_O2_C3\n",
      "Mafp_C3_O2_C4\n",
      "Mafp_C3_O2_Cl1\n",
      "Mafp_C3_O2_F1\n",
      "Mafp_C3_O2_H1\n",
      "Mafp_C3_O2_N2\n",
      "Mafp_C3_O2_N3\n",
      "Mafp_C3_O2_O1\n",
      "Mafp_C3_O2_O2\n",
      "Mafp_C4_C2_C4\n",
      "Mafp_C4_C2_H1\n",
      "Mafp_C4_C2_N1\n",
      "Mafp_C4_C2_N2\n",
      "Mafp_C4_C2_N3\n",
      "Mafp_C4_C2_O1\n",
      "Mafp_C4_C2_O2\n",
      "Mafp_C4_C3_C4\n",
      "Mafp_C4_C3_Cl1\n",
      "Mafp_C4_C3_F1\n",
      "Mafp_C4_C3_H1\n",
      "Mafp_C4_C3_N1\n",
      "Mafp_C4_C3_N2\n",
      "Mafp_C4_C3_N3\n",
      "Mafp_C4_C3_O1\n",
      "Mafp_C4_C3_O2\n",
      "Mafp_C4_C4_C4\n",
      "Mafp_C4_C4_Cl1\n",
      "Mafp_C4_C4_F1\n",
      "Mafp_C4_C4_H1\n",
      "Mafp_C4_C4_N1\n",
      "Mafp_C4_C4_N2\n",
      "Mafp_C4_C4_N3\n",
      "Mafp_C4_C4_O1\n",
      "Mafp_C4_C4_O2\n",
      "Mafp_C4_N2_C4\n",
      "Mafp_C4_N2_H1\n",
      "Mafp_C4_N2_N1\n",
      "Mafp_C4_N2_N2\n",
      "Mafp_C4_N2_N3\n",
      "Mafp_C4_N2_O1\n",
      "Mafp_C4_N3_C4\n",
      "Mafp_C4_N3_F1\n",
      "Mafp_C4_N3_H1\n",
      "Mafp_C4_N3_N1\n",
      "Mafp_C4_N3_N2\n",
      "Mafp_C4_N3_N3\n",
      "Mafp_C4_N3_O1\n",
      "Mafp_C4_N3_O2\n",
      "Mafp_C4_O2_C4\n",
      "Mafp_C4_O2_Cl1\n",
      "Mafp_C4_O2_F1\n",
      "Mafp_C4_O2_H1\n",
      "Mafp_C4_O2_N2\n",
      "Mafp_C4_O2_N3\n",
      "Mafp_C4_O2_O1\n",
      "Mafp_C4_O2_O2\n",
      "Mafp_Cl1_C3_Cl1\n",
      "Mafp_Cl1_C3_H1\n",
      "Mafp_Cl1_C3_N1\n",
      "Mafp_Cl1_C3_N2\n",
      "Mafp_Cl1_C3_N3\n",
      "Mafp_Cl1_C3_O1\n",
      "Mafp_Cl1_C4_Cl1\n",
      "Mafp_Cl1_C4_H1\n",
      "Mafp_Cl1_C4_N2\n",
      "Mafp_Cl1_C4_N3\n",
      "Mafp_Cl1_C4_O1\n",
      "Mafp_Cl1_C4_O2\n",
      "Mafp_Cl1_N2_N1\n",
      "Mafp_Cl1_N2_N2\n",
      "Mafp_Cl1_N3_Cl1\n",
      "Mafp_Cl1_N3_H1\n",
      "Mafp_Cl1_N3_N2\n",
      "Mafp_F1_C3_F1\n",
      "Mafp_F1_C3_N1\n",
      "Mafp_F1_C3_N2\n",
      "Mafp_F1_C3_N3\n",
      "Mafp_F1_C3_O1\n",
      "Mafp_F1_C3_O2\n",
      "Mafp_F1_C4_F1\n",
      "Mafp_F1_C4_H1\n",
      "Mafp_F1_C4_N2\n",
      "Mafp_F1_C4_N3\n",
      "Mafp_F1_C4_O1\n",
      "Mafp_F1_C4_O2\n",
      "Mafp_F1_N2_N2\n",
      "Mafp_F1_N3_F1\n",
      "Mafp_F1_N3_H1\n",
      "Mafp_F1_N3_N1\n",
      "Mafp_F1_N3_N2\n",
      "Mafp_F1_N3_N3\n",
      "Mafp_H1_C2_H1\n",
      "Mafp_H1_C2_N1\n",
      "Mafp_H1_C2_N2\n",
      "Mafp_H1_C2_N3\n",
      "Mafp_H1_C2_O1\n",
      "Mafp_H1_C2_O2\n",
      "Mafp_H1_C3_H1\n",
      "Mafp_H1_C3_N1\n",
      "Mafp_H1_C3_N2\n",
      "Mafp_H1_C3_N3\n",
      "Mafp_H1_C3_O1\n",
      "Mafp_H1_C3_O2\n",
      "Mafp_H1_C4_H1\n",
      "Mafp_H1_C4_N2\n",
      "Mafp_H1_C4_N3\n",
      "Mafp_H1_C4_O1\n",
      "Mafp_H1_C4_O2\n",
      "Mafp_H1_N2_H1\n",
      "Mafp_H1_N2_N1\n",
      "Mafp_H1_N2_N2\n",
      "Mafp_H1_N2_N3\n",
      "Mafp_H1_N3_H1\n",
      "Mafp_H1_N3_N1\n",
      "Mafp_H1_N3_N2\n",
      "Mafp_H1_N3_N3\n",
      "Mafp_H1_N3_O2\n",
      "Mafp_H1_O2_H1\n",
      "Mafp_H1_O2_N2\n",
      "Mafp_H1_O2_O1\n",
      "Mafp_H1_O2_O2\n",
      "Mafp_N1_C2_N2\n",
      "Mafp_N1_C2_N3\n",
      "Mafp_N1_C2_O2\n",
      "Mafp_N1_C3_N2\n",
      "Mafp_N1_C3_N3\n",
      "Mafp_N1_C3_O1\n",
      "Mafp_N1_C3_O2\n",
      "Mafp_N1_C4_N2\n",
      "Mafp_N1_C4_N3\n",
      "Mafp_N1_C4_O2\n",
      "Mafp_N1_N2_N3\n",
      "Mafp_N1_N3_N2\n",
      "Mafp_N1_N3_N3\n",
      "Mafp_N2_C2_N2\n",
      "Mafp_N2_C2_N3\n",
      "Mafp_N2_C2_O2\n",
      "Mafp_N2_C3_N2\n",
      "Mafp_N2_C3_N3\n",
      "Mafp_N2_C3_O1\n",
      "Mafp_N2_C3_O2\n",
      "Mafp_N2_C4_N2\n",
      "Mafp_N2_C4_N3\n",
      "Mafp_N2_C4_O1\n",
      "Mafp_N2_C4_O2\n",
      "Mafp_N2_N2_N2\n",
      "Mafp_N2_N2_N3\n",
      "Mafp_N2_N2_O2\n",
      "Mafp_N2_N3_N2\n",
      "Mafp_N2_N3_N3\n",
      "Mafp_N2_N3_O2\n",
      "Mafp_N2_O2_N3\n",
      "Mafp_N3_C2_O1\n",
      "Mafp_N3_C3_N3\n",
      "Mafp_N3_C3_O1\n",
      "Mafp_N3_C3_O2\n",
      "Mafp_N3_C4_N3\n",
      "Mafp_N3_C4_O1\n",
      "Mafp_N3_C4_O2\n",
      "Mafp_N3_N2_N3\n",
      "Mafp_N3_N2_O1\n",
      "Mafp_N3_N2_O2\n",
      "Mafp_N3_N3_N3\n",
      "Mafp_N3_N3_O1\n",
      "Mafp_N3_N3_O2\n",
      "Mafp_N3_O2_N3\n",
      "Mafp_O1_C2_O1\n",
      "Mafp_O1_C2_O2\n",
      "Mafp_O1_C3_O1\n",
      "Mafp_O1_C3_O2\n",
      "Mafp_O1_C4_O1\n",
      "Mafp_O1_C4_O2\n",
      "Mafp_O1_N3_O2\n",
      "Mafp_O2_C2_O2\n",
      "Mafp_O2_C3_O2\n",
      "Mafp_O2_C4_O2\n",
      "Mafp_O2_N3_O2\n",
      "Mefp_fam_acrylate\n",
      "Mefp_fam_carbonateester\n",
      "Mefp_fam_ketone\n",
      "Mefp_fam_polyamides\n",
      "Mefp_fam_single\n",
      "Mefp_norm_mol_wt\n",
      "Mefp_numatoms_none_H\n",
      "Mefp_ring\n",
      "Mmfp_Chi0n\n",
      "Mmfp_Chi0v\n",
      "Mmfp_Chi1n\n",
      "Mmfp_Chi1v\n",
      "Mmfp_Chi2n\n",
      "Mmfp_Chi2v\n",
      "Mmfp_HallKierAlpha\n",
      "Mmfp_MQNs13\n",
      "Mmfp_MQNs14\n",
      "Mmfp_MQNs15\n",
      "Mmfp_MQNs16\n",
      "Mmfp_MQNs17\n",
      "Mmfp_MQNs18\n",
      "Mmfp_MQNs19\n",
      "Mmfp_MQNs20\n",
      "Mmfp_MQNs21\n",
      "Mmfp_MQNs22\n",
      "Mmfp_MQNs23\n",
      "Mmfp_MQNs24\n",
      "Mmfp_MQNs25\n",
      "Mmfp_MQNs26\n",
      "Mmfp_MQNs27\n",
      "Mmfp_MQNs28\n",
      "Mmfp_MQNs29\n",
      "Mmfp_MQNs30\n",
      "Mmfp_MQNs31\n",
      "Mmfp_MQNs32\n",
      "Mmfp_MQNs33\n",
      "Mmfp_MQNs34\n",
      "Mmfp_MQNs35\n",
      "Mmfp_MQNs36\n",
      "Mmfp_MQNs37\n",
      "Mmfp_MQNs38\n",
      "Mmfp_MQNs39\n",
      "Mmfp_MQNs40\n",
      "Mmfp_MQNs41\n",
      "Mmfp_MQNs42\n",
      "Mmfp_NumAliphaticRings\n",
      "Mmfp_NumAromaticRings\n",
      "Mmfp_tpsa\n",
      "Top five distance:  [4080, 47509, 50637, 109751, 26753]\n"
     ]
    }
   ],
   "source": [
    "rand = False\n",
    "global features\n",
    "features = non_pg_cols + pg_cols\n",
    "print(\"Using following %s features\" %len(features))\n",
    "for i in features:\n",
    "    print(i)\n",
    "if rand:\n",
    "    filenames = df['filename'].unique().tolist()\n",
    "    random.shuffle(filenames)\n",
    "    n_files = len(filenames)\n",
    "    n_train = round(n_files*training_pct)\n",
    "    train_fn = filenames[0:n_train]\n",
    "    test_fn = filenames[n_train:]\n",
    "else:\n",
    "    nn_tree, filenames = NNtree(df, stacked, n_core)\n",
    "    dt = nn_tree[0] #distance tree\n",
    "    nt = nn_tree[1] #neighbor tree\n",
    "\n",
    "    to_sort = zip(nt[:, 0],dt[:, 1]) \n",
    "    srt = sorted(to_sort, key=lambda x: x[1], reverse=True)\n",
    "    srt_inds = [x[0] for x in srt]\n",
    "    train_inds = srt_inds[:n_remote]\n",
    "    print(\"Top five distance: \", train_inds[0:5])\n",
    "    remaining = srt_inds[n_remote:]\n",
    "\n",
    "    seed = 0\n",
    "    random.Random(seed).shuffle(remaining)\n",
    "    n_train_left = round(len(srt_inds)*training_pct - n_remote)\n",
    "\n",
    "    train_inds += remaining[:n_train_left]\n",
    "    train_fn = [filenames[ind] for ind in train_inds]\n",
    "\n",
    "    test_inds = remaining[n_train_left:]\n",
    "    test_fn = [filenames[ind] for ind in test_inds]   \n",
    "\n",
    "train_df = df[df['filename'].isin(train_fn)].reset_index().drop('index', axis=1)\n",
    "test_df = df[df['filename'].isin(test_fn)].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"a\": [1,2,3,4,5,20,200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = cKDTree(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tree = tree.query(tmp, k=2, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = nn_tree[0] #distance tree\n",
    "nt = nn_tree[1] #neighbor tree\n",
    "\n",
    "to_sort = zip(nt[:, 0],dt[:, 1]) \n",
    "srt = sorted(to_sort, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 180.0), (5, 15.0), (0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    if n_core < 4:\n",
    "        N_JOBS = 18\n",
    "    else:\n",
    "        N_JOBS = n_core\n",
    "    \n",
    "    nn_tree = tree.query(mat, k=2, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "ml_data, property_used = prepToSplit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
