{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/modules/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'efrc_ml_production'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b738a7ced43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mefrc_ml_production\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efrc_ml_production'"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from rdkit import Chem\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append('~/py_scripts')\n",
    "import efrc_ml_production as ml\n",
    "importlib.reload(ml)\n",
    "\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "class hmofMLdataset:\n",
    "    def __init__(self, results_dir, SI_grav_data_path='/data/rgur/efrc/prep_data/all_v1/ml_data.csv', \n",
    "                 SD_grav_data_path='/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv',SI_stacked_path=\n",
    "                '/data/rgur/efrc/prep_data/all_v1/stacked.csv',\n",
    "                 SD_stacked_path='/data/rgur/efrc/prep_data/all_no_norm/stacked.csv',\n",
    "                 Y_DATA_PATH='/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx', n_core=15, skip=[], \n",
    "                 job_dict = None):\n",
    "        self.results_dir = results_dir\n",
    "        os.chdir(self.results_dir)\n",
    "        self.SI_grav_data_path = SI_grav_data_path\n",
    "        self.SD_grav_data_path = SD_grav_data_path\n",
    "        self.SI_stacked_path = SI_stacked_path\n",
    "        self.SD_stacked_path = SD_stacked_path\n",
    "        self.n_core = n_core\n",
    "        self.Y_DATA_PATH = Y_DATA_PATH\n",
    "        self.PCA_COMPONENTS = 400\n",
    "        self.del_defective_mofs = False\n",
    "        self.cat_si_sd = True\n",
    "        self.add_size_fp = True #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "        self.srt_size_fp = True\n",
    "        self.iso_start_str_sd = 'Density'\n",
    "        self.iso_end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "        self.grav_start_str_sd = 'CH4_v/v_248_bar'\n",
    "        self.grav_end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "        self.start_str_si = 'filename'\n",
    "        self.end_str_si = 'valence_pa'\n",
    "        self.cat_col_names = ['cat_1', 'cat_2', 'cat_3', 'cat_4']\n",
    "        self.skip = skip\n",
    "        self.feature_codes = ['10000', '11000', '01000', '10100', '11100', '01100',\n",
    "                             '10010', '11010', '01010', '10110', '11110', '01110',\n",
    "                             '10001', '11001', '01001', '10101', '11101', '01101',\n",
    "                             '10011', '11011', '01011', '10111', '11111', '01111']\n",
    "        self.feature_codes = [i for i in self.feature_codes if i not in self.skip]\n",
    "        self.job_dict = job_dict\n",
    "        if self.job_dict != None:\n",
    "            self.feature_codes = list(job_dict.keys())\n",
    "        print(\"There are %s unique feature codes\" %len(set(self.feature_codes)))\n",
    "        print(\"now is %s\" %now)\n",
    "        \n",
    "    \n",
    "    def makeMasterDFs(self):\n",
    "        #gravimetric\n",
    "        self.grav, self.grav_prop, self.grav_target_mean, self.grav_target_std, self.grav_all_features = ml.prepToSplit(\n",
    "                                            'xgb', self.cat_si_sd, self.SD_grav_data_path, self.SI_grav_data_path, \n",
    "                                            self.grav_start_str_sd, self.grav_end_str_sd, self.start_str_si, \n",
    "                                            self.end_str_si, 1, self.del_defective_mofs, self.add_size_fp, \n",
    "                                            self.srt_size_fp, None, stacked=False, n_core=self.n_core, \n",
    "                                            del_geometric_fp=False, cat_col_names=self.cat_col_names, \n",
    "                                            Y_DATA_PATH=self.Y_DATA_PATH)\n",
    "        size_cols = [\"size_%s\" %s for s in range(20)]\n",
    "        self.LS_dict = {row[1]['filename']:row[1][size_cols] for row in self.grav.iterrows()} # map from filename \n",
    "                                                                                        #to linkersize-vector\n",
    "        #stacked\n",
    "        self.iso, self.iso_prop, self.iso_target_mean, self.iso_target_std, self.iso_all_features, self.pinfo = \\\n",
    "                                            ml.prepToSplit(\n",
    "                                            'nn', self.cat_si_sd, self.SD_stacked_path, self.SI_stacked_path, \n",
    "                                            self.iso_start_str_sd, self.iso_end_str_sd, self.start_str_si, \n",
    "                                            self.end_str_si, 1, self.del_defective_mofs, self.add_size_fp, \n",
    "                                            self.srt_size_fp, None, True, self.n_core, False, self.cat_col_names, \n",
    "                                            self.Y_DATA_PATH, self.LS_dict)\n",
    "\n",
    "    def select_features(self, code, stacked):\n",
    "        '''\n",
    "        Should only be called after makeMasterDFs\n",
    "        '''\n",
    "        si = bool(int(code[0])) #True (=1) if size-independent features are included\n",
    "        sd = bool(int(code[1])) #True (=1) if size-dependent features are included\n",
    "        size_fp = bool(int(code[2])) #True (=1) if linker size features are included\n",
    "        geo_fp = bool(int(code[3])) #True (=1) if geometric features are included\n",
    "        non_pg = ml.getNonPGcolNames(size_fp, stacked, not geo_fp, self.cat_col_names)\n",
    "        pg = []\n",
    "        if si:\n",
    "            si_df = pd.read_csv(self.SI_grav_data_path)\n",
    "            self.all_pg = [s for s in ml.getPGcolNames(si_df, start_str=self.start_str_si, end_str=self.end_str_si)]\n",
    "            pg += [s+'_si' for s in ml.getPGcolNames(si_df, start_str=self.start_str_si, end_str=self.end_str_si) \n",
    "                   if s+'_si' in self.grav_all_features]\n",
    "            del si_df\n",
    "        if sd:\n",
    "            sd_df = pd.read_csv(self.SD_grav_data_path)\n",
    "            pg += [s for s in ml.getPGcolNames(sd_df, self.grav_start_str_sd, self.grav_end_str_sd) if s in\n",
    "                  self.grav_all_features]\n",
    "        return non_pg + pg\n",
    "    \n",
    "    def makeResult(self, i):\n",
    "        STACKED = bool(int(i[-1])) #True (=1) if stacked\n",
    "        CODE = i[:-1]\n",
    "        run_features = self.select_features(code=CODE, stacked=STACKED)\n",
    "        if STACKED:\n",
    "            print(\"Running code %s for isotherm model\" %CODE)\n",
    "            drop_features = [s for s in self.iso_all_features if s not in run_features]\n",
    "            FpDataSet(self.iso.drop(drop_features, axis=1), run_features, self.iso_prop, \n",
    "                               self.iso_target_mean, self.iso_target_std, stacked=STACKED, fp_code=CODE).run()\n",
    "        else:\n",
    "            print(\"Running code %s for gravimetric uptake model\" %CODE)\n",
    "            drop_features = [s for s in self.grav_all_features if s not in run_features]\n",
    "            FpDataSet(self.grav.drop(drop_features, axis=1), run_features, self.grav_prop, \n",
    "                               self.grav_target_mean, self.grav_target_std, stacked=STACKED, fp_code=CODE)        \n",
    "    \n",
    "    def makeAllResults(self):\n",
    "        self.makeMasterDFs()\n",
    "        print('\\n')\n",
    "        #Parallel(n_jobs=self.n_core)(delayed(self.makeResult)(j) for j in self.feature_codes)\n",
    "        for i in self.feature_codes: #True if stacked\n",
    "                STACKED = bool(int(i[-1])) #True (=1) if stacked\n",
    "                CODE = i[:-1]\n",
    "                run_features = self.select_features(code=CODE, stacked=STACKED)\n",
    "                try:\n",
    "                    TRAIN_GRID, SEEDS = self.job_dict[CODE]\n",
    "                except:\n",
    "                    TRAIN_GRID = [.5, .6, .7, .8, .9]\n",
    "                    SEEDS = [0, 10, 20]\n",
    "                if STACKED:\n",
    "                    print(\"Running code %s for isotherm model\" %CODE)\n",
    "                    drop_features = [s for s in self.iso_all_features if s not in run_features]\n",
    "                    #l.append(self.iso.drop(drop_features, axis=1))\n",
    "                    FpDataSet(self.iso.drop(drop_features, axis=1), run_features, self.iso_prop, \n",
    "                                       self.iso_target_mean, self.iso_target_std, stacked=STACKED, fp_code=CODE, n_core=\n",
    "                                       1, rand_seeds=SEEDS, train_grid=TRAIN_GRID).run()\n",
    "                else:\n",
    "                    print(\"Running code %s for gravimetric uptake model\" %CODE)\n",
    "                    drop_features = [s for s in self.grav_all_features if s not in run_features]\n",
    "                    FpDataSet(self.grav.drop(drop_features, axis=1), run_features, self.grav_prop, \n",
    "                                       self.grav_target_mean, self.grav_target_std, stacked=STACKED, fp_code=CODE,\n",
    "                                         rand_seeds=SEEDS, train_grid=TRAIN_GRID).run()\n",
    "       \n",
    "class FpDataSet:\n",
    "    def __init__(self, df, features, property_used, target_mean, target_std, stacked, fp_code, PCA_DIM=400, \n",
    "                 rand_seeds=[0, 10, 20], train_grid = [.5, .6, .7, .8, .9], n_core=15):\n",
    "        self.df = df\n",
    "        self.rand_seeds = rand_seeds\n",
    "        self.fp_code = fp_code\n",
    "        self.property_used = property_used\n",
    "        self.target_mean = target_mean\n",
    "        self.target_std = target_std\n",
    "        self.n_samples = len(self.df)\n",
    "        self.PCA_DIM = min(PCA_DIM, self.n_samples)\n",
    "        self.features = features\n",
    "        self.stacked = stacked\n",
    "        self.n_core = n_core\n",
    "        if self.stacked:\n",
    "            self.algo = 'nn'\n",
    "            self.model_tag = 'iso'\n",
    "        else:\n",
    "            self.algo = 'xgb'\n",
    "            self.model_tag = 'grav'\n",
    "        self.train_grid = train_grid\n",
    "    \n",
    "    def sortRemoteInds(self):\n",
    "        '''\n",
    "        Create list of most remote indices\n",
    "        '''\n",
    "        n_core = 1\n",
    "        use_pca = True\n",
    "        nn_tree, filenames = ml.NNtree(self.df, self.stacked, n_core, self.features, use_pca, self.PCA_DIM)\n",
    "        dt = nn_tree[0] #distance tree\n",
    "        nt = nn_tree[1] #neighbor tree\n",
    "        to_sort = list(zip(nt[:, 0],dt[:, 1]))\n",
    "\n",
    "        srt_d = {}\n",
    "        for i in to_sort:\n",
    "            try:\n",
    "                srt_d[i[0]] += i[1]\n",
    "            except:\n",
    "                srt_d[i[0]] = [i[1]]\n",
    "\n",
    "        srt_ind_set = [(k, max(v)) for k,v in zip(srt_d.keys(), srt_d.values())]\n",
    "\n",
    "        srt = sorted(srt_ind_set, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        srt_inds = list([x[0] for x in srt])\n",
    "        self.remote_info = [(x, filenames[x]) for x in srt_inds]\n",
    "        \n",
    "    \n",
    "    def helper(self, data):\n",
    "        train_pct = data[0]\n",
    "        seed = data[1]\n",
    "        TRAIN_PCT = int(round(train_pct*100))\n",
    "        results_df, MODEL = trainTestSplit(self.df, train_pct, self.features, self.property_used,\n",
    "                                                   self.target_mean, self.target_std, seed, self.remote_info,\n",
    "                                                   self.stacked).makeResults()\n",
    "        save_fragment = '%s_code_%s_train_%s_seed_%s_%s' %(self.model_tag, self.fp_code, TRAIN_PCT, seed, now)\n",
    "        print(\"Save Results using Fragment %s\" %save_fragment)\n",
    "        results_df.to_csv('results_%s.csv' %save_fragment)\n",
    "        try:\n",
    "            MODEL.save_model('%s.xgb' %save_fragment)\n",
    "        except:\n",
    "            MODEL.save('%s.h5' %save_fragment,save_format='h5')\n",
    "    def run(self):\n",
    "        self.sortRemoteInds()\n",
    "        self.results = []\n",
    "        self.parallel_data = []\n",
    "        for train_pct in self.train_grid:\n",
    "            for seed in self.rand_seeds:\n",
    "                self.parallel_data.append((train_pct, seed))\n",
    "        Parallel(n_jobs=self.n_core)(delayed(self.helper)(j) for j in self.parallel_data)\n",
    "#         for train_pct in self.train_grid:\n",
    "#             TRAIN_PCT = int(round(train_pct*100))\n",
    "#             for seed in self.rand_seeds:\n",
    "#                 results_df, MODEL = trainTestSplit(self.df, train_pct, self.features, self.property_used,\n",
    "#                                                   self.target_mean, self.target_std, seed, self.remote_info,\n",
    "#                                                   self.stacked).makeResults()\n",
    "#                 save_fragment = '%s_code_%s_train_%s_seed_%s_%s' %(self.model_tag, self.fp_code, TRAIN_PCT, seed, now)\n",
    "#                 print(\"Save Results using Fragment %s\" %save_fragment)\n",
    "#                 results_df.to_csv('results_%s.csv' %save_fragment)\n",
    "#                 try:\n",
    "#                     MODEL.save_model('%s.xgb' %save_fragment)\n",
    "#                 except:\n",
    "#                     MODEL.save('%s.h5' %save_fragment,save_format='h5')\n",
    "\n",
    "class trainTestSplit:\n",
    "    def __init__(self, df, train_pct, features, property_used, target_mean, target_std, seed, remote_info, stacked, \n",
    "                 hp_frac=.1, n_trees=5000):\n",
    "        self.df = df\n",
    "        self.filenames = df['filename'].unique().tolist()\n",
    "        self.n_samples = len(self.filenames)\n",
    "        self.train_pct = train_pct\n",
    "        self.seed = seed\n",
    "        self.target_mean = target_mean\n",
    "        self.target_std = target_std\n",
    "        self.remote_info = remote_info\n",
    "        self.pct_remote = self.train_pct - .5\n",
    "        self.n_remote = round(self.n_samples*self.pct_remote)\n",
    "        self.n_train = round(self.n_samples*self.train_pct)\n",
    "        self.n_random = self.n_train - self.n_remote\n",
    "        self.stacked = stacked\n",
    "        self.features = features\n",
    "        self.property_used = property_used\n",
    "        self.hp_frac = hp_frac\n",
    "        if stacked:\n",
    "            self.hp_frac = .05\n",
    "        self.n_trees = n_trees\n",
    "\n",
    "        if self.stacked:\n",
    "            self.algo = 'nn'\n",
    "            self.model_tag = 'iso'\n",
    "        else:\n",
    "            self.algo = 'xgb'\n",
    "            self.model_tag = 'grav'\n",
    "        \n",
    "    \n",
    "    def split(self):\n",
    "        '''\n",
    "        Split into train and test set\n",
    "        '''\n",
    "\n",
    "        self.train_fn = [x[1] for x in self.remote_info[:self.n_remote]]\n",
    "        \n",
    "        self.remaining = list(set(self.filenames) - set(self.train_fn))\n",
    "\n",
    "        random.Random(self.seed).shuffle(self.remaining)\n",
    "\n",
    "        self.train_fn += self.remaining[:self.n_random]\n",
    "\n",
    "        self.test_fn = self.remaining[self.n_random:]\n",
    "        train_df = self.df[self.df['filename'].isin(self.train_fn)].reset_index().drop('index', axis=1)\n",
    "        test_df = self.df[self.df['filename'].isin(self.test_fn)].reset_index().drop('index', axis=1)\n",
    "        self.train_fn_order = train_df['filename'].tolist()\n",
    "        self.test_fn_order = test_df['filename'].tolist()\n",
    "        print(\"Total len of test_df + train_df: %s\" %(len(train_df) + len(test_df)))\n",
    "        train_fp = train_df[self.features].to_numpy().astype('float32')\n",
    "        train_label = train_df[self.property_used]\n",
    "        test_fp = test_df[self.features].to_numpy().astype('float32')\n",
    "        test_label = test_df[self.property_used]\n",
    "    \n",
    "        if self.algo == 'xgb':\n",
    "            train_d = xgb.DMatrix(data=train_fp, label=train_label)\n",
    "            test_d = xgb.DMatrix(data=test_fp, label=test_label)\n",
    "            return train_d, test_d, train_label, test_label\n",
    "        if self.algo == 'nn':\n",
    "            train_files = train_df['filename'].tolist()\n",
    "            test_files = test_df['filename'].tolist()\n",
    "            train_pressures = train_df['pressure'].tolist()\n",
    "            test_pressures = test_df['pressure'].tolist()\n",
    "            return (train_fp, train_label.to_numpy(), train_files, train_pressures), (test_fp, test_label.to_numpy(), \\\n",
    "                    test_files, test_pressures), train_label, test_label\n",
    "        \n",
    "    def hp_opt(self):\n",
    "        start = time.time()\n",
    "        if self.algo == 'nn':\n",
    "            self.max_batch = 512\n",
    "            if self.max_batch > self.n_train:\n",
    "                self.max_batch = self.n_train // 2\n",
    "            self.space = [(100, 400), #n_units\n",
    "                    (.0005, .003),#learning rate\n",
    "                    (2, 15), #patience\n",
    "                    (12, self.max_batch), #batch size\n",
    "                    (.01, .6)] #validation split\n",
    "            \n",
    "        else:\n",
    "            self.max_depth_ub = 15\n",
    "            if self.n_train < 200:\n",
    "                self.max_depth_ub = 4\n",
    "            self.space = [(.3, .95), #colsample_bytree\n",
    "                            (.01, .5),#learning_rate\n",
    "                            (2, 15), #max_depth\n",
    "                            (1, 20)] #alpha\n",
    "            \n",
    "        hp_df = self.df.sample(frac=self.hp_frac, random_state=self.seed)\n",
    "        hp_remote_info = [x for x in self.remote_info if x[1] in hp_df['filename'].tolist()]\n",
    "        opt_hps = HPOpt(hp_df, self.train_pct, self.features, self.property_used, self.target_mean, \n",
    "                            self.target_std, self.seed, hp_remote_info, self.stacked, self.space).get_params()\n",
    "        end = time.time()\n",
    "        print(\"Time Elapsed during HPOpt: %s\" %(end-start) )\n",
    "        return opt_hps\n",
    "    \n",
    "    def run_model(self):\n",
    "        self.params = self.hp_opt()\n",
    "        self.train_d, self.test_d, self.train_label, self.test_label = self.split()\n",
    "        self.MODEL = ml.run_model(self.algo, self.train_d, self.n_trees, self.params)\n",
    "        \n",
    "        \n",
    "    def makeResults(self):\n",
    "        self.run_model()\n",
    "        if self.algo=='xgb':\n",
    "            test_predictions = self.MODEL.predict(self.test_d)\n",
    "            train_predictions = self.MODEL.predict(self.train_d)\n",
    "            pressures = [35]*(self.n_samples)\n",
    "            files = self.train_fn_order + self.test_fn_order\n",
    "        if self.algo=='nn':\n",
    "            test_fp = self.test_d[0]\n",
    "            train_fp = self.train_d[0]\n",
    "            files = self.train_d[2] + self.test_d[2]\n",
    "            pressures = self.train_d[3] + self.test_d[3]\n",
    "            test_predictions = self.MODEL.predict(test_fp).flatten()\n",
    "            train_predictions = self.MODEL.predict(train_fp).flatten()        \n",
    "        \n",
    "        res_test_predictions, res_test_label, res_train_label, res_train_predictions = ml.unscale(self.property_used, \n",
    "                                                                                       test_predictions, \n",
    "                                                                                       train_predictions, \n",
    "                                                                                       self.test_label, \n",
    "                                                                                       self.train_label, \n",
    "                                                                                    self.target_mean, \n",
    "                                                                                    self.target_std)\n",
    "        preds = res_train_predictions.tolist() + res_test_predictions.tolist()\n",
    "        sample_class = ['Train']*len(res_train_predictions) + ['Test']*len(res_test_predictions)\n",
    "#        truth = res_train_label.tolist() + res_test_label.tolist()\n",
    "#         results_df = pd.DataFrame({\"Filename\": files, \"Pressure\": pressures, \"Class\": sample_class,\n",
    "#                                   \"Prediction\": preds, \"Truth\": truth})\n",
    "        results_df = pd.DataFrame({\"Filename\": files, \"Pressure\": pressures, \"Class\": sample_class,\n",
    "                                  \"Prediction\": preds})\n",
    "        return results_df, self.MODEL\n",
    "\n",
    "#class(HPtrainTestSplit(trainTestSplit):\n",
    "\n",
    "class HPOpt:\n",
    "    def __init__(self, df, train_pct, features, property_used, target_mean, target_std, seed, remote_info, stacked, \n",
    "                 space, n_trees=50):\n",
    "        self.df = df\n",
    "        self.seed = seed\n",
    "        self.space = space\n",
    "        self.remote_info = remote_info\n",
    "        self.stacked = stacked\n",
    "        self.property_used = property_used\n",
    "        self.features = features\n",
    "        self.target_mean = target_mean\n",
    "        self.target_std = target_std\n",
    "        self.n_trees = n_trees\n",
    "        if stacked:\n",
    "            self.N_CALLS = 40\n",
    "        else:\n",
    "            self.N_CALLS = 75\n",
    "        \n",
    "        print(\"Using %s calls for HPOpt\" %self.N_CALLS)\n",
    "        \n",
    "        if self.stacked:\n",
    "            self.algo = 'nn'\n",
    "        else:\n",
    "            self.algo = 'xgb'\n",
    "        self.train_pct = train_pct\n",
    "    \n",
    "    def objective(self, params):\n",
    "        MODEL = ml.run_model(self.algo, self.train_d, self.n_trees, params)\n",
    "        return ml.model_rmse(MODEL, self.train_d, self.test_d, self.stacked, self.algo, self.target_mean, \n",
    "                             self.target_std, self.property_used, self.test_label, self.train_label, save=False, \n",
    "                             fname=None, subset_inds=None)\n",
    "    \n",
    "    def get_params(self):\n",
    "        HP_TTS = trainTestSplit(self.df, self.train_pct, \n",
    "                                                        self.features, self.property_used, self.target_mean,\n",
    "                                                        self.target_std, self.seed, self.remote_info, \n",
    "                                                        self.stacked)\n",
    "        self.train_d, self.test_d, self.train_label, self.test_label = HP_TTS.split()\n",
    "        defects = []\n",
    "        start = time.time()\n",
    "        r = gp_minimize(self.objective, self.space, n_calls=self.N_CALLS, random_state=self.seed)\n",
    "        end = time.time()\n",
    "        self.params = r.x\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
