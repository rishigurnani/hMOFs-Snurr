{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize\n",
    "\n",
    "import importlib\n",
    "import efrc_ml_production as ml\n",
    "importlib.reload(ml)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin hp opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following must be defined\n",
    "algo = 'nn' #am I using XGBoost (xgb) or Neural Nets (nn)?\n",
    "total_frac_hp = .05 #total fraction of data set to work with\n",
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = True #make True if the training data should be chosen randomly\n",
    "n_remote = 10000 #the n_remote most remote points will be added to training set if random_split = False\n",
    "USE_PCA = True #should I use PCA?\n",
    "N_COMPONENTS=400 #how many PCA Components should I use?\n",
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "cat_si_sd = False #make True if you want to concatenate size-indep and size-dep fps\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature$\n",
    "stacked = True #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 18 #number of cores to use\n",
    "if not stacked:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv' #path to size-dep data\n",
    "else:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/stacked.csv'\n",
    "if not stacked:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv' #path to size-indep data\n",
    "else:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/stacked.csv'\n",
    "if not stacked:\n",
    "    start_str_sd = 'CH4_v/v_248_bar'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "else:\n",
    "    start_str_sd = 'Density'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "\n",
    "start_str_si = 'filename'\n",
    "end_str_si = 'valence_pa'\n",
    "del_geometric_fp = False #make True if you want to ignore the geometric features\n",
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns\n",
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data\n",
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}\n",
    "n_trees = 50 #number of weak learners. Bigger is better until 5000\n",
    "save_pp = False #make True if you want to save the parity plot\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps before hp_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using following 420 features\n",
      "Mafp_Br1_C2_C1\n",
      "Mafp_Br1_C2_C2\n",
      "Mafp_Br1_C2_C3\n",
      "Mafp_Br1_C3_Br1\n",
      "Mafp_Br1_C3_C1\n",
      "Mafp_Br1_C3_C2\n",
      "Mafp_Br1_C3_C3\n",
      "Mafp_Br1_C3_C4\n",
      "Mafp_Br1_C3_N1\n",
      "Mafp_Br1_C3_N2\n",
      "Mafp_Br1_C3_N3\n",
      "Mafp_Br1_C3_O1\n",
      "Mafp_Br1_C4_Br1\n",
      "Mafp_Br1_C4_C2\n",
      "Mafp_Br1_C4_C3\n",
      "Mafp_Br1_C4_C4\n",
      "Mafp_Br1_C4_H1\n",
      "Mafp_Br1_C4_N1\n",
      "Mafp_Br1_C4_N2\n",
      "Mafp_Br1_C4_N3\n",
      "Mafp_Br1_C4_O1\n",
      "Mafp_Br1_C4_O2\n",
      "Mafp_Br1_N2_C2\n",
      "Mafp_Br1_N2_C3\n",
      "Mafp_Br1_N2_C4\n",
      "Mafp_Br1_N2_N1\n",
      "Mafp_Br1_N2_N2\n",
      "Mafp_Br1_N2_N3\n",
      "Mafp_Br1_N3_Br1\n",
      "Mafp_Br1_N3_C2\n",
      "Mafp_Br1_N3_C3\n",
      "Mafp_Br1_N3_H1\n",
      "Mafp_Br1_N3_N2\n",
      "Mafp_Br1_N3_O2\n",
      "Mafp_Br1_O2_C2\n",
      "Mafp_Br1_O2_C3\n",
      "Mafp_Br1_O2_C4\n",
      "Mafp_C1_C2_C2\n",
      "Mafp_C1_C2_C3\n",
      "Mafp_C1_C2_C4\n",
      "Mafp_C1_C2_F1\n",
      "Mafp_C1_C2_H1\n",
      "Mafp_C1_C2_O1\n",
      "Mafp_C1_C2_O2\n",
      "Mafp_C1_C3_C2\n",
      "Mafp_C1_C3_C3\n",
      "Mafp_C1_C3_C4\n",
      "Mafp_C1_C3_Cl1\n",
      "Mafp_C1_C3_F1\n",
      "Mafp_C1_C3_H1\n",
      "Mafp_C1_C3_N2\n",
      "Mafp_C1_C3_N3\n",
      "Mafp_C1_C3_O1\n",
      "Mafp_C1_C3_O2\n",
      "Mafp_C1_C4_C2\n",
      "Mafp_C1_C4_C3\n",
      "Mafp_C1_C4_C4\n",
      "Mafp_C1_C4_H1\n",
      "Mafp_C1_C4_O1\n",
      "Mafp_C1_C4_O2\n",
      "Mafp_C1_N2_C2\n",
      "Mafp_C1_N2_C3\n",
      "Mafp_C1_N2_N2\n",
      "Mafp_C1_N2_N3\n",
      "Mafp_C1_N3_C3\n",
      "Mafp_C1_N3_C4\n",
      "Mafp_C1_N3_N2\n",
      "Mafp_C1_N3_N3\n",
      "Mafp_C1_N3_O1\n",
      "Mafp_C1_O2_C3\n",
      "Mafp_C1_O2_C4\n",
      "Mafp_C2_C2_C2\n",
      "Mafp_C2_C2_C3\n",
      "Mafp_C2_C2_C4\n",
      "Mafp_C2_C2_Cl1\n",
      "Mafp_C2_C2_F1\n",
      "Mafp_C2_C2_H1\n",
      "Mafp_C2_C2_N1\n",
      "Mafp_C2_C2_N2\n",
      "Mafp_C2_C2_N3\n",
      "Mafp_C2_C2_O1\n",
      "Mafp_C2_C2_O2\n",
      "Mafp_C2_C3_C2\n",
      "Mafp_C2_C3_C3\n",
      "Mafp_C2_C3_C4\n",
      "Mafp_C2_C3_Cl1\n",
      "Mafp_C2_C3_F1\n",
      "Mafp_C2_C3_H1\n",
      "Mafp_C2_C3_N1\n",
      "Mafp_C2_C3_N2\n",
      "Mafp_C2_C3_N3\n",
      "Mafp_C2_C3_O1\n",
      "Mafp_C2_C3_O2\n",
      "Mafp_C2_C4_C2\n",
      "Mafp_C2_C4_C3\n",
      "Mafp_C2_C4_C4\n",
      "Mafp_C2_C4_Cl1\n",
      "Mafp_C2_C4_F1\n",
      "Mafp_C2_C4_H1\n",
      "Mafp_C2_C4_N2\n",
      "Mafp_C2_C4_N3\n",
      "Mafp_C2_C4_O1\n",
      "Mafp_C2_C4_O2\n",
      "Mafp_C2_N2_C2\n",
      "Mafp_C2_N2_C3\n",
      "Mafp_C2_N2_C4\n",
      "Mafp_C2_N2_Cl1\n",
      "Mafp_C2_N2_F1\n",
      "Mafp_C2_N2_H1\n",
      "Mafp_C2_N2_N1\n",
      "Mafp_C2_N2_N2\n",
      "Mafp_C2_N2_N3\n",
      "Mafp_C2_N2_O1\n",
      "Mafp_C2_N2_O2\n",
      "Mafp_C2_N3_C2\n",
      "Mafp_C2_N3_C3\n",
      "Mafp_C2_N3_C4\n",
      "Mafp_C2_N3_F1\n",
      "Mafp_C2_N3_H1\n",
      "Mafp_C2_N3_N2\n",
      "Mafp_C2_N3_N3\n",
      "Mafp_C2_N3_O1\n",
      "Mafp_C2_N3_O2\n",
      "Mafp_C2_O2_C2\n",
      "Mafp_C2_O2_C3\n",
      "Mafp_C2_O2_C4\n",
      "Mafp_C2_O2_Cl1\n",
      "Mafp_C2_O2_H1\n",
      "Mafp_C2_O2_N2\n",
      "Mafp_C2_O2_N3\n",
      "Mafp_C3_C2_C3\n",
      "Mafp_C3_C2_C4\n",
      "Mafp_C3_C2_Cl1\n",
      "Mafp_C3_C2_F1\n",
      "Mafp_C3_C2_H1\n",
      "Mafp_C3_C2_N1\n",
      "Mafp_C3_C2_N2\n",
      "Mafp_C3_C2_N3\n",
      "Mafp_C3_C2_O1\n",
      "Mafp_C3_C2_O2\n",
      "Mafp_C3_C3_C3\n",
      "Mafp_C3_C3_C4\n",
      "Mafp_C3_C3_Cl1\n",
      "Mafp_C3_C3_F1\n",
      "Mafp_C3_C3_H1\n",
      "Mafp_C3_C3_N1\n",
      "Mafp_C3_C3_N2\n",
      "Mafp_C3_C3_N3\n",
      "Mafp_C3_C3_O1\n",
      "Mafp_C3_C3_O2\n",
      "Mafp_C3_C4_C3\n",
      "Mafp_C3_C4_C4\n",
      "Mafp_C3_C4_Cl1\n",
      "Mafp_C3_C4_F1\n",
      "Mafp_C3_C4_H1\n",
      "Mafp_C3_C4_N1\n",
      "Mafp_C3_C4_N2\n",
      "Mafp_C3_C4_N3\n",
      "Mafp_C3_C4_O1\n",
      "Mafp_C3_C4_O2\n",
      "Mafp_C3_N2_C3\n",
      "Mafp_C3_N2_C4\n",
      "Mafp_C3_N2_Cl1\n",
      "Mafp_C3_N2_F1\n",
      "Mafp_C3_N2_H1\n",
      "Mafp_C3_N2_N1\n",
      "Mafp_C3_N2_N2\n",
      "Mafp_C3_N2_N3\n",
      "Mafp_C3_N2_O1\n",
      "Mafp_C3_N2_O2\n",
      "Mafp_C3_N3_C3\n",
      "Mafp_C3_N3_C4\n",
      "Mafp_C3_N3_Cl1\n",
      "Mafp_C3_N3_F1\n",
      "Mafp_C3_N3_H1\n",
      "Mafp_C3_N3_N1\n",
      "Mafp_C3_N3_N2\n",
      "Mafp_C3_N3_N3\n",
      "Mafp_C3_N3_O2\n",
      "Mafp_C3_O2_C3\n",
      "Mafp_C3_O2_C4\n",
      "Mafp_C3_O2_Cl1\n",
      "Mafp_C3_O2_F1\n",
      "Mafp_C3_O2_H1\n",
      "Mafp_C3_O2_N2\n",
      "Mafp_C3_O2_N3\n",
      "Mafp_C3_O2_O1\n",
      "Mafp_C3_O2_O2\n",
      "Mafp_C4_C2_C4\n",
      "Mafp_C4_C2_H1\n",
      "Mafp_C4_C2_N1\n",
      "Mafp_C4_C2_N2\n",
      "Mafp_C4_C2_N3\n",
      "Mafp_C4_C2_O1\n",
      "Mafp_C4_C2_O2\n",
      "Mafp_C4_C3_C4\n",
      "Mafp_C4_C3_Cl1\n",
      "Mafp_C4_C3_F1\n",
      "Mafp_C4_C3_H1\n",
      "Mafp_C4_C3_N1\n",
      "Mafp_C4_C3_N2\n",
      "Mafp_C4_C3_N3\n",
      "Mafp_C4_C3_O1\n",
      "Mafp_C4_C3_O2\n",
      "Mafp_C4_C4_C4\n",
      "Mafp_C4_C4_Cl1\n",
      "Mafp_C4_C4_F1\n",
      "Mafp_C4_C4_H1\n",
      "Mafp_C4_C4_N1\n",
      "Mafp_C4_C4_N2\n",
      "Mafp_C4_C4_N3\n",
      "Mafp_C4_C4_O1\n",
      "Mafp_C4_C4_O2\n",
      "Mafp_C4_N2_C4\n",
      "Mafp_C4_N2_H1\n",
      "Mafp_C4_N2_N1\n",
      "Mafp_C4_N2_N2\n",
      "Mafp_C4_N2_N3\n",
      "Mafp_C4_N2_O1\n",
      "Mafp_C4_N3_C4\n",
      "Mafp_C4_N3_F1\n",
      "Mafp_C4_N3_H1\n",
      "Mafp_C4_N3_N1\n",
      "Mafp_C4_N3_N2\n",
      "Mafp_C4_N3_N3\n",
      "Mafp_C4_N3_O1\n",
      "Mafp_C4_N3_O2\n",
      "Mafp_C4_O2_C4\n",
      "Mafp_C4_O2_Cl1\n",
      "Mafp_C4_O2_F1\n",
      "Mafp_C4_O2_H1\n",
      "Mafp_C4_O2_N2\n",
      "Mafp_C4_O2_N3\n",
      "Mafp_C4_O2_O1\n",
      "Mafp_C4_O2_O2\n",
      "Mafp_Cl1_C3_Cl1\n",
      "Mafp_Cl1_C3_H1\n",
      "Mafp_Cl1_C3_N1\n",
      "Mafp_Cl1_C3_N2\n",
      "Mafp_Cl1_C3_N3\n",
      "Mafp_Cl1_C3_O1\n",
      "Mafp_Cl1_C4_Cl1\n",
      "Mafp_Cl1_C4_H1\n",
      "Mafp_Cl1_C4_N2\n",
      "Mafp_Cl1_C4_N3\n",
      "Mafp_Cl1_C4_O1\n",
      "Mafp_Cl1_C4_O2\n",
      "Mafp_Cl1_N2_N1\n",
      "Mafp_Cl1_N2_N2\n",
      "Mafp_Cl1_N3_Cl1\n",
      "Mafp_Cl1_N3_H1\n",
      "Mafp_Cl1_N3_N2\n",
      "Mafp_F1_C3_F1\n",
      "Mafp_F1_C3_N1\n",
      "Mafp_F1_C3_N2\n",
      "Mafp_F1_C3_N3\n",
      "Mafp_F1_C3_O1\n",
      "Mafp_F1_C3_O2\n",
      "Mafp_F1_C4_F1\n",
      "Mafp_F1_C4_H1\n",
      "Mafp_F1_C4_N2\n",
      "Mafp_F1_C4_N3\n",
      "Mafp_F1_C4_O1\n",
      "Mafp_F1_C4_O2\n",
      "Mafp_F1_N2_N2\n",
      "Mafp_F1_N3_F1\n",
      "Mafp_F1_N3_H1\n",
      "Mafp_F1_N3_N1\n",
      "Mafp_F1_N3_N2\n",
      "Mafp_F1_N3_N3\n",
      "Mafp_H1_C2_H1\n",
      "Mafp_H1_C2_N1\n",
      "Mafp_H1_C2_N2\n",
      "Mafp_H1_C2_N3\n",
      "Mafp_H1_C2_O1\n",
      "Mafp_H1_C2_O2\n",
      "Mafp_H1_C3_H1\n",
      "Mafp_H1_C3_N1\n",
      "Mafp_H1_C3_N2\n",
      "Mafp_H1_C3_N3\n",
      "Mafp_H1_C3_O1\n",
      "Mafp_H1_C3_O2\n",
      "Mafp_H1_C4_H1\n",
      "Mafp_H1_C4_N2\n",
      "Mafp_H1_C4_N3\n",
      "Mafp_H1_C4_O1\n",
      "Mafp_H1_C4_O2\n",
      "Mafp_H1_N2_H1\n",
      "Mafp_H1_N2_N1\n",
      "Mafp_H1_N2_N2\n",
      "Mafp_H1_N2_N3\n",
      "Mafp_H1_N3_H1\n",
      "Mafp_H1_N3_N1\n",
      "Mafp_H1_N3_N2\n",
      "Mafp_H1_N3_N3\n",
      "Mafp_H1_N3_O2\n",
      "Mafp_H1_O2_H1\n",
      "Mafp_H1_O2_N2\n",
      "Mafp_H1_O2_O1\n",
      "Mafp_H1_O2_O2\n",
      "Mafp_N1_C2_N2\n",
      "Mafp_N1_C2_N3\n",
      "Mafp_N1_C2_O2\n",
      "Mafp_N1_C3_N2\n",
      "Mafp_N1_C3_N3\n",
      "Mafp_N1_C3_O1\n",
      "Mafp_N1_C3_O2\n",
      "Mafp_N1_C4_N2\n",
      "Mafp_N1_C4_N3\n",
      "Mafp_N1_C4_O2\n",
      "Mafp_N1_N2_N3\n",
      "Mafp_N1_N3_N2\n",
      "Mafp_N1_N3_N3\n",
      "Mafp_N2_C2_N2\n",
      "Mafp_N2_C2_N3\n",
      "Mafp_N2_C2_O2\n",
      "Mafp_N2_C3_N2\n",
      "Mafp_N2_C3_N3\n",
      "Mafp_N2_C3_O1\n",
      "Mafp_N2_C3_O2\n",
      "Mafp_N2_C4_N2\n",
      "Mafp_N2_C4_N3\n",
      "Mafp_N2_C4_O1\n",
      "Mafp_N2_C4_O2\n",
      "Mafp_N2_N2_N2\n",
      "Mafp_N2_N2_N3\n",
      "Mafp_N2_N2_O2\n",
      "Mafp_N2_N3_N2\n",
      "Mafp_N2_N3_N3\n",
      "Mafp_N2_N3_O2\n",
      "Mafp_N2_O2_N3\n",
      "Mafp_N3_C2_O1\n",
      "Mafp_N3_C3_N3\n",
      "Mafp_N3_C3_O1\n",
      "Mafp_N3_C3_O2\n",
      "Mafp_N3_C4_N3\n",
      "Mafp_N3_C4_O1\n",
      "Mafp_N3_C4_O2\n",
      "Mafp_N3_N2_N3\n",
      "Mafp_N3_N2_O1\n",
      "Mafp_N3_N2_O2\n",
      "Mafp_N3_N3_N3\n",
      "Mafp_N3_N3_O1\n",
      "Mafp_N3_N3_O2\n",
      "Mafp_N3_O2_N3\n",
      "Mafp_O1_C2_O1\n",
      "Mafp_O1_C2_O2\n",
      "Mafp_O1_C3_O1\n",
      "Mafp_O1_C3_O2\n",
      "Mafp_O1_C4_O1\n",
      "Mafp_O1_C4_O2\n",
      "Mafp_O1_N3_O2\n",
      "Mafp_O2_C2_O2\n",
      "Mafp_O2_C3_O2\n",
      "Mafp_O2_C4_O2\n",
      "Mafp_O2_N3_O2\n",
      "Mefp_fam_acrylate\n",
      "Mefp_fam_carbonateester\n",
      "Mefp_fam_ketone\n",
      "Mefp_fam_polyamides\n",
      "Mefp_fam_single\n",
      "Mefp_norm_mol_wt\n",
      "Mefp_numatoms_none_H\n",
      "Mefp_ring\n",
      "Mmfp_Chi0n\n",
      "Mmfp_Chi0v\n",
      "Mmfp_Chi1n\n",
      "Mmfp_Chi1v\n",
      "Mmfp_Chi2n\n",
      "Mmfp_Chi2v\n",
      "Mmfp_HallKierAlpha\n",
      "Mmfp_MQNs13\n",
      "Mmfp_MQNs14\n",
      "Mmfp_MQNs15\n",
      "Mmfp_MQNs16\n",
      "Mmfp_MQNs17\n",
      "Mmfp_MQNs18\n",
      "Mmfp_MQNs19\n",
      "Mmfp_MQNs20\n",
      "Mmfp_MQNs21\n",
      "Mmfp_MQNs22\n",
      "Mmfp_MQNs23\n",
      "Mmfp_MQNs24\n",
      "Mmfp_MQNs25\n",
      "Mmfp_MQNs26\n",
      "Mmfp_MQNs27\n",
      "Mmfp_MQNs28\n",
      "Mmfp_MQNs29\n",
      "Mmfp_MQNs30\n",
      "Mmfp_MQNs31\n",
      "Mmfp_MQNs32\n",
      "Mmfp_MQNs33\n",
      "Mmfp_MQNs34\n",
      "Mmfp_MQNs35\n",
      "Mmfp_MQNs36\n",
      "Mmfp_MQNs37\n",
      "Mmfp_MQNs38\n",
      "Mmfp_MQNs39\n",
      "Mmfp_MQNs40\n",
      "Mmfp_MQNs41\n",
      "Mmfp_MQNs42\n",
      "Mmfp_NumAliphaticRings\n",
      "Mmfp_NumAromaticRings\n",
      "Mmfp_tpsa\n",
      "norm_valence_pa\n",
      "norm_atomic_rad_pa_(angstroms)\n",
      "norm_affinity_pa_(eV)\n",
      "norm_ionization_potential_pa_(eV)\n",
      "norm_electronegativity_pa\n",
      "norm_Dom._Pore_(ang.)\n",
      "norm_Max._Pore_(ang.)\n",
      "norm_Void_Fraction\n",
      "norm_Surf._Area_(m2/g)\n",
      "norm_Vol._Surf._Area\n",
      "norm_Density\n",
      "oh_1\n",
      "oh_2\n",
      "oh_3\n",
      "oh_4\n",
      "norm_log_pressure\n",
      "The following columns have been dropped: []\n"
     ]
    }
   ],
   "source": [
    "if not stacked:\n",
    "    ml_data_hp, property_used, target_mean, target_std, features = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac_hp, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "if stacked:\n",
    "    ml_data_hp, property_used, target_mean, target_std, features, p_info = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac_hp, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total len of test_df + train_df: 26672\n"
     ]
    }
   ],
   "source": [
    "train_df_hp, test_df_hp= ml.trainTestSplit(ml_data_hp, property_used, training_pct, stacked, \n",
    "                                     n_core, random_split, n_remote, features, USE_PCA, N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d_hp, test_d_hp, train_label_hp, test_label_hp = ml.alter_dtype(train_df_hp, test_df_hp, \n",
    "                                                                      property_used, n_core, algo, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_hp) + len(test_label_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    print(\"Size of training set %s\" %len(train_label_hp))\n",
    "    MODEL = ml.run_model(algo, train_d_hp, n_trees, params)\n",
    "    return ml.model_rmse(MODEL, train_d_hp, test_d_hp, stacked, algo, target_mean, target_std, property_used, \n",
    "                         test_label_hp, train_label_hp, save=False, fname=None, subset_inds=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with two hyperparameters in the model:<br>\n",
    "<br>\n",
    "1)Number of units in the first dense layer<br>\n",
    "2)Learning rate<br>\n",
    "3)Patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [(100, 400), #n_units\n",
    "        (.001, .002),#learning rate\n",
    "        (2, 15), #patience\n",
    "        (4, 128), #batch size\n",
    "        (.01, .6)] #validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set 18691\n",
      "Train on 9254 samples, validate on 9437 samples\n",
      "Epoch 1/1000\n",
      "9180/9254 [============================>.] - ETA: 0s - loss: 0.0516 - mae: 0.1606 - mse: 0.0516\n",
      "Epoch 00001: val_loss improved from inf to 0.02879, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0514,  mae:0.1603,  mse:0.0514,  val_loss:0.0288,  val_mae:0.1336,  val_mse:0.0288,  \n",
      "9254/9254 [==============================] - 2s 197us/sample - loss: 0.0514 - mae: 0.1603 - mse: 0.0514 - val_loss: 0.0288 - val_mae: 0.1336 - val_mse: 0.0288\n",
      "Epoch 2/1000\n",
      "9146/9254 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.1120 - mse: 0.0230\n",
      "Epoch 00002: val_loss improved from 0.02879 to 0.02575, saving model to model_checkpoint.h5\n",
      "9254/9254 [==============================] - 1s 122us/sample - loss: 0.0230 - mae: 0.1120 - mse: 0.0230 - val_loss: 0.0257 - val_mae: 0.1236 - val_mse: 0.0257\n",
      "Epoch 3/1000\n",
      "8942/9254 [===========================>..] - ETA: 0s - loss: 0.0200 - mae: 0.1034 - mse: 0.0200\n",
      "Epoch 00003: val_loss improved from 0.02575 to 0.01952, saving model to model_checkpoint.h5\n",
      "9254/9254 [==============================] - 1s 118us/sample - loss: 0.0199 - mae: 0.1033 - mse: 0.0199 - val_loss: 0.0195 - val_mae: 0.1002 - val_mse: 0.0195\n",
      "Epoch 4/1000\n",
      "8772/9254 [===========================>..] - ETA: 0s - loss: 0.0180 - mae: 0.0995 - mse: 0.0180\n",
      "Epoch 00004: val_loss did not improve from 0.01952\n",
      "9254/9254 [==============================] - 1s 119us/sample - loss: 0.0182 - mae: 0.0998 - mse: 0.0182 - val_loss: 0.0323 - val_mae: 0.1378 - val_mse: 0.0323\n",
      "Epoch 5/1000\n",
      "8806/9254 [===========================>..] - ETA: 0s - loss: 0.0205 - mae: 0.1060 - mse: 0.0205\n",
      "Epoch 00005: val_loss improved from 0.01952 to 0.01870, saving model to model_checkpoint.h5\n",
      "9254/9254 [==============================] - 1s 122us/sample - loss: 0.0203 - mae: 0.1053 - mse: 0.0203 - val_loss: 0.0187 - val_mae: 0.0959 - val_mse: 0.0187\n",
      "Epoch 6/1000\n",
      "8568/9254 [==========================>...] - ETA: 0s - loss: 0.0170 - mae: 0.0961 - mse: 0.0170\n",
      "Epoch 00006: val_loss did not improve from 0.01870\n",
      "9254/9254 [==============================] - 1s 118us/sample - loss: 0.0168 - mae: 0.0954 - mse: 0.0168 - val_loss: 0.0203 - val_mae: 0.0982 - val_mse: 0.0203\n",
      "Epoch 7/1000\n",
      "9180/9254 [============================>.] - ETA: 0s - loss: 0.0150 - mae: 0.0893 - mse: 0.0150\n",
      "Epoch 00007: val_loss improved from 0.01870 to 0.01744, saving model to model_checkpoint.h5\n",
      "9254/9254 [==============================] - 1s 116us/sample - loss: 0.0150 - mae: 0.0893 - mse: 0.0150 - val_loss: 0.0174 - val_mae: 0.0917 - val_mse: 0.0174\n",
      "Epoch 8/1000\n",
      "8704/9254 [===========================>..] - ETA: 0s - loss: 0.0147 - mae: 0.0885 - mse: 0.0147\n",
      "Epoch 00008: val_loss did not improve from 0.01744\n",
      "9254/9254 [==============================] - 1s 119us/sample - loss: 0.0147 - mae: 0.0886 - mse: 0.0147 - val_loss: 0.0245 - val_mae: 0.1163 - val_mse: 0.0245\n",
      "Epoch 9/1000\n",
      "8568/9254 [==========================>...] - ETA: 0s - loss: 0.0153 - mae: 0.0909 - mse: 0.0153\n",
      "Epoch 00009: val_loss did not improve from 0.01744\n",
      "9254/9254 [==============================] - 1s 112us/sample - loss: 0.0154 - mae: 0.0912 - mse: 0.0154 - val_loss: 0.0234 - val_mae: 0.1110 - val_mse: 0.0234\n",
      "Epoch 10/1000\n",
      "8568/9254 [==========================>...] - ETA: 0s - loss: 0.0151 - mae: 0.0920 - mse: 0.0151\n",
      "Epoch 00010: val_loss did not improve from 0.01744\n",
      "9254/9254 [==============================] - 1s 113us/sample - loss: 0.0154 - mae: 0.0925 - mse: 0.0154 - val_loss: 0.0203 - val_mae: 0.0998 - val_mse: 0.0203\n",
      "Epoch 11/1000\n",
      "9044/9254 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0840 - mse: 0.0133\n",
      "Epoch 00011: val_loss did not improve from 0.01744\n",
      "9254/9254 [==============================] - 1s 114us/sample - loss: 0.0133 - mae: 0.0839 - mse: 0.0133 - val_loss: 0.0188 - val_mae: 0.0946 - val_mse: 0.0188\n",
      "Elapsed time during model training:  12.99892783164978\n",
      "Size of training set 18691\n",
      "Train on 13901 samples, validate on 4790 samples\n",
      "Epoch 1/1000\n",
      "13442/13901 [============================>.] - ETA: 0s - loss: 0.0426 - mae: 0.1465 - mse: 0.0426\n",
      "Epoch 00001: val_loss improved from inf to 0.02432, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0421,  mae:0.1458,  mse:0.0421,  val_loss:0.0243,  val_mae:0.1161,  val_mse:0.0243,  \n",
      "13901/13901 [==============================] - 2s 140us/sample - loss: 0.0421 - mae: 0.1458 - mse: 0.0421 - val_loss: 0.0243 - val_mae: 0.1161 - val_mse: 0.0243\n",
      "Epoch 2/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.1146 - mse: 0.0239\n",
      "Epoch 00002: val_loss improved from 0.02432 to 0.02035, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 142us/sample - loss: 0.0239 - mae: 0.1146 - mse: 0.0239 - val_loss: 0.0203 - val_mae: 0.1050 - val_mse: 0.0203\n",
      "Epoch 3/1000\n",
      "13754/13901 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1097 - mse: 0.0222\n",
      "Epoch 00003: val_loss improved from 0.02035 to 0.01984, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 151us/sample - loss: 0.0222 - mae: 0.1098 - mse: 0.0222 - val_loss: 0.0198 - val_mae: 0.1030 - val_mse: 0.0198\n",
      "Epoch 4/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.1013 - mse: 0.0193\n",
      "Epoch 00004: val_loss did not improve from 0.01984\n",
      "13901/13901 [==============================] - 2s 139us/sample - loss: 0.0193 - mae: 0.1013 - mse: 0.0193 - val_loss: 0.0377 - val_mae: 0.1617 - val_mse: 0.0377\n",
      "Epoch 5/1000\n",
      "13728/13901 [============================>.] - ETA: 0s - loss: 0.0190 - mae: 0.1009 - mse: 0.0190\n",
      "Epoch 00005: val_loss improved from 0.01984 to 0.01712, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 140us/sample - loss: 0.0189 - mae: 0.1007 - mse: 0.0189 - val_loss: 0.0171 - val_mae: 0.0925 - val_mse: 0.0171\n",
      "Epoch 6/1000\n",
      "13624/13901 [============================>.] - ETA: 0s - loss: 0.0175 - mae: 0.0961 - mse: 0.0175\n",
      "Epoch 00006: val_loss did not improve from 0.01712\n",
      "13901/13901 [==============================] - 2s 115us/sample - loss: 0.0175 - mae: 0.0961 - mse: 0.0175 - val_loss: 0.0174 - val_mae: 0.0954 - val_mse: 0.0174\n",
      "Epoch 7/1000\n",
      "13832/13901 [============================>.] - ETA: 0s - loss: 0.0173 - mae: 0.0960 - mse: 0.0173\n",
      "Epoch 00007: val_loss did not improve from 0.01712\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0173 - mae: 0.0961 - mse: 0.0173 - val_loss: 0.0176 - val_mae: 0.0962 - val_mse: 0.0176\n",
      "Epoch 8/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0166 - mae: 0.0938 - mse: 0.0166\n",
      "Epoch 00008: val_loss improved from 0.01712 to 0.01630, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0166 - mae: 0.0938 - mse: 0.0166 - val_loss: 0.0163 - val_mae: 0.0913 - val_mse: 0.0163\n",
      "Epoch 9/1000\n",
      "13390/13901 [===========================>..] - ETA: 0s - loss: 0.0155 - mae: 0.0890 - mse: 0.0155\n",
      "Epoch 00009: val_loss improved from 0.01630 to 0.01589, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0154 - mae: 0.0888 - mse: 0.0154 - val_loss: 0.0159 - val_mae: 0.0870 - val_mse: 0.0159\n",
      "Epoch 10/1000\n",
      "13572/13901 [============================>.] - ETA: 0s - loss: 0.0146 - mae: 0.0872 - mse: 0.0146\n",
      "Epoch 00010: val_loss improved from 0.01589 to 0.01486, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0146 - mae: 0.0872 - mse: 0.0146 - val_loss: 0.0149 - val_mae: 0.0827 - val_mse: 0.0149\n",
      "Epoch 11/1000\n",
      "13780/13901 [============================>.] - ETA: 0s - loss: 0.0152 - mae: 0.0896 - mse: 0.0152\n",
      "Epoch 00011: val_loss did not improve from 0.01486\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0152 - mae: 0.0895 - mse: 0.0152 - val_loss: 0.0161 - val_mae: 0.0873 - val_mse: 0.0161\n",
      "Epoch 12/1000\n",
      "13572/13901 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0847 - mse: 0.0140\n",
      "Epoch 00012: val_loss did not improve from 0.01486\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0139 - mae: 0.0847 - mse: 0.0139 - val_loss: 0.0222 - val_mae: 0.1081 - val_mse: 0.0222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000\n",
      "13650/13901 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0837 - mse: 0.0137\n",
      "Epoch 00013: val_loss improved from 0.01486 to 0.01445, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 157us/sample - loss: 0.0136 - mae: 0.0836 - mse: 0.0136 - val_loss: 0.0145 - val_mae: 0.0836 - val_mse: 0.0145\n",
      "Epoch 14/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0832 - mse: 0.0133\n",
      "Epoch 00014: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 153us/sample - loss: 0.0133 - mae: 0.0832 - mse: 0.0133 - val_loss: 0.0165 - val_mae: 0.0896 - val_mse: 0.0165\n",
      "Epoch 15/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0131 - mae: 0.0814 - mse: 0.0131\n",
      "Epoch 00015: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 152us/sample - loss: 0.0130 - mae: 0.0813 - mse: 0.0130 - val_loss: 0.0146 - val_mae: 0.0845 - val_mse: 0.0146\n",
      "Epoch 16/1000\n",
      "13676/13901 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0815 - mse: 0.0129\n",
      "Epoch 00016: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 154us/sample - loss: 0.0131 - mae: 0.0822 - mse: 0.0131 - val_loss: 0.0174 - val_mae: 0.0968 - val_mse: 0.0174\n",
      "Epoch 17/1000\n",
      "13572/13901 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0796 - mse: 0.0123\n",
      "Epoch 00017: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 143us/sample - loss: 0.0123 - mae: 0.0796 - mse: 0.0123 - val_loss: 0.0147 - val_mae: 0.0817 - val_mse: 0.0147\n",
      "Epoch 18/1000\n",
      "13806/13901 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0765 - mse: 0.0117\n",
      "Epoch 00018: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 122us/sample - loss: 0.0117 - mae: 0.0765 - mse: 0.0117 - val_loss: 0.0148 - val_mae: 0.0856 - val_mse: 0.0148\n",
      "Epoch 19/1000\n",
      "13780/13901 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0762 - mse: 0.0112\n",
      "Epoch 00019: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0112 - mae: 0.0761 - mse: 0.0112 - val_loss: 0.0145 - val_mae: 0.0805 - val_mse: 0.0145\n",
      "Epoch 20/1000\n",
      "13598/13901 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0756 - mse: 0.0114\n",
      "Epoch 00020: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 1s 108us/sample - loss: 0.0114 - mae: 0.0755 - mse: 0.0114 - val_loss: 0.0154 - val_mae: 0.0860 - val_mse: 0.0154\n",
      "Epoch 21/1000\n",
      "13702/13901 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0743 - mse: 0.0108\n",
      "Epoch 00021: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 1s 108us/sample - loss: 0.0109 - mae: 0.0745 - mse: 0.0109 - val_loss: 0.0151 - val_mae: 0.0881 - val_mse: 0.0151\n",
      "Epoch 22/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0727 - mse: 0.0104\n",
      "Epoch 00022: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0104 - mae: 0.0726 - mse: 0.0104 - val_loss: 0.0147 - val_mae: 0.0851 - val_mse: 0.0147\n",
      "Epoch 23/1000\n",
      "13676/13901 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0727 - mse: 0.0101\n",
      "Epoch 00023: val_loss did not improve from 0.01445\n",
      "13901/13901 [==============================] - 1s 108us/sample - loss: 0.0105 - mae: 0.0729 - mse: 0.0105 - val_loss: 0.0171 - val_mae: 0.0967 - val_mse: 0.0171\n",
      "Epoch 24/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0709 - mse: 0.0100\n",
      "Epoch 00024: val_loss improved from 0.01445 to 0.01439, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 111us/sample - loss: 0.0100 - mae: 0.0709 - mse: 0.0100 - val_loss: 0.0144 - val_mae: 0.0824 - val_mse: 0.0144\n",
      "Epoch 25/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0715 - mse: 0.0101\n",
      "Epoch 00025: val_loss improved from 0.01439 to 0.01368, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 156us/sample - loss: 0.0101 - mae: 0.0716 - mse: 0.0101 - val_loss: 0.0137 - val_mae: 0.0801 - val_mse: 0.0137\n",
      "Epoch 26/1000\n",
      "13754/13901 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0696 - mse: 0.0095\n",
      "Epoch 00026: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 2s 151us/sample - loss: 0.0095 - mae: 0.0697 - mse: 0.0095 - val_loss: 0.0164 - val_mae: 0.0859 - val_mse: 0.0164\n",
      "Epoch 27/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0689 - mse: 0.0094\n",
      "Epoch 00027: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 2s 144us/sample - loss: 0.0095 - mae: 0.0690 - mse: 0.0095 - val_loss: 0.0151 - val_mae: 0.0843 - val_mse: 0.0151\n",
      "Epoch 28/1000\n",
      "13702/13901 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0691 - mse: 0.0093\n",
      "Epoch 00028: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 2s 125us/sample - loss: 0.0093 - mae: 0.0691 - mse: 0.0093 - val_loss: 0.0140 - val_mae: 0.0798 - val_mse: 0.0140\n",
      "Epoch 29/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0688 - mse: 0.0093\n",
      "Epoch 00029: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 2s 114us/sample - loss: 0.0093 - mae: 0.0688 - mse: 0.0093 - val_loss: 0.0163 - val_mae: 0.0969 - val_mse: 0.0163\n",
      "Epoch 30/1000\n",
      "13754/13901 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0653 - mse: 0.0085\n",
      "Epoch 00030: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 1s 108us/sample - loss: 0.0085 - mae: 0.0653 - mse: 0.0085 - val_loss: 0.0154 - val_mae: 0.0862 - val_mse: 0.0154\n",
      "Epoch 31/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0668 - mse: 0.0089\n",
      "Epoch 00031: val_loss did not improve from 0.01368\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0089 - mae: 0.0667 - mse: 0.0089 - val_loss: 0.0144 - val_mae: 0.0838 - val_mse: 0.0144\n",
      "Epoch 32/1000\n",
      "13442/13901 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0641 - mse: 0.0083\n",
      "Epoch 00032: val_loss improved from 0.01368 to 0.01366, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 111us/sample - loss: 0.0082 - mae: 0.0641 - mse: 0.0082 - val_loss: 0.0137 - val_mae: 0.0773 - val_mse: 0.0137\n",
      "Epoch 33/1000\n",
      "13546/13901 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0654 - mse: 0.0085\n",
      "Epoch 00033: val_loss did not improve from 0.01366\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0084 - mae: 0.0655 - mse: 0.0084 - val_loss: 0.0144 - val_mae: 0.0849 - val_mse: 0.0144\n",
      "Epoch 34/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0644 - mse: 0.0083\n",
      "Epoch 00034: val_loss did not improve from 0.01366\n",
      "13901/13901 [==============================] - 2s 151us/sample - loss: 0.0083 - mae: 0.0645 - mse: 0.0083 - val_loss: 0.0146 - val_mae: 0.0815 - val_mse: 0.0146\n",
      "Epoch 35/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0627 - mse: 0.0078\n",
      "Epoch 00035: val_loss improved from 0.01366 to 0.01342, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 151us/sample - loss: 0.0078 - mae: 0.0627 - mse: 0.0078 - val_loss: 0.0134 - val_mae: 0.0774 - val_mse: 0.0134\n",
      "Epoch 36/1000\n",
      "13728/13901 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0619 - mse: 0.0076\n",
      "Epoch 00036: val_loss did not improve from 0.01342\n",
      "13901/13901 [==============================] - 2s 134us/sample - loss: 0.0076 - mae: 0.0622 - mse: 0.0076 - val_loss: 0.0163 - val_mae: 0.0945 - val_mse: 0.0163\n",
      "Epoch 37/1000\n",
      "13494/13901 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0614 - mse: 0.0075\n",
      "Epoch 00037: val_loss did not improve from 0.01342\n",
      "13901/13901 [==============================] - 2s 141us/sample - loss: 0.0074 - mae: 0.0613 - mse: 0.0074 - val_loss: 0.0141 - val_mae: 0.0789 - val_mse: 0.0141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000\n",
      "13624/13901 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0622 - mse: 0.0076\n",
      "Epoch 00038: val_loss did not improve from 0.01342\n",
      "13901/13901 [==============================] - 2s 119us/sample - loss: 0.0075 - mae: 0.0622 - mse: 0.0075 - val_loss: 0.0143 - val_mae: 0.0817 - val_mse: 0.0143\n",
      "Epoch 39/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0618 - mse: 0.0072\n",
      "Epoch 00039: val_loss did not improve from 0.01342\n",
      "13901/13901 [==============================] - 2s 112us/sample - loss: 0.0076 - mae: 0.0621 - mse: 0.0076 - val_loss: 0.0202 - val_mae: 0.1124 - val_mse: 0.0202\n",
      "Epoch 40/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0601 - mse: 0.0071\n",
      "Epoch 00040: val_loss did not improve from 0.01342\n",
      "13901/13901 [==============================] - 2s 112us/sample - loss: 0.0071 - mae: 0.0601 - mse: 0.0071 - val_loss: 0.0158 - val_mae: 0.0820 - val_mse: 0.0158\n",
      "Epoch 41/1000\n",
      "13598/13901 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0618 - mse: 0.0076\n",
      "Epoch 00041: val_loss improved from 0.01342 to 0.01328, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 114us/sample - loss: 0.0076 - mae: 0.0616 - mse: 0.0076 - val_loss: 0.0133 - val_mae: 0.0757 - val_mse: 0.0133\n",
      "Epoch 42/1000\n",
      "13390/13901 [===========================>..] - ETA: 0s - loss: 0.0070 - mae: 0.0588 - mse: 0.0070\n",
      "Epoch 00042: val_loss did not improve from 0.01328\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0069 - mae: 0.0586 - mse: 0.0069 - val_loss: 0.0138 - val_mae: 0.0765 - val_mse: 0.0138\n",
      "Epoch 43/1000\n",
      "13572/13901 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0583 - mse: 0.0068\n",
      "Epoch 00043: val_loss did not improve from 0.01328\n",
      "13901/13901 [==============================] - 2s 112us/sample - loss: 0.0068 - mae: 0.0584 - mse: 0.0068 - val_loss: 0.0142 - val_mae: 0.0785 - val_mse: 0.0142\n",
      "Epoch 44/1000\n",
      "13494/13901 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0589 - mse: 0.0068\n",
      "Epoch 00044: val_loss did not improve from 0.01328\n",
      "13901/13901 [==============================] - 2s 112us/sample - loss: 0.0069 - mae: 0.0590 - mse: 0.0069 - val_loss: 0.0149 - val_mae: 0.0841 - val_mse: 0.0149\n",
      "Epoch 45/1000\n",
      "13650/13901 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0568 - mse: 0.0064\n",
      "Epoch 00045: val_loss did not improve from 0.01328\n",
      "13901/13901 [==============================] - 2s 112us/sample - loss: 0.0064 - mae: 0.0567 - mse: 0.0064 - val_loss: 0.0141 - val_mae: 0.0788 - val_mse: 0.0141\n",
      "Epoch 46/1000\n",
      "13832/13901 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0601 - mse: 0.0071\n",
      "Epoch 00046: val_loss did not improve from 0.01328\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0071 - mae: 0.0601 - mse: 0.0071 - val_loss: 0.0147 - val_mae: 0.0815 - val_mse: 0.0147\n",
      "Epoch 47/1000\n",
      "13520/13901 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0574 - mse: 0.0064\n",
      "Epoch 00047: val_loss improved from 0.01328 to 0.01301, saving model to model_checkpoint.h5\n",
      "13901/13901 [==============================] - 2s 111us/sample - loss: 0.0064 - mae: 0.0574 - mse: 0.0064 - val_loss: 0.0130 - val_mae: 0.0746 - val_mse: 0.0130\n",
      "Epoch 48/1000\n",
      "13806/13901 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0560 - mse: 0.0063\n",
      "Epoch 00048: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 149us/sample - loss: 0.0063 - mae: 0.0559 - mse: 0.0063 - val_loss: 0.0143 - val_mae: 0.0811 - val_mse: 0.0143\n",
      "Epoch 49/1000\n",
      "13884/13901 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0554 - mse: 0.0059\n",
      "Epoch 00049: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 118us/sample - loss: 0.0059 - mae: 0.0554 - mse: 0.0059 - val_loss: 0.0142 - val_mae: 0.0776 - val_mse: 0.0142\n",
      "Epoch 50/1000\n",
      "13624/13901 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0559 - mse: 0.0061\n",
      "Epoch 00050: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 117us/sample - loss: 0.0061 - mae: 0.0558 - mse: 0.0061 - val_loss: 0.0142 - val_mae: 0.0793 - val_mse: 0.0142\n",
      "Epoch 51/1000\n",
      "13416/13901 [===========================>..] - ETA: 0s - loss: 0.0059 - mae: 0.0548 - mse: 0.0059\n",
      "Epoch 00051: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 109us/sample - loss: 0.0059 - mae: 0.0548 - mse: 0.0059 - val_loss: 0.0141 - val_mae: 0.0798 - val_mse: 0.0141\n",
      "Epoch 52/1000\n",
      "13416/13901 [===========================>..] - ETA: 0s - loss: 0.0060 - mae: 0.0543 - mse: 0.0060\n",
      "Epoch 00052: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0060 - mae: 0.0543 - mse: 0.0060 - val_loss: 0.0144 - val_mae: 0.0791 - val_mse: 0.0144\n",
      "Epoch 53/1000\n",
      "13364/13901 [===========================>..] - ETA: 0s - loss: 0.0058 - mae: 0.0549 - mse: 0.0058\n",
      "Epoch 00053: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0059 - mae: 0.0549 - mse: 0.0059 - val_loss: 0.0136 - val_mae: 0.0764 - val_mse: 0.0136\n",
      "Epoch 54/1000\n",
      "13442/13901 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0528 - mse: 0.0055\n",
      "Epoch 00054: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 114us/sample - loss: 0.0055 - mae: 0.0530 - mse: 0.0055 - val_loss: 0.0147 - val_mae: 0.0800 - val_mse: 0.0147\n",
      "Epoch 55/1000\n",
      "13624/13901 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0538 - mse: 0.0056\n",
      "Epoch 00055: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 117us/sample - loss: 0.0056 - mae: 0.0538 - mse: 0.0056 - val_loss: 0.0140 - val_mae: 0.0810 - val_mse: 0.0140\n",
      "Epoch 56/1000\n",
      "13780/13901 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0547 - mse: 0.0059\n",
      "Epoch 00056: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 111us/sample - loss: 0.0059 - mae: 0.0547 - mse: 0.0059 - val_loss: 0.0150 - val_mae: 0.0847 - val_mse: 0.0150\n",
      "Epoch 57/1000\n",
      "13650/13901 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0528 - mse: 0.0055\n",
      "Epoch 00057: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 1s 107us/sample - loss: 0.0055 - mae: 0.0528 - mse: 0.0055 - val_loss: 0.0139 - val_mae: 0.0771 - val_mse: 0.0139\n",
      "Epoch 58/1000\n",
      "13364/13901 [===========================>..] - ETA: 0s - loss: 0.0055 - mae: 0.0526 - mse: 0.0055\n",
      "Epoch 00058: val_loss did not improve from 0.01301\n",
      "13901/13901 [==============================] - 2s 110us/sample - loss: 0.0055 - mae: 0.0529 - mse: 0.0055 - val_loss: 0.0152 - val_mae: 0.0864 - val_mse: 0.0152\n",
      "Elapsed time during model training:  99.35601949691772\n",
      "Size of training set 18691\n",
      "Train on 12061 samples, validate on 6630 samples\n",
      "Epoch 1/1000\n",
      "11218/12061 [==========================>...] - ETA: 0s - loss: 0.0729 - mae: 0.1842 - mse: 0.0729\n",
      "Epoch 00001: val_loss improved from inf to 0.02406, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0695,  mae:0.1794,  mse:0.0695,  val_loss:0.0241,  val_mae:0.1091,  val_mse:0.0241,  \n",
      "12061/12061 [==============================] - 1s 92us/sample - loss: 0.0695 - mae: 0.1794 - mse: 0.0695 - val_loss: 0.0241 - val_mae: 0.1091 - val_mse: 0.0241\n",
      "Epoch 2/1000\n",
      "10349/12061 [========================>.....] - ETA: 0s - loss: 0.0206 - mae: 0.1060 - mse: 0.0206\n",
      "Epoch 00002: val_loss improved from 0.02406 to 0.01979, saving model to model_checkpoint.h5\n",
      "12061/12061 [==============================] - 1s 43us/sample - loss: 0.0204 - mae: 0.1053 - mse: 0.0204 - val_loss: 0.0198 - val_mae: 0.0995 - val_mse: 0.0198\n",
      "Epoch 3/1000\n",
      "10744/12061 [=========================>....] - ETA: 0s - loss: 0.0183 - mae: 0.0989 - mse: 0.0183\n",
      "Epoch 00003: val_loss improved from 0.01979 to 0.01834, saving model to model_checkpoint.h5\n",
      "12061/12061 [==============================] - 1s 43us/sample - loss: 0.0182 - mae: 0.0984 - mse: 0.0182 - val_loss: 0.0183 - val_mae: 0.0947 - val_mse: 0.0183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "10902/12061 [==========================>...] - ETA: 0s - loss: 0.0174 - mae: 0.0972 - mse: 0.0174\n",
      "Epoch 00004: val_loss did not improve from 0.01834\n",
      "12061/12061 [==============================] - 0s 41us/sample - loss: 0.0175 - mae: 0.0973 - mse: 0.0175 - val_loss: 0.0186 - val_mae: 0.0935 - val_mse: 0.0186\n",
      "Epoch 5/1000\n",
      "10902/12061 [==========================>...] - ETA: 0s - loss: 0.0171 - mae: 0.0957 - mse: 0.0171\n",
      "Epoch 00005: val_loss did not improve from 0.01834\n",
      "12061/12061 [==============================] - 0s 41us/sample - loss: 0.0171 - mae: 0.0960 - mse: 0.0171 - val_loss: 0.0190 - val_mae: 0.0971 - val_mse: 0.0190\n",
      "Epoch 6/1000\n",
      "10744/12061 [=========================>....] - ETA: 0s - loss: 0.0158 - mae: 0.0905 - mse: 0.0158\n",
      "Epoch 00006: val_loss improved from 0.01834 to 0.01760, saving model to model_checkpoint.h5\n",
      "12061/12061 [==============================] - 1s 43us/sample - loss: 0.0157 - mae: 0.0904 - mse: 0.0157 - val_loss: 0.0176 - val_mae: 0.0916 - val_mse: 0.0176\n",
      "Epoch 7/1000\n",
      "10981/12061 [==========================>...] - ETA: 0s - loss: 0.0140 - mae: 0.0851 - mse: 0.0140\n",
      "Epoch 00007: val_loss did not improve from 0.01760\n",
      "12061/12061 [==============================] - 0s 40us/sample - loss: 0.0141 - mae: 0.0850 - mse: 0.0141 - val_loss: 0.0201 - val_mae: 0.1036 - val_mse: 0.0201\n",
      "Epoch 8/1000\n",
      "11139/12061 [==========================>...] - ETA: 0s - loss: 0.0137 - mae: 0.0847 - mse: 0.0137\n",
      "Epoch 00008: val_loss improved from 0.01760 to 0.01652, saving model to model_checkpoint.h5\n",
      "12061/12061 [==============================] - 1s 46us/sample - loss: 0.0138 - mae: 0.0847 - mse: 0.0138 - val_loss: 0.0165 - val_mae: 0.0880 - val_mse: 0.0165\n",
      "Epoch 9/1000\n",
      "10823/12061 [=========================>....] - ETA: 0s - loss: 0.0135 - mae: 0.0838 - mse: 0.0135\n",
      "Epoch 00009: val_loss did not improve from 0.01652\n",
      "12061/12061 [==============================] - 1s 47us/sample - loss: 0.0134 - mae: 0.0835 - mse: 0.0134 - val_loss: 0.0170 - val_mae: 0.0894 - val_mse: 0.0170\n",
      "Epoch 10/1000\n",
      "11929/12061 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0817 - mse: 0.0127\n",
      "Epoch 00010: val_loss improved from 0.01652 to 0.01557, saving model to model_checkpoint.h5\n",
      "12061/12061 [==============================] - 1s 44us/sample - loss: 0.0127 - mae: 0.0816 - mse: 0.0127 - val_loss: 0.0156 - val_mae: 0.0835 - val_mse: 0.0156\n",
      "Epoch 11/1000\n",
      "10665/12061 [=========================>....] - ETA: 0s - loss: 0.0120 - mae: 0.0781 - mse: 0.0120\n",
      "Epoch 00011: val_loss did not improve from 0.01557\n",
      "12061/12061 [==============================] - 0s 41us/sample - loss: 0.0120 - mae: 0.0786 - mse: 0.0120 - val_loss: 0.0163 - val_mae: 0.0873 - val_mse: 0.0163\n",
      "Epoch 12/1000\n",
      "11929/12061 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0821 - mse: 0.0127\n",
      "Epoch 00012: val_loss did not improve from 0.01557\n",
      "12061/12061 [==============================] - 1s 43us/sample - loss: 0.0127 - mae: 0.0820 - mse: 0.0127 - val_loss: 0.0166 - val_mae: 0.0884 - val_mse: 0.0166\n",
      "Epoch 13/1000\n",
      "11218/12061 [==========================>...] - ETA: 0s - loss: 0.0122 - mae: 0.0806 - mse: 0.0122\n",
      "Epoch 00013: val_loss did not improve from 0.01557\n",
      "12061/12061 [==============================] - 1s 46us/sample - loss: 0.0121 - mae: 0.0802 - mse: 0.0121 - val_loss: 0.0156 - val_mae: 0.0833 - val_mse: 0.0156\n",
      "Epoch 14/1000\n",
      "11771/12061 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0793 - mse: 0.0121\n",
      "Epoch 00014: val_loss did not improve from 0.01557\n",
      "12061/12061 [==============================] - 1s 43us/sample - loss: 0.0121 - mae: 0.0793 - mse: 0.0121 - val_loss: 0.0241 - val_mae: 0.1133 - val_mse: 0.0241\n",
      "Epoch 15/1000\n",
      "11613/12061 [===========================>..] - ETA: 0s - loss: 0.0113 - mae: 0.0777 - mse: 0.0113\n",
      "Epoch 00015: val_loss did not improve from 0.01557\n",
      "12061/12061 [==============================] - 1s 44us/sample - loss: 0.0113 - mae: 0.0776 - mse: 0.0113 - val_loss: 0.0171 - val_mae: 0.0895 - val_mse: 0.0171\n",
      "Elapsed time during model training:  8.73229718208313\n",
      "Size of training set 18691\n",
      "Train on 14342 samples, validate on 4349 samples\n",
      "Epoch 1/1000\n",
      "13090/14342 [==========================>...] - ETA: 0s - loss: 0.0914 - mae: 0.2023 - mse: 0.0914\n",
      "Epoch 00001: val_loss improved from inf to 0.02184, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0856,  mae:0.1945,  mse:0.0856,  val_loss:0.0218,  val_mae:0.1098,  val_mse:0.0218,  \n",
      "14342/14342 [==============================] - 1s 55us/sample - loss: 0.0856 - mae: 0.1945 - mse: 0.0856 - val_loss: 0.0218 - val_mae: 0.1098 - val_mse: 0.0218\n",
      "Epoch 2/1000\n",
      "13420/14342 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1075 - mse: 0.0220\n",
      "Epoch 00002: val_loss improved from 0.02184 to 0.01844, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 30us/sample - loss: 0.0218 - mae: 0.1070 - mse: 0.0218 - val_loss: 0.0184 - val_mae: 0.0980 - val_mse: 0.0184\n",
      "Epoch 3/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0190 - mae: 0.0988 - mse: 0.0190\n",
      "Epoch 00003: val_loss did not improve from 0.01844\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0191 - mae: 0.0991 - mse: 0.0191 - val_loss: 0.0214 - val_mae: 0.1078 - val_mse: 0.0214\n",
      "Epoch 4/1000\n",
      "12430/14342 [=========================>....] - ETA: 0s - loss: 0.0179 - mae: 0.0950 - mse: 0.0179\n",
      "Epoch 00004: val_loss improved from 0.01844 to 0.01703, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 27us/sample - loss: 0.0176 - mae: 0.0947 - mse: 0.0176 - val_loss: 0.0170 - val_mae: 0.0903 - val_mse: 0.0170\n",
      "Epoch 5/1000\n",
      "12650/14342 [=========================>....] - ETA: 0s - loss: 0.0174 - mae: 0.0945 - mse: 0.0174\n",
      "Epoch 00005: val_loss did not improve from 0.01703\n",
      "14342/14342 [==============================] - 0s 26us/sample - loss: 0.0178 - mae: 0.0954 - mse: 0.0178 - val_loss: 0.0200 - val_mae: 0.1020 - val_mse: 0.0200\n",
      "Epoch 6/1000\n",
      "12210/14342 [========================>.....] - ETA: 0s - loss: 0.0159 - mae: 0.0901 - mse: 0.0159\n",
      "Epoch 00006: val_loss did not improve from 0.01703\n",
      "14342/14342 [==============================] - 0s 26us/sample - loss: 0.0165 - mae: 0.0911 - mse: 0.0165 - val_loss: 0.0215 - val_mae: 0.1067 - val_mse: 0.0215\n",
      "Epoch 7/1000\n",
      "12320/14342 [========================>.....] - ETA: 0s - loss: 0.0164 - mae: 0.0908 - mse: 0.0164\n",
      "Epoch 00007: val_loss improved from 0.01703 to 0.01587, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 28us/sample - loss: 0.0160 - mae: 0.0897 - mse: 0.0160 - val_loss: 0.0159 - val_mae: 0.0877 - val_mse: 0.0159\n",
      "Epoch 8/1000\n",
      "12540/14342 [=========================>....] - ETA: 0s - loss: 0.0147 - mae: 0.0851 - mse: 0.0147\n",
      "Epoch 00008: val_loss improved from 0.01587 to 0.01529, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 27us/sample - loss: 0.0147 - mae: 0.0849 - mse: 0.0147 - val_loss: 0.0153 - val_mae: 0.0859 - val_mse: 0.0153\n",
      "Epoch 9/1000\n",
      "12650/14342 [=========================>....] - ETA: 0s - loss: 0.0148 - mae: 0.0859 - mse: 0.0148\n",
      "Epoch 00009: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0148 - mae: 0.0859 - mse: 0.0148 - val_loss: 0.0160 - val_mae: 0.0882 - val_mse: 0.0160\n",
      "Epoch 10/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0141 - mae: 0.0839 - mse: 0.0141\n",
      "Epoch 00010: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0144 - mae: 0.0851 - mse: 0.0144 - val_loss: 0.0158 - val_mae: 0.0882 - val_mse: 0.0158\n",
      "Epoch 11/1000\n",
      "12430/14342 [=========================>....] - ETA: 0s - loss: 0.0138 - mae: 0.0823 - mse: 0.0138\n",
      "Epoch 00011: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 26us/sample - loss: 0.0141 - mae: 0.0835 - mse: 0.0141 - val_loss: 0.0168 - val_mae: 0.0930 - val_mse: 0.0168\n",
      "Epoch 12/1000\n",
      "12650/14342 [=========================>....] - ETA: 0s - loss: 0.0133 - mae: 0.0814 - mse: 0.0133\n",
      "Epoch 00012: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0134 - mae: 0.0816 - mse: 0.0134 - val_loss: 0.0180 - val_mae: 0.0968 - val_mse: 0.0180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000\n",
      "12100/14342 [========================>.....] - ETA: 0s - loss: 0.0144 - mae: 0.0852 - mse: 0.0144\n",
      "Epoch 00013: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 26us/sample - loss: 0.0144 - mae: 0.0853 - mse: 0.0144 - val_loss: 0.0201 - val_mae: 0.1045 - val_mse: 0.0201\n",
      "Epoch 14/1000\n",
      "12650/14342 [=========================>....] - ETA: 0s - loss: 0.0130 - mae: 0.0804 - mse: 0.0130\n",
      "Epoch 00014: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0130 - mae: 0.0802 - mse: 0.0130 - val_loss: 0.0168 - val_mae: 0.0903 - val_mse: 0.0168\n",
      "Epoch 15/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0123 - mae: 0.0780 - mse: 0.0123\n",
      "Epoch 00015: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0125 - mae: 0.0788 - mse: 0.0125 - val_loss: 0.0171 - val_mae: 0.0903 - val_mse: 0.0171\n",
      "Epoch 16/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0132 - mae: 0.0822 - mse: 0.0132\n",
      "Epoch 00016: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0132 - mae: 0.0824 - mse: 0.0132 - val_loss: 0.0154 - val_mae: 0.0872 - val_mse: 0.0154\n",
      "Epoch 17/1000\n",
      "12650/14342 [=========================>....] - ETA: 0s - loss: 0.0121 - mae: 0.0772 - mse: 0.0121\n",
      "Epoch 00017: val_loss did not improve from 0.01529\n",
      "14342/14342 [==============================] - 0s 25us/sample - loss: 0.0120 - mae: 0.0775 - mse: 0.0120 - val_loss: 0.0171 - val_mae: 0.0953 - val_mse: 0.0171\n",
      "Epoch 18/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0116 - mae: 0.0754 - mse: 0.0116\n",
      "Epoch 00018: val_loss improved from 0.01529 to 0.01407, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 27us/sample - loss: 0.0117 - mae: 0.0760 - mse: 0.0117 - val_loss: 0.0141 - val_mae: 0.0809 - val_mse: 0.0141\n",
      "Epoch 19/1000\n",
      "13200/14342 [==========================>...] - ETA: 0s - loss: 0.0122 - mae: 0.0779 - mse: 0.0122\n",
      "Epoch 00019: val_loss improved from 0.01407 to 0.01371, saving model to model_checkpoint.h5\n",
      "14342/14342 [==============================] - 0s 31us/sample - loss: 0.0120 - mae: 0.0775 - mse: 0.0120 - val_loss: 0.0137 - val_mae: 0.0782 - val_mse: 0.0137\n",
      "Epoch 20/1000\n",
      "13200/14342 [==========================>...] - ETA: 0s - loss: 0.0119 - mae: 0.0774 - mse: 0.0119\n",
      "Epoch 00020: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 30us/sample - loss: 0.0118 - mae: 0.0770 - mse: 0.0118 - val_loss: 0.0148 - val_mae: 0.0815 - val_mse: 0.0148\n",
      "Epoch 21/1000\n",
      "12540/14342 [=========================>....] - ETA: 0s - loss: 0.0108 - mae: 0.0747 - mse: 0.0108\n",
      "Epoch 00021: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 30us/sample - loss: 0.0112 - mae: 0.0750 - mse: 0.0112 - val_loss: 0.0158 - val_mae: 0.0887 - val_mse: 0.0158\n",
      "Epoch 22/1000\n",
      "14190/14342 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0740 - mse: 0.0109\n",
      "Epoch 00022: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 27us/sample - loss: 0.0108 - mae: 0.0740 - mse: 0.0108 - val_loss: 0.0142 - val_mae: 0.0836 - val_mse: 0.0142\n",
      "Epoch 23/1000\n",
      "12760/14342 [=========================>....] - ETA: 0s - loss: 0.0110 - mae: 0.0741 - mse: 0.0110\n",
      "Epoch 00023: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 31us/sample - loss: 0.0109 - mae: 0.0739 - mse: 0.0109 - val_loss: 0.0141 - val_mae: 0.0805 - val_mse: 0.0141\n",
      "Epoch 24/1000\n",
      "12870/14342 [=========================>....] - ETA: 0s - loss: 0.0114 - mae: 0.0762 - mse: 0.0114\n",
      "Epoch 00024: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 35us/sample - loss: 0.0115 - mae: 0.0765 - mse: 0.0115 - val_loss: 0.0174 - val_mae: 0.0975 - val_mse: 0.0174\n",
      "Epoch 25/1000\n",
      "14080/14342 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0757 - mse: 0.0112\n",
      "Epoch 00025: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 37us/sample - loss: 0.0112 - mae: 0.0758 - mse: 0.0112 - val_loss: 0.0148 - val_mae: 0.0830 - val_mse: 0.0148\n",
      "Epoch 26/1000\n",
      "13530/14342 [===========================>..] - ETA: 0s - loss: 0.0098 - mae: 0.0698 - mse: 0.0098\n",
      "Epoch 00026: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 38us/sample - loss: 0.0098 - mae: 0.0700 - mse: 0.0098 - val_loss: 0.0144 - val_mae: 0.0855 - val_mse: 0.0144\n",
      "Epoch 27/1000\n",
      "13640/14342 [===========================>..] - ETA: 0s - loss: 0.0099 - mae: 0.0710 - mse: 0.0099\n",
      "Epoch 00027: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 38us/sample - loss: 0.0099 - mae: 0.0711 - mse: 0.0099 - val_loss: 0.0151 - val_mae: 0.0867 - val_mse: 0.0151\n",
      "Epoch 28/1000\n",
      "13640/14342 [===========================>..] - ETA: 0s - loss: 0.0099 - mae: 0.0708 - mse: 0.0099\n",
      "Epoch 00028: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 37us/sample - loss: 0.0098 - mae: 0.0705 - mse: 0.0098 - val_loss: 0.0150 - val_mae: 0.0805 - val_mse: 0.0150\n",
      "Epoch 29/1000\n",
      "13750/14342 [===========================>..] - ETA: 0s - loss: 0.0099 - mae: 0.0703 - mse: 0.0099\n",
      "Epoch 00029: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 37us/sample - loss: 0.0099 - mae: 0.0702 - mse: 0.0099 - val_loss: 0.0146 - val_mae: 0.0830 - val_mse: 0.0146\n",
      "Epoch 30/1000\n",
      "13640/14342 [===========================>..] - ETA: 0s - loss: 0.0093 - mae: 0.0684 - mse: 0.0093\n",
      "Epoch 00030: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 1s 36us/sample - loss: 0.0093 - mae: 0.0684 - mse: 0.0093 - val_loss: 0.0138 - val_mae: 0.0794 - val_mse: 0.0138\n",
      "Epoch 31/1000\n",
      "13420/14342 [===========================>..] - ETA: 0s - loss: 0.0095 - mae: 0.0695 - mse: 0.0095\n",
      "Epoch 00031: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 29us/sample - loss: 0.0095 - mae: 0.0697 - mse: 0.0095 - val_loss: 0.0160 - val_mae: 0.0913 - val_mse: 0.0160\n",
      "Epoch 32/1000\n",
      "14300/14342 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0691 - mse: 0.0094\n",
      "Epoch 00032: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 27us/sample - loss: 0.0094 - mae: 0.0690 - mse: 0.0094 - val_loss: 0.0137 - val_mae: 0.0794 - val_mse: 0.0137\n",
      "Epoch 33/1000\n",
      "12540/14342 [=========================>....] - ETA: 0s - loss: 0.0089 - mae: 0.0670 - mse: 0.0089\n",
      "Epoch 00033: val_loss did not improve from 0.01371\n",
      "14342/14342 [==============================] - 0s 26us/sample - loss: 0.0089 - mae: 0.0670 - mse: 0.0089 - val_loss: 0.0142 - val_mae: 0.0820 - val_mse: 0.0142\n",
      "Elapsed time during model training:  14.520091772079468\n",
      "Size of training set 18691\n",
      "Train on 14211 samples, validate on 4480 samples\n",
      "Epoch 1/1000\n",
      "14175/14211 [============================>.] - ETA: 0s - loss: 0.0445 - mae: 0.1516 - mse: 0.0445\n",
      "Epoch 00001: val_loss improved from inf to 0.03681, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0445,  mae:0.1516,  mse:0.0445,  val_loss:0.0368,  val_mae:0.1461,  val_mse:0.0368,  \n",
      "14211/14211 [==============================] - 4s 294us/sample - loss: 0.0445 - mae: 0.1516 - mse: 0.0445 - val_loss: 0.0368 - val_mae: 0.1461 - val_mse: 0.0368\n",
      "Epoch 2/1000\n",
      "14076/14211 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.1178 - mse: 0.0252\n",
      "Epoch 00002: val_loss improved from 0.03681 to 0.02095, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 267us/sample - loss: 0.0252 - mae: 0.1178 - mse: 0.0252 - val_loss: 0.0209 - val_mae: 0.1002 - val_mse: 0.0209\n",
      "Epoch 3/1000\n",
      "14013/14211 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.1108 - mse: 0.0223\n",
      "Epoch 00003: val_loss did not improve from 0.02095\n",
      "14211/14211 [==============================] - 4s 267us/sample - loss: 0.0223 - mae: 0.1110 - mse: 0.0223 - val_loss: 0.0213 - val_mae: 0.1069 - val_mse: 0.0213\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14031/14211 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.1058 - mse: 0.0207\n",
      "Epoch 00004: val_loss did not improve from 0.02095\n",
      "14211/14211 [==============================] - 4s 262us/sample - loss: 0.0207 - mae: 0.1058 - mse: 0.0207 - val_loss: 0.0223 - val_mae: 0.1060 - val_mse: 0.0223\n",
      "Epoch 5/1000\n",
      "14058/14211 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.1004 - mse: 0.0191\n",
      "Epoch 00005: val_loss improved from 0.02095 to 0.01644, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 263us/sample - loss: 0.0191 - mae: 0.1004 - mse: 0.0191 - val_loss: 0.0164 - val_mae: 0.0921 - val_mse: 0.0164\n",
      "Epoch 6/1000\n",
      "14004/14211 [============================>.] - ETA: 0s - loss: 0.0186 - mae: 0.0985 - mse: 0.0186\n",
      "Epoch 00006: val_loss did not improve from 0.01644\n",
      "14211/14211 [==============================] - 4s 293us/sample - loss: 0.0185 - mae: 0.0983 - mse: 0.0185 - val_loss: 0.0175 - val_mae: 0.0908 - val_mse: 0.0175\n",
      "Epoch 7/1000\n",
      "14004/14211 [============================>.] - ETA: 0s - loss: 0.0171 - mae: 0.0938 - mse: 0.0171\n",
      "Epoch 00007: val_loss did not improve from 0.01644\n",
      "14211/14211 [==============================] - 4s 263us/sample - loss: 0.0171 - mae: 0.0939 - mse: 0.0171 - val_loss: 0.0185 - val_mae: 0.0921 - val_mse: 0.0185\n",
      "Epoch 8/1000\n",
      "13977/14211 [============================>.] - ETA: 0s - loss: 0.0165 - mae: 0.0927 - mse: 0.0165\n",
      "Epoch 00008: val_loss did not improve from 0.01644\n",
      "14211/14211 [==============================] - 4s 260us/sample - loss: 0.0165 - mae: 0.0927 - mse: 0.0165 - val_loss: 0.0167 - val_mae: 0.0892 - val_mse: 0.0167\n",
      "Epoch 9/1000\n",
      "14094/14211 [============================>.] - ETA: 0s - loss: 0.0159 - mae: 0.0902 - mse: 0.0159\n",
      "Epoch 00009: val_loss did not improve from 0.01644\n",
      "14211/14211 [==============================] - 4s 273us/sample - loss: 0.0159 - mae: 0.0902 - mse: 0.0159 - val_loss: 0.0189 - val_mae: 0.1036 - val_mse: 0.0189\n",
      "Epoch 10/1000\n",
      "14094/14211 [============================>.] - ETA: 0s - loss: 0.0152 - mae: 0.0881 - mse: 0.0152\n",
      "Epoch 00010: val_loss improved from 0.01644 to 0.01545, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 264us/sample - loss: 0.0152 - mae: 0.0881 - mse: 0.0152 - val_loss: 0.0154 - val_mae: 0.0874 - val_mse: 0.0154\n",
      "Epoch 11/1000\n",
      "14103/14211 [============================>.] - ETA: 0s - loss: 0.0147 - mae: 0.0865 - mse: 0.0147\n",
      "Epoch 00011: val_loss did not improve from 0.01545\n",
      "14211/14211 [==============================] - 4s 277us/sample - loss: 0.0148 - mae: 0.0866 - mse: 0.0148 - val_loss: 0.0177 - val_mae: 0.0933 - val_mse: 0.0177\n",
      "Epoch 12/1000\n",
      "14157/14211 [============================>.] - ETA: 0s - loss: 0.0146 - mae: 0.0865 - mse: 0.0146\n",
      "Epoch 00012: val_loss improved from 0.01545 to 0.01537, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 278us/sample - loss: 0.0146 - mae: 0.0865 - mse: 0.0146 - val_loss: 0.0154 - val_mae: 0.0874 - val_mse: 0.0154\n",
      "Epoch 13/1000\n",
      "14148/14211 [============================>.] - ETA: 0s - loss: 0.0144 - mae: 0.0843 - mse: 0.0144\n",
      "Epoch 00013: val_loss improved from 0.01537 to 0.01525, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 261us/sample - loss: 0.0144 - mae: 0.0843 - mse: 0.0144 - val_loss: 0.0153 - val_mae: 0.0838 - val_mse: 0.0153\n",
      "Epoch 14/1000\n",
      "14040/14211 [============================>.] - ETA: 0s - loss: 0.0141 - mae: 0.0845 - mse: 0.0141\n",
      "Epoch 00014: val_loss improved from 0.01525 to 0.01401, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 266us/sample - loss: 0.0140 - mae: 0.0844 - mse: 0.0140 - val_loss: 0.0140 - val_mae: 0.0807 - val_mse: 0.0140\n",
      "Epoch 15/1000\n",
      "14013/14211 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0810 - mse: 0.0130\n",
      "Epoch 00015: val_loss did not improve from 0.01401\n",
      "14211/14211 [==============================] - 4s 260us/sample - loss: 0.0130 - mae: 0.0809 - mse: 0.0130 - val_loss: 0.0152 - val_mae: 0.0863 - val_mse: 0.0152\n",
      "Epoch 16/1000\n",
      "14031/14211 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0802 - mse: 0.0129\n",
      "Epoch 00016: val_loss did not improve from 0.01401\n",
      "14211/14211 [==============================] - 4s 295us/sample - loss: 0.0128 - mae: 0.0802 - mse: 0.0128 - val_loss: 0.0140 - val_mae: 0.0798 - val_mse: 0.0140\n",
      "Epoch 17/1000\n",
      "14157/14211 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0791 - mse: 0.0125\n",
      "Epoch 00017: val_loss did not improve from 0.01401\n",
      "14211/14211 [==============================] - 4s 289us/sample - loss: 0.0125 - mae: 0.0791 - mse: 0.0125 - val_loss: 0.0150 - val_mae: 0.0836 - val_mse: 0.0150\n",
      "Epoch 18/1000\n",
      "14049/14211 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0781 - mse: 0.0123\n",
      "Epoch 00018: val_loss improved from 0.01401 to 0.01392, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 271us/sample - loss: 0.0123 - mae: 0.0781 - mse: 0.0123 - val_loss: 0.0139 - val_mae: 0.0786 - val_mse: 0.0139\n",
      "Epoch 19/1000\n",
      "13977/14211 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0779 - mse: 0.0122\n",
      "Epoch 00019: val_loss improved from 0.01392 to 0.01387, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 269us/sample - loss: 0.0121 - mae: 0.0779 - mse: 0.0121 - val_loss: 0.0139 - val_mae: 0.0799 - val_mse: 0.0139\n",
      "Epoch 20/1000\n",
      "14130/14211 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0764 - mse: 0.0116\n",
      "Epoch 00020: val_loss did not improve from 0.01387\n",
      "14211/14211 [==============================] - 4s 301us/sample - loss: 0.0116 - mae: 0.0764 - mse: 0.0116 - val_loss: 0.0151 - val_mae: 0.0860 - val_mse: 0.0151\n",
      "Epoch 21/1000\n",
      "14004/14211 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0759 - mse: 0.0115\n",
      "Epoch 00021: val_loss did not improve from 0.01387\n",
      "14211/14211 [==============================] - 4s 283us/sample - loss: 0.0115 - mae: 0.0760 - mse: 0.0115 - val_loss: 0.0156 - val_mae: 0.0863 - val_mse: 0.0156\n",
      "Epoch 22/1000\n",
      "14130/14211 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0742 - mse: 0.0111\n",
      "Epoch 00022: val_loss improved from 0.01387 to 0.01332, saving model to model_checkpoint.h5\n",
      "14211/14211 [==============================] - 4s 264us/sample - loss: 0.0111 - mae: 0.0741 - mse: 0.0111 - val_loss: 0.0133 - val_mae: 0.0770 - val_mse: 0.0133\n",
      "Epoch 23/1000\n",
      "14058/14211 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0747 - mse: 0.0111\n",
      "Epoch 00023: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 263us/sample - loss: 0.0112 - mae: 0.0747 - mse: 0.0112 - val_loss: 0.0147 - val_mae: 0.0868 - val_mse: 0.0147\n",
      "Epoch 24/1000\n",
      "14166/14211 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0724 - mse: 0.0106\n",
      "Epoch 00024: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 280us/sample - loss: 0.0106 - mae: 0.0723 - mse: 0.0106 - val_loss: 0.0149 - val_mae: 0.0861 - val_mse: 0.0149\n",
      "Epoch 25/1000\n",
      "14004/14211 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0733 - mse: 0.0107\n",
      "Epoch 00025: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 298us/sample - loss: 0.0106 - mae: 0.0733 - mse: 0.0106 - val_loss: 0.0169 - val_mae: 0.0888 - val_mse: 0.0169\n",
      "Epoch 26/1000\n",
      "14022/14211 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0722 - mse: 0.0104\n",
      "Epoch 00026: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 272us/sample - loss: 0.0104 - mae: 0.0722 - mse: 0.0104 - val_loss: 0.0146 - val_mae: 0.0820 - val_mse: 0.0146\n",
      "Epoch 27/1000\n",
      "14049/14211 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0721 - mse: 0.0104\n",
      "Epoch 00027: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 273us/sample - loss: 0.0103 - mae: 0.0720 - mse: 0.0103 - val_loss: 0.0136 - val_mae: 0.0768 - val_mse: 0.0136\n",
      "Epoch 28/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13968/14211 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0709 - mse: 0.0100\n",
      "Epoch 00028: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 271us/sample - loss: 0.0100 - mae: 0.0708 - mse: 0.0100 - val_loss: 0.0139 - val_mae: 0.0802 - val_mse: 0.0139\n",
      "Epoch 29/1000\n",
      "14058/14211 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0702 - mse: 0.0099\n",
      "Epoch 00029: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 5s 354us/sample - loss: 0.0099 - mae: 0.0701 - mse: 0.0099 - val_loss: 0.0159 - val_mae: 0.0876 - val_mse: 0.0159\n",
      "Epoch 30/1000\n",
      "14013/14211 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0695 - mse: 0.0097\n",
      "Epoch 00030: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 295us/sample - loss: 0.0096 - mae: 0.0695 - mse: 0.0096 - val_loss: 0.0139 - val_mae: 0.0806 - val_mse: 0.0139\n",
      "Epoch 31/1000\n",
      "13977/14211 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0689 - mse: 0.0095\n",
      "Epoch 00031: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 263us/sample - loss: 0.0095 - mae: 0.0689 - mse: 0.0095 - val_loss: 0.0142 - val_mae: 0.0812 - val_mse: 0.0142\n",
      "Epoch 32/1000\n",
      "14130/14211 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0676 - mse: 0.0093\n",
      "Epoch 00032: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 263us/sample - loss: 0.0093 - mae: 0.0676 - mse: 0.0093 - val_loss: 0.0142 - val_mae: 0.0813 - val_mse: 0.0142\n",
      "Epoch 33/1000\n",
      "14031/14211 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0684 - mse: 0.0094\n",
      "Epoch 00033: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 278us/sample - loss: 0.0094 - mae: 0.0685 - mse: 0.0094 - val_loss: 0.0135 - val_mae: 0.0767 - val_mse: 0.0135\n",
      "Epoch 34/1000\n",
      "14076/14211 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0672 - mse: 0.0091\n",
      "Epoch 00034: val_loss did not improve from 0.01332\n",
      "14211/14211 [==============================] - 4s 292us/sample - loss: 0.0091 - mae: 0.0674 - mse: 0.0091 - val_loss: 0.0148 - val_mae: 0.0852 - val_mse: 0.0148\n",
      "Elapsed time during model training:  134.27734899520874\n",
      "Size of training set 18691\n",
      "Train on 13070 samples, validate on 5621 samples\n",
      "Epoch 1/1000\n",
      "12519/13070 [===========================>..] - ETA: 0s - loss: 0.0469 - mae: 0.1504 - mse: 0.0469\n",
      "Epoch 00001: val_loss improved from inf to 0.03603, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0461,  mae:0.1491,  mse:0.0461,  val_loss:0.0360,  val_mae:0.1412,  val_mse:0.0360,  \n",
      "13070/13070 [==============================] - 2s 140us/sample - loss: 0.0461 - mae: 0.1491 - mse: 0.0461 - val_loss: 0.0360 - val_mae: 0.1412 - val_mse: 0.0360\n",
      "Epoch 2/1000\n",
      "12870/13070 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.1147 - mse: 0.0239\n",
      "Epoch 00002: val_loss improved from 0.03603 to 0.02089, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 105us/sample - loss: 0.0239 - mae: 0.1145 - mse: 0.0239 - val_loss: 0.0209 - val_mae: 0.1065 - val_mse: 0.0209\n",
      "Epoch 3/1000\n",
      "12558/13070 [===========================>..] - ETA: 0s - loss: 0.0207 - mae: 0.1058 - mse: 0.0207\n",
      "Epoch 00003: val_loss improved from 0.02089 to 0.01929, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 82us/sample - loss: 0.0205 - mae: 0.1053 - mse: 0.0205 - val_loss: 0.0193 - val_mae: 0.1000 - val_mse: 0.0193\n",
      "Epoch 4/1000\n",
      "12246/13070 [===========================>..] - ETA: 0s - loss: 0.0199 - mae: 0.1038 - mse: 0.0199\n",
      "Epoch 00004: val_loss did not improve from 0.01929\n",
      "13070/13070 [==============================] - 1s 77us/sample - loss: 0.0199 - mae: 0.1040 - mse: 0.0199 - val_loss: 0.0371 - val_mae: 0.1520 - val_mse: 0.0371\n",
      "Epoch 5/1000\n",
      "12246/13070 [===========================>..] - ETA: 0s - loss: 0.0194 - mae: 0.1018 - mse: 0.0194\n",
      "Epoch 00005: val_loss did not improve from 0.01929\n",
      "13070/13070 [==============================] - 1s 77us/sample - loss: 0.0192 - mae: 0.1014 - mse: 0.0192 - val_loss: 0.0201 - val_mae: 0.1000 - val_mse: 0.0201\n",
      "Epoch 6/1000\n",
      "12246/13070 [===========================>..] - ETA: 0s - loss: 0.0173 - mae: 0.0944 - mse: 0.0173\n",
      "Epoch 00006: val_loss improved from 0.01929 to 0.01863, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0174 - mae: 0.0947 - mse: 0.0174 - val_loss: 0.0186 - val_mae: 0.1012 - val_mse: 0.0186\n",
      "Epoch 7/1000\n",
      "12402/13070 [===========================>..] - ETA: 0s - loss: 0.0169 - mae: 0.0936 - mse: 0.0169\n",
      "Epoch 00007: val_loss improved from 0.01863 to 0.01647, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 77us/sample - loss: 0.0168 - mae: 0.0937 - mse: 0.0168 - val_loss: 0.0165 - val_mae: 0.0923 - val_mse: 0.0165\n",
      "Epoch 8/1000\n",
      "12285/13070 [===========================>..] - ETA: 0s - loss: 0.0159 - mae: 0.0912 - mse: 0.0159\n",
      "Epoch 00008: val_loss improved from 0.01647 to 0.01569, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 84us/sample - loss: 0.0159 - mae: 0.0912 - mse: 0.0159 - val_loss: 0.0157 - val_mae: 0.0877 - val_mse: 0.0157\n",
      "Epoch 9/1000\n",
      "12675/13070 [============================>.] - ETA: 0s - loss: 0.0171 - mae: 0.0966 - mse: 0.0171\n",
      "Epoch 00009: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0172 - mae: 0.0969 - mse: 0.0172 - val_loss: 0.0179 - val_mae: 0.0954 - val_mse: 0.0179\n",
      "Epoch 10/1000\n",
      "12714/13070 [============================>.] - ETA: 0s - loss: 0.0149 - mae: 0.0878 - mse: 0.0149\n",
      "Epoch 00010: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0150 - mae: 0.0880 - mse: 0.0150 - val_loss: 0.0186 - val_mae: 0.0966 - val_mse: 0.0186\n",
      "Epoch 11/1000\n",
      "12441/13070 [===========================>..] - ETA: 0s - loss: 0.0149 - mae: 0.0885 - mse: 0.0149\n",
      "Epoch 00011: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 78us/sample - loss: 0.0149 - mae: 0.0883 - mse: 0.0149 - val_loss: 0.0178 - val_mae: 0.0951 - val_mse: 0.0178\n",
      "Epoch 12/1000\n",
      "12831/13070 [============================>.] - ETA: 0s - loss: 0.0136 - mae: 0.0837 - mse: 0.0136\n",
      "Epoch 00012: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 90us/sample - loss: 0.0136 - mae: 0.0837 - mse: 0.0136 - val_loss: 0.0174 - val_mae: 0.0904 - val_mse: 0.0174\n",
      "Epoch 13/1000\n",
      "12441/13070 [===========================>..] - ETA: 0s - loss: 0.0142 - mae: 0.0864 - mse: 0.0142\n",
      "Epoch 00013: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 93us/sample - loss: 0.0143 - mae: 0.0866 - mse: 0.0143 - val_loss: 0.0176 - val_mae: 0.0951 - val_mse: 0.0176\n",
      "Epoch 14/1000\n",
      "12519/13070 [===========================>..] - ETA: 0s - loss: 0.0128 - mae: 0.0813 - mse: 0.0128\n",
      "Epoch 00014: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 82us/sample - loss: 0.0129 - mae: 0.0814 - mse: 0.0129 - val_loss: 0.0172 - val_mae: 0.0911 - val_mse: 0.0172\n",
      "Epoch 15/1000\n",
      "12948/13070 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0813 - mse: 0.0128\n",
      "Epoch 00015: val_loss did not improve from 0.01569\n",
      "13070/13070 [==============================] - 1s 82us/sample - loss: 0.0129 - mae: 0.0815 - mse: 0.0129 - val_loss: 0.0225 - val_mae: 0.1080 - val_mse: 0.0225\n",
      "Epoch 16/1000\n",
      "12558/13070 [===========================>..] - ETA: 0s - loss: 0.0129 - mae: 0.0819 - mse: 0.0129\n",
      "Epoch 00016: val_loss improved from 0.01569 to 0.01530, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 92us/sample - loss: 0.0129 - mae: 0.0818 - mse: 0.0129 - val_loss: 0.0153 - val_mae: 0.0867 - val_mse: 0.0153\n",
      "Epoch 17/1000\n",
      "12480/13070 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0782 - mse: 0.0119\n",
      "Epoch 00017: val_loss improved from 0.01530 to 0.01484, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 104us/sample - loss: 0.0119 - mae: 0.0782 - mse: 0.0119 - val_loss: 0.0148 - val_mae: 0.0815 - val_mse: 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000\n",
      "12909/13070 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0799 - mse: 0.0122\n",
      "Epoch 00018: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 108us/sample - loss: 0.0122 - mae: 0.0798 - mse: 0.0122 - val_loss: 0.0168 - val_mae: 0.0908 - val_mse: 0.0168\n",
      "Epoch 19/1000\n",
      "12948/13070 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0794 - mse: 0.0122\n",
      "Epoch 00019: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 107us/sample - loss: 0.0122 - mae: 0.0794 - mse: 0.0122 - val_loss: 0.0164 - val_mae: 0.0894 - val_mse: 0.0164\n",
      "Epoch 20/1000\n",
      "12714/13070 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0771 - mse: 0.0115\n",
      "Epoch 00020: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 84us/sample - loss: 0.0114 - mae: 0.0769 - mse: 0.0114 - val_loss: 0.0159 - val_mae: 0.0870 - val_mse: 0.0159\n",
      "Epoch 21/1000\n",
      "12441/13070 [===========================>..] - ETA: 0s - loss: 0.0114 - mae: 0.0768 - mse: 0.0114\n",
      "Epoch 00021: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 81us/sample - loss: 0.0113 - mae: 0.0767 - mse: 0.0113 - val_loss: 0.0156 - val_mae: 0.0903 - val_mse: 0.0156\n",
      "Epoch 22/1000\n",
      "13026/13070 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0737 - mse: 0.0108\n",
      "Epoch 00022: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 78us/sample - loss: 0.0108 - mae: 0.0737 - mse: 0.0108 - val_loss: 0.0216 - val_mae: 0.1075 - val_mse: 0.0216\n",
      "Epoch 23/1000\n",
      "12987/13070 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0735 - mse: 0.0105\n",
      "Epoch 00023: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 78us/sample - loss: 0.0105 - mae: 0.0736 - mse: 0.0105 - val_loss: 0.0150 - val_mae: 0.0864 - val_mse: 0.0150\n",
      "Epoch 24/1000\n",
      "12480/13070 [===========================>..] - ETA: 0s - loss: 0.0111 - mae: 0.0754 - mse: 0.0111\n",
      "Epoch 00024: val_loss did not improve from 0.01484\n",
      "13070/13070 [==============================] - 1s 77us/sample - loss: 0.0111 - mae: 0.0755 - mse: 0.0111 - val_loss: 0.0151 - val_mae: 0.0837 - val_mse: 0.0151\n",
      "Epoch 25/1000\n",
      "12402/13070 [===========================>..] - ETA: 0s - loss: 0.0098 - mae: 0.0707 - mse: 0.0098\n",
      "Epoch 00025: val_loss improved from 0.01484 to 0.01418, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 83us/sample - loss: 0.0099 - mae: 0.0711 - mse: 0.0099 - val_loss: 0.0142 - val_mae: 0.0804 - val_mse: 0.0142\n",
      "Epoch 26/1000\n",
      "13065/13070 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0721 - mse: 0.0102\n",
      "Epoch 00026: val_loss did not improve from 0.01418\n",
      "13070/13070 [==============================] - 1s 85us/sample - loss: 0.0102 - mae: 0.0721 - mse: 0.0102 - val_loss: 0.0174 - val_mae: 0.0955 - val_mse: 0.0174\n",
      "Epoch 27/1000\n",
      "12948/13070 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0692 - mse: 0.0094\n",
      "Epoch 00027: val_loss improved from 0.01418 to 0.01410, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 84us/sample - loss: 0.0094 - mae: 0.0692 - mse: 0.0094 - val_loss: 0.0141 - val_mae: 0.0791 - val_mse: 0.0141\n",
      "Epoch 28/1000\n",
      "12753/13070 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0674 - mse: 0.0091\n",
      "Epoch 00028: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0090 - mae: 0.0674 - mse: 0.0090 - val_loss: 0.0152 - val_mae: 0.0892 - val_mse: 0.0152\n",
      "Epoch 29/1000\n",
      "12636/13070 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0680 - mse: 0.0091\n",
      "Epoch 00029: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 84us/sample - loss: 0.0091 - mae: 0.0679 - mse: 0.0091 - val_loss: 0.0175 - val_mae: 0.0948 - val_mse: 0.0175\n",
      "Epoch 30/1000\n",
      "12714/13070 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0673 - mse: 0.0089\n",
      "Epoch 00030: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0089 - mae: 0.0673 - mse: 0.0089 - val_loss: 0.0167 - val_mae: 0.0895 - val_mse: 0.0167\n",
      "Epoch 31/1000\n",
      "12636/13070 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0674 - mse: 0.0089\n",
      "Epoch 00031: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 81us/sample - loss: 0.0089 - mae: 0.0674 - mse: 0.0089 - val_loss: 0.0147 - val_mae: 0.0799 - val_mse: 0.0147\n",
      "Epoch 32/1000\n",
      "12558/13070 [===========================>..] - ETA: 0s - loss: 0.0086 - mae: 0.0661 - mse: 0.0086\n",
      "Epoch 00032: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0087 - mae: 0.0663 - mse: 0.0087 - val_loss: 0.0185 - val_mae: 0.0978 - val_mse: 0.0185\n",
      "Epoch 33/1000\n",
      "12948/13070 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0667 - mse: 0.0089\n",
      "Epoch 00033: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 78us/sample - loss: 0.0089 - mae: 0.0667 - mse: 0.0089 - val_loss: 0.0141 - val_mae: 0.0800 - val_mse: 0.0141\n",
      "Epoch 34/1000\n",
      "12948/13070 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0638 - mse: 0.0080\n",
      "Epoch 00034: val_loss did not improve from 0.01410\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0081 - mae: 0.0639 - mse: 0.0081 - val_loss: 0.0142 - val_mae: 0.0816 - val_mse: 0.0142\n",
      "Epoch 35/1000\n",
      "13026/13070 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0639 - mse: 0.0081\n",
      "Epoch 00035: val_loss improved from 0.01410 to 0.01379, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0081 - mae: 0.0639 - mse: 0.0081 - val_loss: 0.0138 - val_mae: 0.0783 - val_mse: 0.0138\n",
      "Epoch 36/1000\n",
      "12441/13070 [===========================>..] - ETA: 0s - loss: 0.0080 - mae: 0.0642 - mse: 0.0080\n",
      "Epoch 00036: val_loss did not improve from 0.01379\n",
      "13070/13070 [==============================] - 1s 78us/sample - loss: 0.0080 - mae: 0.0645 - mse: 0.0080 - val_loss: 0.0157 - val_mae: 0.0857 - val_mse: 0.0157\n",
      "Epoch 37/1000\n",
      "12909/13070 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0625 - mse: 0.0077\n",
      "Epoch 00037: val_loss did not improve from 0.01379\n",
      "13070/13070 [==============================] - 1s 107us/sample - loss: 0.0077 - mae: 0.0625 - mse: 0.0077 - val_loss: 0.0140 - val_mae: 0.0784 - val_mse: 0.0140\n",
      "Epoch 38/1000\n",
      "12831/13070 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0634 - mse: 0.0078\n",
      "Epoch 00038: val_loss did not improve from 0.01379\n",
      "13070/13070 [==============================] - 1s 96us/sample - loss: 0.0078 - mae: 0.0635 - mse: 0.0078 - val_loss: 0.0153 - val_mae: 0.0849 - val_mse: 0.0153\n",
      "Epoch 39/1000\n",
      "12558/13070 [===========================>..] - ETA: 0s - loss: 0.0081 - mae: 0.0644 - mse: 0.0081\n",
      "Epoch 00039: val_loss improved from 0.01379 to 0.01334, saving model to model_checkpoint.h5\n",
      "13070/13070 [==============================] - 1s 83us/sample - loss: 0.0080 - mae: 0.0643 - mse: 0.0080 - val_loss: 0.0133 - val_mae: 0.0763 - val_mse: 0.0133\n",
      "Epoch 40/1000\n",
      "12636/13070 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0611 - mse: 0.0073\n",
      "Epoch 00040: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 97us/sample - loss: 0.0073 - mae: 0.0610 - mse: 0.0073 - val_loss: 0.0141 - val_mae: 0.0786 - val_mse: 0.0141\n",
      "Epoch 41/1000\n",
      "12597/13070 [===========================>..] - ETA: 0s - loss: 0.0072 - mae: 0.0610 - mse: 0.0072\n",
      "Epoch 00041: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0072 - mae: 0.0610 - mse: 0.0072 - val_loss: 0.0141 - val_mae: 0.0803 - val_mse: 0.0141\n",
      "Epoch 42/1000\n",
      "12909/13070 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0594 - mse: 0.0070\n",
      "Epoch 00042: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 83us/sample - loss: 0.0070 - mae: 0.0594 - mse: 0.0070 - val_loss: 0.0141 - val_mae: 0.0784 - val_mse: 0.0141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000\n",
      "12831/13070 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0602 - mse: 0.0069\n",
      "Epoch 00043: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0069 - mae: 0.0602 - mse: 0.0069 - val_loss: 0.0162 - val_mae: 0.0883 - val_mse: 0.0162\n",
      "Epoch 44/1000\n",
      "12636/13070 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0613 - mse: 0.0075\n",
      "Epoch 00044: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0075 - mae: 0.0611 - mse: 0.0075 - val_loss: 0.0154 - val_mae: 0.0856 - val_mse: 0.0154\n",
      "Epoch 45/1000\n",
      "12909/13070 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0573 - mse: 0.0061\n",
      "Epoch 00045: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 79us/sample - loss: 0.0065 - mae: 0.0575 - mse: 0.0065 - val_loss: 0.0187 - val_mae: 0.1037 - val_mse: 0.0187\n",
      "Epoch 46/1000\n",
      "12597/13070 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0584 - mse: 0.0066\n",
      "Epoch 00046: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 80us/sample - loss: 0.0066 - mae: 0.0587 - mse: 0.0066 - val_loss: 0.0171 - val_mae: 0.0931 - val_mse: 0.0171\n",
      "Epoch 47/1000\n",
      "13065/13070 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0570 - mse: 0.0064\n",
      "Epoch 00047: val_loss did not improve from 0.01334\n",
      "13070/13070 [==============================] - 1s 77us/sample - loss: 0.0064 - mae: 0.0570 - mse: 0.0064 - val_loss: 0.0141 - val_mae: 0.0776 - val_mse: 0.0141\n",
      "Elapsed time during model training:  52.88845753669739\n",
      "Size of training set 18691\n",
      "Train on 16167 samples, validate on 2524 samples\n",
      "Epoch 1/1000\n",
      "16140/16167 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.1448 - mse: 0.0406\n",
      "Epoch 00001: val_loss improved from inf to 0.02502, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0406,  mae:0.1447,  mse:0.0406,  val_loss:0.0250,  val_mae:0.1167,  val_mse:0.0250,  \n",
      "16167/16167 [==============================] - 4s 238us/sample - loss: 0.0406 - mae: 0.1447 - mse: 0.0406 - val_loss: 0.0250 - val_mae: 0.1167 - val_mse: 0.0250\n",
      "Epoch 2/1000\n",
      "15960/16167 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.1140 - mse: 0.0236\n",
      "Epoch 00002: val_loss improved from 0.02502 to 0.02384, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 3s 207us/sample - loss: 0.0237 - mae: 0.1142 - mse: 0.0237 - val_loss: 0.0238 - val_mae: 0.1146 - val_mse: 0.0238\n",
      "Epoch 3/1000\n",
      "16080/16167 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1099 - mse: 0.0222\n",
      "Epoch 00003: val_loss improved from 0.02384 to 0.01816, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 4s 278us/sample - loss: 0.0222 - mae: 0.1099 - mse: 0.0222 - val_loss: 0.0182 - val_mae: 0.0976 - val_mse: 0.0182\n",
      "Epoch 4/1000\n",
      "16104/16167 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.1021 - mse: 0.0197\n",
      "Epoch 00004: val_loss improved from 0.01816 to 0.01722, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 4s 247us/sample - loss: 0.0197 - mae: 0.1021 - mse: 0.0197 - val_loss: 0.0172 - val_mae: 0.0936 - val_mse: 0.0172\n",
      "Epoch 5/1000\n",
      "16092/16167 [============================>.] - ETA: 0s - loss: 0.0181 - mae: 0.0974 - mse: 0.0181\n",
      "Epoch 00005: val_loss did not improve from 0.01722\n",
      "16167/16167 [==============================] - 3s 213us/sample - loss: 0.0181 - mae: 0.0973 - mse: 0.0181 - val_loss: 0.0187 - val_mae: 0.1004 - val_mse: 0.0187\n",
      "Epoch 6/1000\n",
      "16116/16167 [============================>.] - ETA: 0s - loss: 0.0176 - mae: 0.0960 - mse: 0.0176\n",
      "Epoch 00006: val_loss did not improve from 0.01722\n",
      "16167/16167 [==============================] - 3s 207us/sample - loss: 0.0176 - mae: 0.0960 - mse: 0.0176 - val_loss: 0.0175 - val_mae: 0.0925 - val_mse: 0.0175\n",
      "Epoch 7/1000\n",
      "16044/16167 [============================>.] - ETA: 0s - loss: 0.0167 - mae: 0.0922 - mse: 0.0167\n",
      "Epoch 00007: val_loss improved from 0.01722 to 0.01553, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 3s 205us/sample - loss: 0.0167 - mae: 0.0922 - mse: 0.0167 - val_loss: 0.0155 - val_mae: 0.0867 - val_mse: 0.0155\n",
      "Epoch 8/1000\n",
      "15996/16167 [============================>.] - ETA: 0s - loss: 0.0157 - mae: 0.0901 - mse: 0.0157\n",
      "Epoch 00008: val_loss did not improve from 0.01553\n",
      "16167/16167 [==============================] - 4s 263us/sample - loss: 0.0157 - mae: 0.0901 - mse: 0.0157 - val_loss: 0.0158 - val_mae: 0.0879 - val_mse: 0.0158\n",
      "Epoch 9/1000\n",
      "16092/16167 [============================>.] - ETA: 0s - loss: 0.0156 - mae: 0.0892 - mse: 0.0156\n",
      "Epoch 00009: val_loss improved from 0.01553 to 0.01550, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 4s 241us/sample - loss: 0.0156 - mae: 0.0892 - mse: 0.0156 - val_loss: 0.0155 - val_mae: 0.0867 - val_mse: 0.0155\n",
      "Epoch 10/1000\n",
      "16008/16167 [============================>.] - ETA: 0s - loss: 0.0149 - mae: 0.0871 - mse: 0.0149\n",
      "Epoch 00010: val_loss did not improve from 0.01550\n",
      "16167/16167 [==============================] - 4s 218us/sample - loss: 0.0148 - mae: 0.0870 - mse: 0.0148 - val_loss: 0.0160 - val_mae: 0.0891 - val_mse: 0.0160\n",
      "Epoch 11/1000\n",
      "16056/16167 [============================>.] - ETA: 0s - loss: 0.0145 - mae: 0.0864 - mse: 0.0145\n",
      "Epoch 00011: val_loss improved from 0.01550 to 0.01507, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 3s 209us/sample - loss: 0.0145 - mae: 0.0864 - mse: 0.0145 - val_loss: 0.0151 - val_mae: 0.0872 - val_mse: 0.0151\n",
      "Epoch 12/1000\n",
      "15900/16167 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0836 - mse: 0.0137\n",
      "Epoch 00012: val_loss did not improve from 0.01507\n",
      "16167/16167 [==============================] - 3s 206us/sample - loss: 0.0137 - mae: 0.0837 - mse: 0.0137 - val_loss: 0.0157 - val_mae: 0.0901 - val_mse: 0.0157\n",
      "Epoch 13/1000\n",
      "15984/16167 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0817 - mse: 0.0134\n",
      "Epoch 00013: val_loss did not improve from 0.01507\n",
      "16167/16167 [==============================] - 4s 237us/sample - loss: 0.0133 - mae: 0.0817 - mse: 0.0133 - val_loss: 0.0152 - val_mae: 0.0840 - val_mse: 0.0152\n",
      "Epoch 14/1000\n",
      "15984/16167 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0809 - mse: 0.0130\n",
      "Epoch 00014: val_loss improved from 0.01507 to 0.01422, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 4s 217us/sample - loss: 0.0130 - mae: 0.0809 - mse: 0.0130 - val_loss: 0.0142 - val_mae: 0.0837 - val_mse: 0.0142\n",
      "Epoch 15/1000\n",
      "16032/16167 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0806 - mse: 0.0129\n",
      "Epoch 00015: val_loss improved from 0.01422 to 0.01421, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 3s 209us/sample - loss: 0.0129 - mae: 0.0806 - mse: 0.0129 - val_loss: 0.0142 - val_mae: 0.0795 - val_mse: 0.0142\n",
      "Epoch 16/1000\n",
      "16056/16167 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0775 - mse: 0.0122\n",
      "Epoch 00016: val_loss did not improve from 0.01421\n",
      "16167/16167 [==============================] - 3s 207us/sample - loss: 0.0122 - mae: 0.0775 - mse: 0.0122 - val_loss: 0.0158 - val_mae: 0.0860 - val_mse: 0.0158\n",
      "Epoch 17/1000\n",
      "16092/16167 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0784 - mse: 0.0122\n",
      "Epoch 00017: val_loss did not improve from 0.01421\n",
      "16167/16167 [==============================] - 3s 213us/sample - loss: 0.0122 - mae: 0.0783 - mse: 0.0122 - val_loss: 0.0148 - val_mae: 0.0852 - val_mse: 0.0148\n",
      "Epoch 18/1000\n",
      "16056/16167 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0775 - mse: 0.0119\n",
      "Epoch 00018: val_loss improved from 0.01421 to 0.01347, saving model to model_checkpoint.h5\n",
      "16167/16167 [==============================] - 3s 209us/sample - loss: 0.0119 - mae: 0.0775 - mse: 0.0119 - val_loss: 0.0135 - val_mae: 0.0801 - val_mse: 0.0135\n",
      "Epoch 19/1000\n",
      "16032/16167 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0745 - mse: 0.0111\n",
      "Epoch 00019: val_loss did not improve from 0.01347\n",
      "16167/16167 [==============================] - 4s 270us/sample - loss: 0.0111 - mae: 0.0744 - mse: 0.0111 - val_loss: 0.0175 - val_mae: 0.0983 - val_mse: 0.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000\n",
      "15996/16167 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0744 - mse: 0.0111\n",
      "Epoch 00020: val_loss did not improve from 0.01347\n",
      "16167/16167 [==============================] - 4s 218us/sample - loss: 0.0111 - mae: 0.0742 - mse: 0.0111 - val_loss: 0.0148 - val_mae: 0.0819 - val_mse: 0.0148\n",
      "Epoch 21/1000\n",
      "16056/16167 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0756 - mse: 0.0112\n",
      "Epoch 00021: val_loss did not improve from 0.01347\n",
      "16167/16167 [==============================] - 3s 214us/sample - loss: 0.0112 - mae: 0.0757 - mse: 0.0112 - val_loss: 0.0185 - val_mae: 0.1013 - val_mse: 0.0185\n",
      "Epoch 22/1000\n",
      "16140/16167 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0742 - mse: 0.0110\n",
      "Epoch 00022: val_loss did not improve from 0.01347\n",
      "16167/16167 [==============================] - 3s 206us/sample - loss: 0.0110 - mae: 0.0741 - mse: 0.0110 - val_loss: 0.0145 - val_mae: 0.0814 - val_mse: 0.0145\n",
      "Elapsed time during model training:  80.05635523796082\n",
      "Size of training set 18691\n",
      "Train on 14524 samples, validate on 4167 samples\n",
      "Epoch 1/1000\n",
      "12644/14524 [=========================>....] - ETA: 0s - loss: 0.0690 - mae: 0.1738 - mse: 0.0690\n",
      "Epoch 00001: val_loss improved from inf to 0.02019, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0630,  mae:0.1652,  mse:0.0630,  val_loss:0.0202,  val_mae:0.1035,  val_mse:0.0202,  \n",
      "14524/14524 [==============================] - 1s 57us/sample - loss: 0.0630 - mae: 0.1652 - mse: 0.0630 - val_loss: 0.0202 - val_mae: 0.1035 - val_mse: 0.0202\n",
      "Epoch 2/1000\n",
      "14500/14524 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.1010 - mse: 0.0197\n",
      "Epoch 00002: val_loss improved from 0.02019 to 0.01885, saving model to model_checkpoint.h5\n",
      "14524/14524 [==============================] - 0s 31us/sample - loss: 0.0197 - mae: 0.1010 - mse: 0.0197 - val_loss: 0.0189 - val_mae: 0.1015 - val_mse: 0.0189\n",
      "Epoch 3/1000\n",
      "12876/14524 [=========================>....] - ETA: 0s - loss: 0.0176 - mae: 0.0956 - mse: 0.0176\n",
      "Epoch 00003: val_loss did not improve from 0.01885\n",
      "14524/14524 [==============================] - 0s 27us/sample - loss: 0.0174 - mae: 0.0950 - mse: 0.0174 - val_loss: 0.0213 - val_mae: 0.1085 - val_mse: 0.0213\n",
      "Epoch 4/1000\n",
      "14152/14524 [============================>.] - ETA: 0s - loss: 0.0167 - mae: 0.0929 - mse: 0.0167\n",
      "Epoch 00004: val_loss did not improve from 0.01885\n",
      "14524/14524 [==============================] - 0s 29us/sample - loss: 0.0167 - mae: 0.0930 - mse: 0.0167 - val_loss: 0.0260 - val_mae: 0.1180 - val_mse: 0.0260\n",
      "Epoch 5/1000\n",
      "12412/14524 [========================>.....] - ETA: 0s - loss: 0.0170 - mae: 0.0942 - mse: 0.0170\n",
      "Epoch 00005: val_loss did not improve from 0.01885\n",
      "14524/14524 [==============================] - 0s 28us/sample - loss: 0.0167 - mae: 0.0936 - mse: 0.0167 - val_loss: 0.0190 - val_mae: 0.0976 - val_mse: 0.0190\n",
      "Epoch 6/1000\n",
      "13920/14524 [===========================>..] - ETA: 0s - loss: 0.0175 - mae: 0.0966 - mse: 0.0175\n",
      "Epoch 00006: val_loss improved from 0.01885 to 0.01863, saving model to model_checkpoint.h5\n",
      "14524/14524 [==============================] - 0s 32us/sample - loss: 0.0174 - mae: 0.0962 - mse: 0.0174 - val_loss: 0.0186 - val_mae: 0.0953 - val_mse: 0.0186\n",
      "Epoch 7/1000\n",
      "13572/14524 [===========================>..] - ETA: 0s - loss: 0.0152 - mae: 0.0890 - mse: 0.0152\n",
      "Epoch 00007: val_loss improved from 0.01863 to 0.01737, saving model to model_checkpoint.h5\n",
      "14524/14524 [==============================] - 0s 32us/sample - loss: 0.0151 - mae: 0.0888 - mse: 0.0151 - val_loss: 0.0174 - val_mae: 0.0956 - val_mse: 0.0174\n",
      "Epoch 8/1000\n",
      "12180/14524 [========================>.....] - ETA: 0s - loss: 0.0152 - mae: 0.0889 - mse: 0.0152\n",
      "Epoch 00008: val_loss improved from 0.01737 to 0.01684, saving model to model_checkpoint.h5\n",
      "14524/14524 [==============================] - 0s 30us/sample - loss: 0.0151 - mae: 0.0888 - mse: 0.0151 - val_loss: 0.0168 - val_mae: 0.0905 - val_mse: 0.0168\n",
      "Epoch 9/1000\n",
      "12992/14524 [=========================>....] - ETA: 0s - loss: 0.0146 - mae: 0.0878 - mse: 0.0146\n",
      "Epoch 00009: val_loss improved from 0.01684 to 0.01501, saving model to model_checkpoint.h5\n",
      "14524/14524 [==============================] - 0s 29us/sample - loss: 0.0143 - mae: 0.0867 - mse: 0.0143 - val_loss: 0.0150 - val_mae: 0.0845 - val_mse: 0.0150\n",
      "Epoch 10/1000\n",
      "14384/14524 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0812 - mse: 0.0130\n",
      "Epoch 00010: val_loss did not improve from 0.01501\n",
      "14524/14524 [==============================] - 0s 33us/sample - loss: 0.0130 - mae: 0.0814 - mse: 0.0130 - val_loss: 0.0178 - val_mae: 0.0991 - val_mse: 0.0178\n",
      "Epoch 11/1000\n",
      "13572/14524 [===========================>..] - ETA: 0s - loss: 0.0132 - mae: 0.0836 - mse: 0.0132\n",
      "Epoch 00011: val_loss did not improve from 0.01501\n",
      "14524/14524 [==============================] - 1s 35us/sample - loss: 0.0131 - mae: 0.0834 - mse: 0.0131 - val_loss: 0.0152 - val_mae: 0.0888 - val_mse: 0.0152\n",
      "Epoch 12/1000\n",
      "12876/14524 [=========================>....] - ETA: 0s - loss: 0.0131 - mae: 0.0826 - mse: 0.0131\n",
      "Epoch 00012: val_loss did not improve from 0.01501\n",
      "14524/14524 [==============================] - 1s 36us/sample - loss: 0.0134 - mae: 0.0836 - mse: 0.0134 - val_loss: 0.0164 - val_mae: 0.0908 - val_mse: 0.0164\n",
      "Epoch 13/1000\n",
      "12760/14524 [=========================>....] - ETA: 0s - loss: 0.0123 - mae: 0.0794 - mse: 0.0123\n",
      "Epoch 00013: val_loss did not improve from 0.01501\n",
      "14524/14524 [==============================] - 1s 37us/sample - loss: 0.0121 - mae: 0.0791 - mse: 0.0121 - val_loss: 0.0186 - val_mae: 0.1014 - val_mse: 0.0186\n",
      "Epoch 14/1000\n",
      "12992/14524 [=========================>....] - ETA: 0s - loss: 0.0126 - mae: 0.0820 - mse: 0.0126\n",
      "Epoch 00014: val_loss did not improve from 0.01501\n",
      "14524/14524 [==============================] - 1s 36us/sample - loss: 0.0126 - mae: 0.0820 - mse: 0.0126 - val_loss: 0.0210 - val_mae: 0.1063 - val_mse: 0.0210\n",
      "Elapsed time during model training:  7.1812403202056885\n",
      "Size of training set 18691\n",
      "Train on 15478 samples, validate on 3213 samples\n",
      "Epoch 1/1000\n",
      "14105/15478 [==========================>...] - ETA: 0s - loss: 0.0611 - mae: 0.1656 - mse: 0.0611\n",
      "Epoch 00001: val_loss improved from inf to 0.02174, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0577,  mae:0.1607,  mse:0.0577,  val_loss:0.0217,  val_mae:0.1104,  val_mse:0.0217,  \n",
      "15478/15478 [==============================] - 1s 64us/sample - loss: 0.0577 - mae: 0.1607 - mse: 0.0577 - val_loss: 0.0217 - val_mae: 0.1104 - val_mse: 0.0217\n",
      "Epoch 2/1000\n",
      "13923/15478 [=========================>....] - ETA: 0s - loss: 0.0207 - mae: 0.1042 - mse: 0.0207\n",
      "Epoch 00002: val_loss did not improve from 0.02174\n",
      "15478/15478 [==============================] - 1s 35us/sample - loss: 0.0204 - mae: 0.1034 - mse: 0.0204 - val_loss: 0.0223 - val_mae: 0.1068 - val_mse: 0.0223\n",
      "Epoch 3/1000\n",
      "14651/15478 [===========================>..] - ETA: 0s - loss: 0.0192 - mae: 0.1002 - mse: 0.0192\n",
      "Epoch 00003: val_loss improved from 0.02174 to 0.01789, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 1s 43us/sample - loss: 0.0190 - mae: 0.0996 - mse: 0.0190 - val_loss: 0.0179 - val_mae: 0.0949 - val_mse: 0.0179\n",
      "Epoch 4/1000\n",
      "14196/15478 [==========================>...] - ETA: 0s - loss: 0.0172 - mae: 0.0944 - mse: 0.0172\n",
      "Epoch 00004: val_loss did not improve from 0.01789\n",
      "15478/15478 [==============================] - 1s 44us/sample - loss: 0.0171 - mae: 0.0942 - mse: 0.0171 - val_loss: 0.0270 - val_mae: 0.1227 - val_mse: 0.0270\n",
      "Epoch 5/1000\n",
      "15379/15478 [============================>.] - ETA: 0s - loss: 0.0165 - mae: 0.0920 - mse: 0.0165\n",
      "Epoch 00005: val_loss did not improve from 0.01789\n",
      "15478/15478 [==============================] - 1s 42us/sample - loss: 0.0166 - mae: 0.0921 - mse: 0.0166 - val_loss: 0.0183 - val_mae: 0.0971 - val_mse: 0.0183\n",
      "Epoch 6/1000\n",
      "14469/15478 [===========================>..] - ETA: 0s - loss: 0.0162 - mae: 0.0921 - mse: 0.0162\n",
      "Epoch 00006: val_loss improved from 0.01789 to 0.01764, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 1s 37us/sample - loss: 0.0162 - mae: 0.0922 - mse: 0.0162 - val_loss: 0.0176 - val_mae: 0.0929 - val_mse: 0.0176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000\n",
      "14924/15478 [===========================>..] - ETA: 0s - loss: 0.0150 - mae: 0.0870 - mse: 0.0150\n",
      "Epoch 00007: val_loss improved from 0.01764 to 0.01538, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 1s 42us/sample - loss: 0.0149 - mae: 0.0870 - mse: 0.0149 - val_loss: 0.0154 - val_mae: 0.0890 - val_mse: 0.0154\n",
      "Epoch 8/1000\n",
      "15197/15478 [============================>.] - ETA: 0s - loss: 0.0142 - mae: 0.0845 - mse: 0.0142\n",
      "Epoch 00008: val_loss did not improve from 0.01538\n",
      "15478/15478 [==============================] - 1s 36us/sample - loss: 0.0143 - mae: 0.0846 - mse: 0.0143 - val_loss: 0.0176 - val_mae: 0.0974 - val_mse: 0.0176\n",
      "Epoch 9/1000\n",
      "14287/15478 [==========================>...] - ETA: 0s - loss: 0.0143 - mae: 0.0862 - mse: 0.0143\n",
      "Epoch 00009: val_loss did not improve from 0.01538\n",
      "15478/15478 [==============================] - 1s 34us/sample - loss: 0.0143 - mae: 0.0860 - mse: 0.0143 - val_loss: 0.0175 - val_mae: 0.0924 - val_mse: 0.0175\n",
      "Epoch 10/1000\n",
      "15197/15478 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0816 - mse: 0.0132\n",
      "Epoch 00010: val_loss did not improve from 0.01538\n",
      "15478/15478 [==============================] - 1s 38us/sample - loss: 0.0132 - mae: 0.0817 - mse: 0.0132 - val_loss: 0.0154 - val_mae: 0.0862 - val_mse: 0.0154\n",
      "Epoch 11/1000\n",
      "14287/15478 [==========================>...] - ETA: 0s - loss: 0.0130 - mae: 0.0815 - mse: 0.0130\n",
      "Epoch 00011: val_loss did not improve from 0.01538\n",
      "15478/15478 [==============================] - 1s 34us/sample - loss: 0.0129 - mae: 0.0809 - mse: 0.0129 - val_loss: 0.0157 - val_mae: 0.0891 - val_mse: 0.0157\n",
      "Epoch 12/1000\n",
      "14287/15478 [==========================>...] - ETA: 0s - loss: 0.0116 - mae: 0.0771 - mse: 0.0116\n",
      "Epoch 00012: val_loss improved from 0.01538 to 0.01502, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 1s 36us/sample - loss: 0.0120 - mae: 0.0774 - mse: 0.0120 - val_loss: 0.0150 - val_mae: 0.0867 - val_mse: 0.0150\n",
      "Epoch 13/1000\n",
      "14014/15478 [==========================>...] - ETA: 0s - loss: 0.0130 - mae: 0.0826 - mse: 0.0130\n",
      "Epoch 00013: val_loss improved from 0.01502 to 0.01426, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 0s 32us/sample - loss: 0.0128 - mae: 0.0818 - mse: 0.0128 - val_loss: 0.0143 - val_mae: 0.0809 - val_mse: 0.0143\n",
      "Epoch 14/1000\n",
      "14196/15478 [==========================>...] - ETA: 0s - loss: 0.0118 - mae: 0.0767 - mse: 0.0118\n",
      "Epoch 00014: val_loss did not improve from 0.01426\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0119 - mae: 0.0772 - mse: 0.0119 - val_loss: 0.0145 - val_mae: 0.0841 - val_mse: 0.0145\n",
      "Epoch 15/1000\n",
      "15379/15478 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0740 - mse: 0.0110\n",
      "Epoch 00015: val_loss did not improve from 0.01426\n",
      "15478/15478 [==============================] - 0s 32us/sample - loss: 0.0110 - mae: 0.0741 - mse: 0.0110 - val_loss: 0.0183 - val_mae: 0.0980 - val_mse: 0.0183\n",
      "Epoch 16/1000\n",
      "13923/15478 [=========================>....] - ETA: 0s - loss: 0.0110 - mae: 0.0748 - mse: 0.0110\n",
      "Epoch 00016: val_loss improved from 0.01426 to 0.01418, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 1s 33us/sample - loss: 0.0110 - mae: 0.0748 - mse: 0.0110 - val_loss: 0.0142 - val_mae: 0.0816 - val_mse: 0.0142\n",
      "Epoch 17/1000\n",
      "13741/15478 [=========================>....] - ETA: 0s - loss: 0.0111 - mae: 0.0767 - mse: 0.0111\n",
      "Epoch 00017: val_loss did not improve from 0.01418\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0117 - mae: 0.0776 - mse: 0.0117 - val_loss: 0.0150 - val_mae: 0.0880 - val_mse: 0.0150\n",
      "Epoch 18/1000\n",
      "14014/15478 [==========================>...] - ETA: 0s - loss: 0.0103 - mae: 0.0722 - mse: 0.0103\n",
      "Epoch 00018: val_loss did not improve from 0.01418\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0103 - mae: 0.0720 - mse: 0.0103 - val_loss: 0.0143 - val_mae: 0.0804 - val_mse: 0.0143\n",
      "Epoch 19/1000\n",
      "14105/15478 [==========================>...] - ETA: 0s - loss: 0.0107 - mae: 0.0740 - mse: 0.0107\n",
      "Epoch 00019: val_loss did not improve from 0.01418\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0107 - mae: 0.0741 - mse: 0.0107 - val_loss: 0.0217 - val_mae: 0.1055 - val_mse: 0.0217\n",
      "Epoch 20/1000\n",
      "14469/15478 [===========================>..] - ETA: 0s - loss: 0.0115 - mae: 0.0762 - mse: 0.0115\n",
      "Epoch 00020: val_loss did not improve from 0.01418\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0114 - mae: 0.0757 - mse: 0.0114 - val_loss: 0.0149 - val_mae: 0.0829 - val_mse: 0.0149\n",
      "Epoch 21/1000\n",
      "14378/15478 [==========================>...] - ETA: 0s - loss: 0.0098 - mae: 0.0709 - mse: 0.0098\n",
      "Epoch 00021: val_loss improved from 0.01418 to 0.01358, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 0s 32us/sample - loss: 0.0098 - mae: 0.0708 - mse: 0.0098 - val_loss: 0.0136 - val_mae: 0.0797 - val_mse: 0.0136\n",
      "Epoch 22/1000\n",
      "14651/15478 [===========================>..] - ETA: 0s - loss: 0.0100 - mae: 0.0716 - mse: 0.0100\n",
      "Epoch 00022: val_loss did not improve from 0.01358\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0099 - mae: 0.0714 - mse: 0.0099 - val_loss: 0.0158 - val_mae: 0.0899 - val_mse: 0.0158\n",
      "Epoch 23/1000\n",
      "14469/15478 [===========================>..] - ETA: 0s - loss: 0.0091 - mae: 0.0675 - mse: 0.0091\n",
      "Epoch 00023: val_loss did not improve from 0.01358\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0091 - mae: 0.0676 - mse: 0.0091 - val_loss: 0.0158 - val_mae: 0.0871 - val_mse: 0.0158\n",
      "Epoch 24/1000\n",
      "13832/15478 [=========================>....] - ETA: 0s - loss: 0.0084 - mae: 0.0661 - mse: 0.0084\n",
      "Epoch 00024: val_loss did not improve from 0.01358\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0088 - mae: 0.0662 - mse: 0.0088 - val_loss: 0.0171 - val_mae: 0.0911 - val_mse: 0.0171\n",
      "Epoch 25/1000\n",
      "14196/15478 [==========================>...] - ETA: 0s - loss: 0.0087 - mae: 0.0661 - mse: 0.0087\n",
      "Epoch 00025: val_loss improved from 0.01358 to 0.01312, saving model to model_checkpoint.h5\n",
      "15478/15478 [==============================] - 0s 32us/sample - loss: 0.0087 - mae: 0.0663 - mse: 0.0087 - val_loss: 0.0131 - val_mae: 0.0754 - val_mse: 0.0131\n",
      "Epoch 26/1000\n",
      "14105/15478 [==========================>...] - ETA: 0s - loss: 0.0086 - mae: 0.0662 - mse: 0.0086\n",
      "Epoch 00026: val_loss did not improve from 0.01312\n",
      "15478/15478 [==============================] - 0s 31us/sample - loss: 0.0087 - mae: 0.0667 - mse: 0.0087 - val_loss: 0.0134 - val_mae: 0.0785 - val_mse: 0.0134\n",
      "Epoch 27/1000\n",
      "14287/15478 [==========================>...] - ETA: 0s - loss: 0.0087 - mae: 0.0665 - mse: 0.0087\n",
      "Epoch 00027: val_loss did not improve from 0.01312\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0086 - mae: 0.0661 - mse: 0.0086 - val_loss: 0.0156 - val_mae: 0.0906 - val_mse: 0.0156\n",
      "Epoch 28/1000\n",
      "14378/15478 [==========================>...] - ETA: 0s - loss: 0.0086 - mae: 0.0660 - mse: 0.0086\n",
      "Epoch 00028: val_loss did not improve from 0.01312\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0085 - mae: 0.0657 - mse: 0.0085 - val_loss: 0.0149 - val_mae: 0.0841 - val_mse: 0.0149\n",
      "Epoch 29/1000\n",
      "14469/15478 [===========================>..] - ETA: 0s - loss: 0.0074 - mae: 0.0621 - mse: 0.0074\n",
      "Epoch 00029: val_loss did not improve from 0.01312\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0078 - mae: 0.0629 - mse: 0.0078 - val_loss: 0.0158 - val_mae: 0.0912 - val_mse: 0.0158\n",
      "Epoch 30/1000\n",
      "14287/15478 [==========================>...] - ETA: 0s - loss: 0.0082 - mae: 0.0651 - mse: 0.0082\n",
      "Epoch 00030: val_loss did not improve from 0.01312\n",
      "15478/15478 [==============================] - 0s 30us/sample - loss: 0.0082 - mae: 0.0650 - mse: 0.0082 - val_loss: 0.0151 - val_mae: 0.0843 - val_mse: 0.0151\n",
      "Epoch 31/1000\n",
      "14742/15478 [===========================>..] - ETA: 0s - loss: 0.0079 - mae: 0.0628 - mse: 0.0079\n",
      "Epoch 00031: val_loss improved from 0.01312 to 0.01286, saving model to model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15478/15478 [==============================] - 0s 32us/sample - loss: 0.0078 - mae: 0.0627 - mse: 0.0078 - val_loss: 0.0129 - val_mae: 0.0753 - val_mse: 0.0129\n",
      "Epoch 32/1000\n",
      "15379/15478 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0605 - mse: 0.0072\n",
      "Epoch 00032: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 44us/sample - loss: 0.0072 - mae: 0.0605 - mse: 0.0072 - val_loss: 0.0139 - val_mae: 0.0814 - val_mse: 0.0139\n",
      "Epoch 33/1000\n",
      "14560/15478 [===========================>..] - ETA: 0s - loss: 0.0074 - mae: 0.0616 - mse: 0.0074\n",
      "Epoch 00033: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 43us/sample - loss: 0.0073 - mae: 0.0614 - mse: 0.0073 - val_loss: 0.0131 - val_mae: 0.0764 - val_mse: 0.0131\n",
      "Epoch 34/1000\n",
      "14833/15478 [===========================>..] - ETA: 0s - loss: 0.0075 - mae: 0.0621 - mse: 0.0075\n",
      "Epoch 00034: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 42us/sample - loss: 0.0075 - mae: 0.0620 - mse: 0.0075 - val_loss: 0.0138 - val_mae: 0.0791 - val_mse: 0.0138\n",
      "Epoch 35/1000\n",
      "14742/15478 [===========================>..] - ETA: 0s - loss: 0.0072 - mae: 0.0611 - mse: 0.0072\n",
      "Epoch 00035: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 42us/sample - loss: 0.0073 - mae: 0.0614 - mse: 0.0073 - val_loss: 0.0136 - val_mae: 0.0806 - val_mse: 0.0136\n",
      "Epoch 36/1000\n",
      "14196/15478 [==========================>...] - ETA: 0s - loss: 0.0070 - mae: 0.0600 - mse: 0.0070\n",
      "Epoch 00036: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 43us/sample - loss: 0.0070 - mae: 0.0602 - mse: 0.0070 - val_loss: 0.0141 - val_mae: 0.0816 - val_mse: 0.0141\n",
      "Epoch 37/1000\n",
      "15470/15478 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0585 - mse: 0.0067\n",
      "Epoch 00037: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 44us/sample - loss: 0.0067 - mae: 0.0585 - mse: 0.0067 - val_loss: 0.0129 - val_mae: 0.0759 - val_mse: 0.0129\n",
      "Epoch 38/1000\n",
      "15379/15478 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0599 - mse: 0.0070\n",
      "Epoch 00038: val_loss did not improve from 0.01286\n",
      "15478/15478 [==============================] - 1s 40us/sample - loss: 0.0070 - mae: 0.0599 - mse: 0.0070 - val_loss: 0.0138 - val_mae: 0.0793 - val_mse: 0.0138\n",
      "Elapsed time during model training:  21.525593757629395\n",
      "Size of training set 18691\n",
      "Train on 7681 samples, validate on 11010 samples\n",
      "Epoch 1/1000\n",
      "7623/7681 [============================>.] - ETA: 0s - loss: 0.0615 - mae: 0.1658 - mse: 0.0615\n",
      "Epoch 00001: val_loss improved from inf to 0.02346, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0613,  mae:0.1656,  mse:0.0613,  val_loss:0.0235,  val_mae:0.1097,  val_mse:0.0235,  \n",
      "7681/7681 [==============================] - 4s 497us/sample - loss: 0.0613 - mae: 0.1656 - mse: 0.0613 - val_loss: 0.0235 - val_mae: 0.1097 - val_mse: 0.0235\n",
      "Epoch 2/1000\n",
      "7546/7681 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.1150 - mse: 0.0243\n",
      "Epoch 00002: val_loss did not improve from 0.02346\n",
      "7681/7681 [==============================] - 3s 402us/sample - loss: 0.0243 - mae: 0.1150 - mse: 0.0243 - val_loss: 0.0241 - val_mae: 0.1154 - val_mse: 0.0241\n",
      "Epoch 3/1000\n",
      "7579/7681 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.1061 - mse: 0.0210\n",
      "Epoch 00003: val_loss did not improve from 0.02346\n",
      "7681/7681 [==============================] - 3s 373us/sample - loss: 0.0212 - mae: 0.1065 - mse: 0.0212 - val_loss: 0.0310 - val_mae: 0.1277 - val_mse: 0.0310\n",
      "Epoch 4/1000\n",
      "7568/7681 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.1047 - mse: 0.0203\n",
      "Epoch 00004: val_loss did not improve from 0.02346\n",
      "7681/7681 [==============================] - 3s 373us/sample - loss: 0.0203 - mae: 0.1047 - mse: 0.0203 - val_loss: 0.0325 - val_mae: 0.1363 - val_mse: 0.0325\n",
      "Epoch 5/1000\n",
      "7678/7681 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.1019 - mse: 0.0194\n",
      "Epoch 00005: val_loss improved from 0.02346 to 0.01943, saving model to model_checkpoint.h5\n",
      "7681/7681 [==============================] - 3s 452us/sample - loss: 0.0194 - mae: 0.1020 - mse: 0.0194 - val_loss: 0.0194 - val_mae: 0.0978 - val_mse: 0.0194\n",
      "Epoch 6/1000\n",
      "7612/7681 [============================>.] - ETA: 0s - loss: 0.0184 - mae: 0.0992 - mse: 0.0184\n",
      "Epoch 00006: val_loss did not improve from 0.01943\n",
      "7681/7681 [==============================] - 4s 487us/sample - loss: 0.0184 - mae: 0.0991 - mse: 0.0184 - val_loss: 0.0273 - val_mae: 0.1205 - val_mse: 0.0273\n",
      "Epoch 7/1000\n",
      "7535/7681 [============================>.] - ETA: 0s - loss: 0.0172 - mae: 0.0956 - mse: 0.0172\n",
      "Epoch 00007: val_loss improved from 0.01943 to 0.01834, saving model to model_checkpoint.h5\n",
      "7681/7681 [==============================] - 3s 390us/sample - loss: 0.0172 - mae: 0.0954 - mse: 0.0172 - val_loss: 0.0183 - val_mae: 0.0929 - val_mse: 0.0183\n",
      "Epoch 8/1000\n",
      "7623/7681 [============================>.] - ETA: 0s - loss: 0.0166 - mae: 0.0943 - mse: 0.0166\n",
      "Epoch 00008: val_loss did not improve from 0.01834\n",
      "7681/7681 [==============================] - 3s 379us/sample - loss: 0.0166 - mae: 0.0942 - mse: 0.0166 - val_loss: 0.0235 - val_mae: 0.1091 - val_mse: 0.0235\n",
      "Epoch 9/1000\n",
      "7656/7681 [============================>.] - ETA: 0s - loss: 0.0168 - mae: 0.0944 - mse: 0.0168\n",
      "Epoch 00009: val_loss did not improve from 0.01834\n",
      "7681/7681 [==============================] - 3s 380us/sample - loss: 0.0168 - mae: 0.0944 - mse: 0.0168 - val_loss: 0.0206 - val_mae: 0.1010 - val_mse: 0.0206\n",
      "Epoch 10/1000\n",
      "7524/7681 [============================>.] - ETA: 0s - loss: 0.0158 - mae: 0.0917 - mse: 0.0158\n",
      "Epoch 00010: val_loss did not improve from 0.01834\n",
      "7681/7681 [==============================] - 3s 383us/sample - loss: 0.0158 - mae: 0.0915 - mse: 0.0158 - val_loss: 0.0202 - val_mae: 0.1024 - val_mse: 0.0202\n",
      "Epoch 11/1000\n",
      "7491/7681 [============================>.] - ETA: 0s - loss: 0.0158 - mae: 0.0924 - mse: 0.0158\n",
      "Epoch 00011: val_loss did not improve from 0.01834\n",
      "7681/7681 [==============================] - 3s 388us/sample - loss: 0.0159 - mae: 0.0923 - mse: 0.0159 - val_loss: 0.0233 - val_mae: 0.1121 - val_mse: 0.0233\n",
      "Epoch 12/1000\n",
      "7535/7681 [============================>.] - ETA: 0s - loss: 0.0158 - mae: 0.0912 - mse: 0.0158\n",
      "Epoch 00012: val_loss did not improve from 0.01834\n",
      "7681/7681 [==============================] - 4s 502us/sample - loss: 0.0158 - mae: 0.0912 - mse: 0.0158 - val_loss: 0.0199 - val_mae: 0.0998 - val_mse: 0.0199\n",
      "Epoch 13/1000\n",
      "7535/7681 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0851 - mse: 0.0140\n",
      "Epoch 00013: val_loss improved from 0.01834 to 0.01618, saving model to model_checkpoint.h5\n",
      "7681/7681 [==============================] - 4s 491us/sample - loss: 0.0139 - mae: 0.0849 - mse: 0.0139 - val_loss: 0.0162 - val_mae: 0.0847 - val_mse: 0.0162\n",
      "Epoch 14/1000\n",
      "7656/7681 [============================>.] - ETA: 0s - loss: 0.0135 - mae: 0.0845 - mse: 0.0135\n",
      "Epoch 00014: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 425us/sample - loss: 0.0135 - mae: 0.0846 - mse: 0.0135 - val_loss: 0.0202 - val_mae: 0.1038 - val_mse: 0.0202\n",
      "Epoch 15/1000\n",
      "7535/7681 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0854 - mse: 0.0137\n",
      "Epoch 00015: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 430us/sample - loss: 0.0137 - mae: 0.0853 - mse: 0.0137 - val_loss: 0.0180 - val_mae: 0.0898 - val_mse: 0.0180\n",
      "Epoch 16/1000\n",
      "7667/7681 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0831 - mse: 0.0133\n",
      "Epoch 00016: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 385us/sample - loss: 0.0133 - mae: 0.0831 - mse: 0.0133 - val_loss: 0.0190 - val_mae: 0.1027 - val_mse: 0.0190\n",
      "Epoch 17/1000\n",
      "7513/7681 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0816 - mse: 0.0126\n",
      "Epoch 00017: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 412us/sample - loss: 0.0126 - mae: 0.0816 - mse: 0.0126 - val_loss: 0.0192 - val_mae: 0.0994 - val_mse: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000\n",
      "7612/7681 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0789 - mse: 0.0119\n",
      "Epoch 00018: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 389us/sample - loss: 0.0119 - mae: 0.0788 - mse: 0.0119 - val_loss: 0.0170 - val_mae: 0.0889 - val_mse: 0.0170\n",
      "Epoch 19/1000\n",
      "7524/7681 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0799 - mse: 0.0123\n",
      "Epoch 00019: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 4s 479us/sample - loss: 0.0123 - mae: 0.0798 - mse: 0.0123 - val_loss: 0.0176 - val_mae: 0.0947 - val_mse: 0.0176\n",
      "Epoch 20/1000\n",
      "7535/7681 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0769 - mse: 0.0115\n",
      "Epoch 00020: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 4s 462us/sample - loss: 0.0115 - mae: 0.0770 - mse: 0.0115 - val_loss: 0.0181 - val_mae: 0.0932 - val_mse: 0.0181\n",
      "Epoch 21/1000\n",
      "7557/7681 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0777 - mse: 0.0113\n",
      "Epoch 00021: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 388us/sample - loss: 0.0114 - mae: 0.0777 - mse: 0.0114 - val_loss: 0.0176 - val_mae: 0.0957 - val_mse: 0.0176\n",
      "Epoch 22/1000\n",
      "7546/7681 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0755 - mse: 0.0110\n",
      "Epoch 00022: val_loss did not improve from 0.01618\n",
      "7681/7681 [==============================] - 3s 382us/sample - loss: 0.0110 - mae: 0.0756 - mse: 0.0110 - val_loss: 0.0174 - val_mae: 0.0910 - val_mse: 0.0174\n",
      "Elapsed time during model training:  71.33949542045593\n",
      "Size of training set 18691\n",
      "Train on 12694 samples, validate on 5997 samples\n",
      "Epoch 1/1000\n",
      "12190/12694 [===========================>..] - ETA: 0s - loss: 0.0531 - mae: 0.1544 - mse: 0.0531\n",
      "Epoch 00001: val_loss improved from inf to 0.02054, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0520,  mae:0.1531,  mse:0.0520,  val_loss:0.0205,  val_mae:0.1011,  val_mse:0.0205,  \n",
      "12694/12694 [==============================] - 2s 122us/sample - loss: 0.0520 - mae: 0.1531 - mse: 0.0520 - val_loss: 0.0205 - val_mae: 0.1011 - val_mse: 0.0205\n",
      "Epoch 2/1000\n",
      "12052/12694 [===========================>..] - ETA: 0s - loss: 0.0216 - mae: 0.1093 - mse: 0.0216\n",
      "Epoch 00002: val_loss did not improve from 0.02054\n",
      "12694/12694 [==============================] - 1s 91us/sample - loss: 0.0215 - mae: 0.1090 - mse: 0.0215 - val_loss: 0.0206 - val_mae: 0.1002 - val_mse: 0.0206\n",
      "Epoch 3/1000\n",
      "12374/12694 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.1033 - mse: 0.0193\n",
      "Epoch 00003: val_loss did not improve from 0.02054\n",
      "12694/12694 [==============================] - 1s 80us/sample - loss: 0.0194 - mae: 0.1035 - mse: 0.0194 - val_loss: 0.0328 - val_mae: 0.1341 - val_mse: 0.0328\n",
      "Epoch 4/1000\n",
      "11868/12694 [===========================>..] - ETA: 0s - loss: 0.0179 - mae: 0.0989 - mse: 0.0179\n",
      "Epoch 00004: val_loss improved from 0.02054 to 0.01954, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 82us/sample - loss: 0.0179 - mae: 0.0987 - mse: 0.0179 - val_loss: 0.0195 - val_mae: 0.0989 - val_mse: 0.0195\n",
      "Epoch 5/1000\n",
      "12604/12694 [============================>.] - ETA: 0s - loss: 0.0175 - mae: 0.0976 - mse: 0.0175\n",
      "Epoch 00005: val_loss did not improve from 0.01954\n",
      "12694/12694 [==============================] - 1s 71us/sample - loss: 0.0175 - mae: 0.0975 - mse: 0.0175 - val_loss: 0.0206 - val_mae: 0.1012 - val_mse: 0.0206\n",
      "Epoch 6/1000\n",
      "12144/12694 [===========================>..] - ETA: 0s - loss: 0.0174 - mae: 0.0973 - mse: 0.0174\n",
      "Epoch 00006: val_loss improved from 0.01954 to 0.01887, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 71us/sample - loss: 0.0174 - mae: 0.0974 - mse: 0.0174 - val_loss: 0.0189 - val_mae: 0.0960 - val_mse: 0.0189\n",
      "Epoch 7/1000\n",
      "12420/12694 [============================>.] - ETA: 0s - loss: 0.0156 - mae: 0.0920 - mse: 0.0156\n",
      "Epoch 00007: val_loss improved from 0.01887 to 0.01718, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 78us/sample - loss: 0.0156 - mae: 0.0920 - mse: 0.0156 - val_loss: 0.0172 - val_mae: 0.0900 - val_mse: 0.0172\n",
      "Epoch 8/1000\n",
      "12098/12694 [===========================>..] - ETA: 0s - loss: 0.0162 - mae: 0.0936 - mse: 0.0162\n",
      "Epoch 00008: val_loss did not improve from 0.01718\n",
      "12694/12694 [==============================] - 1s 97us/sample - loss: 0.0162 - mae: 0.0937 - mse: 0.0162 - val_loss: 0.0202 - val_mae: 0.1027 - val_mse: 0.0202\n",
      "Epoch 9/1000\n",
      "12650/12694 [============================>.] - ETA: 0s - loss: 0.0153 - mae: 0.0920 - mse: 0.0153\n",
      "Epoch 00009: val_loss did not improve from 0.01718\n",
      "12694/12694 [==============================] - 1s 103us/sample - loss: 0.0153 - mae: 0.0919 - mse: 0.0153 - val_loss: 0.0205 - val_mae: 0.1048 - val_mse: 0.0205\n",
      "Epoch 10/1000\n",
      "11960/12694 [===========================>..] - ETA: 0s - loss: 0.0138 - mae: 0.0861 - mse: 0.0138\n",
      "Epoch 00010: val_loss did not improve from 0.01718\n",
      "12694/12694 [==============================] - 1s 89us/sample - loss: 0.0139 - mae: 0.0862 - mse: 0.0139 - val_loss: 0.0172 - val_mae: 0.0906 - val_mse: 0.0172\n",
      "Epoch 11/1000\n",
      "12006/12694 [===========================>..] - ETA: 0s - loss: 0.0133 - mae: 0.0849 - mse: 0.0133\n",
      "Epoch 00011: val_loss improved from 0.01718 to 0.01712, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 76us/sample - loss: 0.0136 - mae: 0.0857 - mse: 0.0136 - val_loss: 0.0171 - val_mae: 0.0905 - val_mse: 0.0171\n",
      "Epoch 12/1000\n",
      "11914/12694 [===========================>..] - ETA: 0s - loss: 0.0139 - mae: 0.0862 - mse: 0.0139\n",
      "Epoch 00012: val_loss did not improve from 0.01712\n",
      "12694/12694 [==============================] - 1s 70us/sample - loss: 0.0138 - mae: 0.0861 - mse: 0.0138 - val_loss: 0.0176 - val_mae: 0.0922 - val_mse: 0.0176\n",
      "Epoch 13/1000\n",
      "12006/12694 [===========================>..] - ETA: 0s - loss: 0.0132 - mae: 0.0840 - mse: 0.0132\n",
      "Epoch 00013: val_loss improved from 0.01712 to 0.01697, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 74us/sample - loss: 0.0132 - mae: 0.0838 - mse: 0.0132 - val_loss: 0.0170 - val_mae: 0.0875 - val_mse: 0.0170\n",
      "Epoch 14/1000\n",
      "12512/12694 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0824 - mse: 0.0125\n",
      "Epoch 00014: val_loss improved from 0.01697 to 0.01687, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 84us/sample - loss: 0.0125 - mae: 0.0823 - mse: 0.0125 - val_loss: 0.0169 - val_mae: 0.0881 - val_mse: 0.0169\n",
      "Epoch 15/1000\n",
      "11776/12694 [==========================>...] - ETA: 0s - loss: 0.0115 - mae: 0.0781 - mse: 0.0115\n",
      "Epoch 00015: val_loss improved from 0.01687 to 0.01652, saving model to model_checkpoint.h5\n",
      "12694/12694 [==============================] - 1s 77us/sample - loss: 0.0116 - mae: 0.0788 - mse: 0.0116 - val_loss: 0.0165 - val_mae: 0.0913 - val_mse: 0.0165\n",
      "Epoch 16/1000\n",
      "12006/12694 [===========================>..] - ETA: 0s - loss: 0.0116 - mae: 0.0784 - mse: 0.0116\n",
      "Epoch 00016: val_loss did not improve from 0.01652\n",
      "12694/12694 [==============================] - 1s 69us/sample - loss: 0.0116 - mae: 0.0785 - mse: 0.0116 - val_loss: 0.0167 - val_mae: 0.0858 - val_mse: 0.0167\n",
      "Epoch 17/1000\n",
      "11960/12694 [===========================>..] - ETA: 0s - loss: 0.0116 - mae: 0.0786 - mse: 0.0116\n",
      "Epoch 00017: val_loss did not improve from 0.01652\n",
      "12694/12694 [==============================] - 1s 69us/sample - loss: 0.0116 - mae: 0.0786 - mse: 0.0116 - val_loss: 0.0175 - val_mae: 0.0882 - val_mse: 0.0175\n",
      "Epoch 18/1000\n",
      "12282/12694 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0795 - mse: 0.0117\n",
      "Epoch 00018: val_loss did not improve from 0.01652\n",
      "12694/12694 [==============================] - 1s 67us/sample - loss: 0.0117 - mae: 0.0795 - mse: 0.0117 - val_loss: 0.0193 - val_mae: 0.0928 - val_mse: 0.0193\n",
      "Epoch 19/1000\n",
      "12604/12694 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0750 - mse: 0.0108\n",
      "Epoch 00019: val_loss did not improve from 0.01652\n",
      "12694/12694 [==============================] - 1s 72us/sample - loss: 0.0108 - mae: 0.0749 - mse: 0.0108 - val_loss: 0.0192 - val_mae: 0.0923 - val_mse: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000\n",
      "12052/12694 [===========================>..] - ETA: 0s - loss: 0.0101 - mae: 0.0728 - mse: 0.0101\n",
      "Epoch 00020: val_loss did not improve from 0.01652\n",
      "12694/12694 [==============================] - 1s 69us/sample - loss: 0.0102 - mae: 0.0730 - mse: 0.0102 - val_loss: 0.0193 - val_mae: 0.0976 - val_mse: 0.0193\n",
      "Elapsed time during model training:  20.83836030960083\n",
      "Size of training set 18691\n",
      "Train on 12312 samples, validate on 6379 samples\n",
      "Epoch 1/1000\n",
      "12240/12312 [============================>.] - ETA: 0s - loss: 0.0489 - mae: 0.1524 - mse: 0.0489\n",
      "Epoch 00001: val_loss improved from inf to 0.02305, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0487,  mae:0.1521,  mse:0.0487,  val_loss:0.0231,  val_mae:0.1105,  val_mse:0.0231,  \n",
      "12312/12312 [==============================] - 2s 130us/sample - loss: 0.0487 - mae: 0.1521 - mse: 0.0487 - val_loss: 0.0231 - val_mae: 0.1105 - val_mse: 0.0231\n",
      "Epoch 2/1000\n",
      "11520/12312 [===========================>..] - ETA: 0s - loss: 0.0209 - mae: 0.1073 - mse: 0.0209\n",
      "Epoch 00002: val_loss did not improve from 0.02305\n",
      "12312/12312 [==============================] - 1s 74us/sample - loss: 0.0210 - mae: 0.1076 - mse: 0.0210 - val_loss: 0.0255 - val_mae: 0.1142 - val_mse: 0.0255\n",
      "Epoch 3/1000\n",
      "12015/12312 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.1037 - mse: 0.0195\n",
      "Epoch 00003: val_loss improved from 0.02305 to 0.02187, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 73us/sample - loss: 0.0196 - mae: 0.1039 - mse: 0.0196 - val_loss: 0.0219 - val_mae: 0.1041 - val_mse: 0.0219\n",
      "Epoch 4/1000\n",
      "11970/12312 [============================>.] - ETA: 0s - loss: 0.0176 - mae: 0.0979 - mse: 0.0176\n",
      "Epoch 00004: val_loss did not improve from 0.02187\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0177 - mae: 0.0981 - mse: 0.0177 - val_loss: 0.0255 - val_mae: 0.1189 - val_mse: 0.0255\n",
      "Epoch 5/1000\n",
      "11880/12312 [===========================>..] - ETA: 0s - loss: 0.0169 - mae: 0.0951 - mse: 0.0169\n",
      "Epoch 00005: val_loss did not improve from 0.02187\n",
      "12312/12312 [==============================] - 1s 93us/sample - loss: 0.0168 - mae: 0.0951 - mse: 0.0168 - val_loss: 0.0222 - val_mae: 0.1155 - val_mse: 0.0222\n",
      "Epoch 6/1000\n",
      "12105/12312 [============================>.] - ETA: 0s - loss: 0.0151 - mae: 0.0907 - mse: 0.0151\n",
      "Epoch 00006: val_loss improved from 0.02187 to 0.01729, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 89us/sample - loss: 0.0152 - mae: 0.0909 - mse: 0.0152 - val_loss: 0.0173 - val_mae: 0.0926 - val_mse: 0.0173\n",
      "Epoch 7/1000\n",
      "11925/12312 [============================>.] - ETA: 0s - loss: 0.0162 - mae: 0.0939 - mse: 0.0162\n",
      "Epoch 00007: val_loss did not improve from 0.01729\n",
      "12312/12312 [==============================] - 1s 78us/sample - loss: 0.0162 - mae: 0.0938 - mse: 0.0162 - val_loss: 0.0218 - val_mae: 0.1081 - val_mse: 0.0218\n",
      "Epoch 8/1000\n",
      "12195/12312 [============================>.] - ETA: 0s - loss: 0.0145 - mae: 0.0883 - mse: 0.0145\n",
      "Epoch 00008: val_loss improved from 0.01729 to 0.01710, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 77us/sample - loss: 0.0145 - mae: 0.0884 - mse: 0.0145 - val_loss: 0.0171 - val_mae: 0.0904 - val_mse: 0.0171\n",
      "Epoch 9/1000\n",
      "11700/12312 [===========================>..] - ETA: 0s - loss: 0.0148 - mae: 0.0893 - mse: 0.0148\n",
      "Epoch 00009: val_loss improved from 0.01710 to 0.01705, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 75us/sample - loss: 0.0146 - mae: 0.0885 - mse: 0.0146 - val_loss: 0.0170 - val_mae: 0.0895 - val_mse: 0.0170\n",
      "Epoch 10/1000\n",
      "11700/12312 [===========================>..] - ETA: 0s - loss: 0.0141 - mae: 0.0875 - mse: 0.0141\n",
      "Epoch 00010: val_loss did not improve from 0.01705\n",
      "12312/12312 [==============================] - 1s 73us/sample - loss: 0.0143 - mae: 0.0880 - mse: 0.0143 - val_loss: 0.0182 - val_mae: 0.0950 - val_mse: 0.0182\n",
      "Epoch 11/1000\n",
      "11655/12312 [===========================>..] - ETA: 0s - loss: 0.0136 - mae: 0.0854 - mse: 0.0136\n",
      "Epoch 00011: val_loss did not improve from 0.01705\n",
      "12312/12312 [==============================] - 1s 73us/sample - loss: 0.0136 - mae: 0.0854 - mse: 0.0136 - val_loss: 0.0183 - val_mae: 0.0923 - val_mse: 0.0183\n",
      "Epoch 12/1000\n",
      "11385/12312 [==========================>...] - ETA: 0s - loss: 0.0125 - mae: 0.0809 - mse: 0.0125\n",
      "Epoch 00012: val_loss improved from 0.01705 to 0.01649, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 79us/sample - loss: 0.0124 - mae: 0.0808 - mse: 0.0124 - val_loss: 0.0165 - val_mae: 0.0848 - val_mse: 0.0165\n",
      "Epoch 13/1000\n",
      "12150/12312 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0842 - mse: 0.0133\n",
      "Epoch 00013: val_loss did not improve from 0.01649\n",
      "12312/12312 [==============================] - 1s 70us/sample - loss: 0.0133 - mae: 0.0842 - mse: 0.0133 - val_loss: 0.0177 - val_mae: 0.0936 - val_mse: 0.0177\n",
      "Epoch 14/1000\n",
      "11880/12312 [===========================>..] - ETA: 0s - loss: 0.0115 - mae: 0.0780 - mse: 0.0115\n",
      "Epoch 00014: val_loss improved from 0.01649 to 0.01586, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 73us/sample - loss: 0.0116 - mae: 0.0783 - mse: 0.0116 - val_loss: 0.0159 - val_mae: 0.0830 - val_mse: 0.0159\n",
      "Epoch 15/1000\n",
      "12105/12312 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0777 - mse: 0.0115\n",
      "Epoch 00015: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 78us/sample - loss: 0.0115 - mae: 0.0779 - mse: 0.0115 - val_loss: 0.0165 - val_mae: 0.0874 - val_mse: 0.0165\n",
      "Epoch 16/1000\n",
      "11790/12312 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0804 - mse: 0.0119\n",
      "Epoch 00016: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 83us/sample - loss: 0.0119 - mae: 0.0804 - mse: 0.0119 - val_loss: 0.0193 - val_mae: 0.0937 - val_mse: 0.0193\n",
      "Epoch 17/1000\n",
      "11880/12312 [===========================>..] - ETA: 0s - loss: 0.0111 - mae: 0.0766 - mse: 0.0111\n",
      "Epoch 00017: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 83us/sample - loss: 0.0112 - mae: 0.0768 - mse: 0.0112 - val_loss: 0.0199 - val_mae: 0.1032 - val_mse: 0.0199\n",
      "Epoch 18/1000\n",
      "11790/12312 [===========================>..] - ETA: 0s - loss: 0.0107 - mae: 0.0755 - mse: 0.0107\n",
      "Epoch 00018: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0107 - mae: 0.0759 - mse: 0.0107 - val_loss: 0.0185 - val_mae: 0.0955 - val_mse: 0.0185\n",
      "Epoch 19/1000\n",
      "12060/12312 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0775 - mse: 0.0111\n",
      "Epoch 00019: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0110 - mae: 0.0775 - mse: 0.0110 - val_loss: 0.0172 - val_mae: 0.0910 - val_mse: 0.0172\n",
      "Epoch 20/1000\n",
      "12150/12312 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0728 - mse: 0.0101\n",
      "Epoch 00020: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 70us/sample - loss: 0.0101 - mae: 0.0729 - mse: 0.0101 - val_loss: 0.0160 - val_mae: 0.0879 - val_mse: 0.0160\n",
      "Epoch 21/1000\n",
      "12105/12312 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0751 - mse: 0.0106\n",
      "Epoch 00021: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 76us/sample - loss: 0.0107 - mae: 0.0753 - mse: 0.0107 - val_loss: 0.0193 - val_mae: 0.0978 - val_mse: 0.0193\n",
      "Epoch 22/1000\n",
      "11520/12312 [===========================>..] - ETA: 0s - loss: 0.0093 - mae: 0.0700 - mse: 0.0093\n",
      "Epoch 00022: val_loss did not improve from 0.01586\n",
      "12312/12312 [==============================] - 1s 78us/sample - loss: 0.0094 - mae: 0.0703 - mse: 0.0094 - val_loss: 0.0161 - val_mae: 0.0872 - val_mse: 0.0161\n",
      "Epoch 23/1000\n",
      "12195/12312 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0697 - mse: 0.0092\n",
      "Epoch 00023: val_loss improved from 0.01586 to 0.01557, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 100us/sample - loss: 0.0092 - mae: 0.0697 - mse: 0.0092 - val_loss: 0.0156 - val_mae: 0.0817 - val_mse: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000\n",
      "12060/12312 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0712 - mse: 0.0093\n",
      "Epoch 00024: val_loss did not improve from 0.01557\n",
      "12312/12312 [==============================] - 1s 86us/sample - loss: 0.0094 - mae: 0.0714 - mse: 0.0094 - val_loss: 0.0160 - val_mae: 0.0844 - val_mse: 0.0160\n",
      "Epoch 25/1000\n",
      "11880/12312 [===========================>..] - ETA: 0s - loss: 0.0085 - mae: 0.0674 - mse: 0.0085\n",
      "Epoch 00025: val_loss improved from 0.01557 to 0.01541, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 84us/sample - loss: 0.0086 - mae: 0.0675 - mse: 0.0086 - val_loss: 0.0154 - val_mae: 0.0843 - val_mse: 0.0154\n",
      "Epoch 26/1000\n",
      "11610/12312 [===========================>..] - ETA: 0s - loss: 0.0084 - mae: 0.0669 - mse: 0.0084\n",
      "Epoch 00026: val_loss improved from 0.01541 to 0.01462, saving model to model_checkpoint.h5\n",
      "12312/12312 [==============================] - 1s 75us/sample - loss: 0.0085 - mae: 0.0669 - mse: 0.0085 - val_loss: 0.0146 - val_mae: 0.0786 - val_mse: 0.0146\n",
      "Epoch 27/1000\n",
      "11970/12312 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0654 - mse: 0.0081\n",
      "Epoch 00027: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0081 - mae: 0.0657 - mse: 0.0081 - val_loss: 0.0168 - val_mae: 0.0931 - val_mse: 0.0168\n",
      "Epoch 28/1000\n",
      "11925/12312 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0660 - mse: 0.0082\n",
      "Epoch 00028: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0082 - mae: 0.0660 - mse: 0.0082 - val_loss: 0.0177 - val_mae: 0.0913 - val_mse: 0.0177\n",
      "Epoch 29/1000\n",
      "12150/12312 [============================>.] - ETA: 0s - loss: 0.0084 - mae: 0.0677 - mse: 0.0084\n",
      "Epoch 00029: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 70us/sample - loss: 0.0084 - mae: 0.0676 - mse: 0.0084 - val_loss: 0.0154 - val_mae: 0.0817 - val_mse: 0.0154\n",
      "Epoch 30/1000\n",
      "11835/12312 [===========================>..] - ETA: 0s - loss: 0.0082 - mae: 0.0666 - mse: 0.0082\n",
      "Epoch 00030: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 72us/sample - loss: 0.0082 - mae: 0.0667 - mse: 0.0082 - val_loss: 0.0177 - val_mae: 0.0918 - val_mse: 0.0177\n",
      "Epoch 31/1000\n",
      "11790/12312 [===========================>..] - ETA: 0s - loss: 0.0076 - mae: 0.0638 - mse: 0.0076\n",
      "Epoch 00031: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 72us/sample - loss: 0.0076 - mae: 0.0637 - mse: 0.0076 - val_loss: 0.0152 - val_mae: 0.0796 - val_mse: 0.0152\n",
      "Epoch 32/1000\n",
      "11655/12312 [===========================>..] - ETA: 0s - loss: 0.0070 - mae: 0.0609 - mse: 0.0070\n",
      "Epoch 00032: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 72us/sample - loss: 0.0071 - mae: 0.0611 - mse: 0.0071 - val_loss: 0.0147 - val_mae: 0.0810 - val_mse: 0.0147\n",
      "Epoch 33/1000\n",
      "11655/12312 [===========================>..] - ETA: 0s - loss: 0.0072 - mae: 0.0620 - mse: 0.0072\n",
      "Epoch 00033: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 72us/sample - loss: 0.0072 - mae: 0.0618 - mse: 0.0072 - val_loss: 0.0148 - val_mae: 0.0793 - val_mse: 0.0148\n",
      "Epoch 34/1000\n",
      "12105/12312 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0592 - mse: 0.0067\n",
      "Epoch 00034: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 70us/sample - loss: 0.0067 - mae: 0.0594 - mse: 0.0067 - val_loss: 0.0158 - val_mae: 0.0851 - val_mse: 0.0158\n",
      "Epoch 35/1000\n",
      "11970/12312 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0601 - mse: 0.0069\n",
      "Epoch 00035: val_loss did not improve from 0.01462\n",
      "12312/12312 [==============================] - 1s 71us/sample - loss: 0.0069 - mae: 0.0604 - mse: 0.0069 - val_loss: 0.0154 - val_mae: 0.0825 - val_mse: 0.0154\n",
      "Elapsed time during model training:  33.90823769569397\n",
      "Size of training set 18691\n",
      "Train on 13303 samples, validate on 5388 samples\n",
      "Epoch 1/1000\n",
      "13152/13303 [============================>.] - ETA: 0s - loss: 0.0468 - mae: 0.1522 - mse: 0.0468\n",
      "Epoch 00001: val_loss improved from inf to 0.02448, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0464,  mae:0.1516,  mse:0.0464,  val_loss:0.0245,  val_mae:0.1165,  val_mse:0.0245,  \n",
      "13303/13303 [==============================] - 2s 129us/sample - loss: 0.0464 - mae: 0.1516 - mse: 0.0464 - val_loss: 0.0245 - val_mae: 0.1165 - val_mse: 0.0245\n",
      "Epoch 2/1000\n",
      "12928/13303 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.1153 - mse: 0.0240\n",
      "Epoch 00002: val_loss improved from 0.02448 to 0.01965, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 94us/sample - loss: 0.0239 - mae: 0.1149 - mse: 0.0239 - val_loss: 0.0196 - val_mae: 0.1031 - val_mse: 0.0196\n",
      "Epoch 3/1000\n",
      "12800/13303 [===========================>..] - ETA: 0s - loss: 0.0213 - mae: 0.1081 - mse: 0.0213\n",
      "Epoch 00003: val_loss did not improve from 0.01965\n",
      "13303/13303 [==============================] - 1s 93us/sample - loss: 0.0213 - mae: 0.1080 - mse: 0.0213 - val_loss: 0.0235 - val_mae: 0.1188 - val_mse: 0.0235\n",
      "Epoch 4/1000\n",
      "13024/13303 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.1033 - mse: 0.0199\n",
      "Epoch 00004: val_loss improved from 0.01965 to 0.01938, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 99us/sample - loss: 0.0197 - mae: 0.1029 - mse: 0.0197 - val_loss: 0.0194 - val_mae: 0.1028 - val_mse: 0.0194\n",
      "Epoch 5/1000\n",
      "13152/13303 [============================>.] - ETA: 0s - loss: 0.0179 - mae: 0.0976 - mse: 0.0179\n",
      "Epoch 00005: val_loss improved from 0.01938 to 0.01678, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 2s 118us/sample - loss: 0.0180 - mae: 0.0978 - mse: 0.0180 - val_loss: 0.0168 - val_mae: 0.0916 - val_mse: 0.0168\n",
      "Epoch 6/1000\n",
      "12832/13303 [===========================>..] - ETA: 0s - loss: 0.0178 - mae: 0.0974 - mse: 0.0178\n",
      "Epoch 00006: val_loss did not improve from 0.01678\n",
      "13303/13303 [==============================] - 2s 130us/sample - loss: 0.0178 - mae: 0.0975 - mse: 0.0178 - val_loss: 0.0213 - val_mae: 0.1092 - val_mse: 0.0213\n",
      "Epoch 7/1000\n",
      "13056/13303 [============================>.] - ETA: 0s - loss: 0.0170 - mae: 0.0948 - mse: 0.0170\n",
      "Epoch 00007: val_loss did not improve from 0.01678\n",
      "13303/13303 [==============================] - 1s 111us/sample - loss: 0.0170 - mae: 0.0949 - mse: 0.0170 - val_loss: 0.0171 - val_mae: 0.0910 - val_mse: 0.0171\n",
      "Epoch 8/1000\n",
      "13248/13303 [============================>.] - ETA: 0s - loss: 0.0157 - mae: 0.0912 - mse: 0.0157\n",
      "Epoch 00008: val_loss improved from 0.01678 to 0.01677, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 97us/sample - loss: 0.0157 - mae: 0.0912 - mse: 0.0157 - val_loss: 0.0168 - val_mae: 0.0924 - val_mse: 0.0168\n",
      "Epoch 9/1000\n",
      "12672/13303 [===========================>..] - ETA: 0s - loss: 0.0164 - mae: 0.0930 - mse: 0.0164\n",
      "Epoch 00009: val_loss did not improve from 0.01677\n",
      "13303/13303 [==============================] - 1s 94us/sample - loss: 0.0165 - mae: 0.0935 - mse: 0.0165 - val_loss: 0.0195 - val_mae: 0.1031 - val_mse: 0.0195\n",
      "Epoch 10/1000\n",
      "13056/13303 [============================>.] - ETA: 0s - loss: 0.0151 - mae: 0.0889 - mse: 0.0151\n",
      "Epoch 00010: val_loss did not improve from 0.01677\n",
      "13303/13303 [==============================] - 1s 91us/sample - loss: 0.0151 - mae: 0.0888 - mse: 0.0151 - val_loss: 0.0232 - val_mae: 0.1071 - val_mse: 0.0232\n",
      "Epoch 11/1000\n",
      "13088/13303 [============================>.] - ETA: 0s - loss: 0.0150 - mae: 0.0884 - mse: 0.0150\n",
      "Epoch 00011: val_loss did not improve from 0.01677\n",
      "13303/13303 [==============================] - 1s 100us/sample - loss: 0.0150 - mae: 0.0886 - mse: 0.0150 - val_loss: 0.0192 - val_mae: 0.1062 - val_mse: 0.0192\n",
      "Epoch 12/1000\n",
      "12736/13303 [===========================>..] - ETA: 0s - loss: 0.0139 - mae: 0.0850 - mse: 0.0139\n",
      "Epoch 00012: val_loss improved from 0.01677 to 0.01661, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 96us/sample - loss: 0.0140 - mae: 0.0852 - mse: 0.0140 - val_loss: 0.0166 - val_mae: 0.0906 - val_mse: 0.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000\n",
      "12736/13303 [===========================>..] - ETA: 0s - loss: 0.0145 - mae: 0.0873 - mse: 0.0145\n",
      "Epoch 00013: val_loss improved from 0.01661 to 0.01638, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 95us/sample - loss: 0.0144 - mae: 0.0870 - mse: 0.0144 - val_loss: 0.0164 - val_mae: 0.0931 - val_mse: 0.0164\n",
      "Epoch 14/1000\n",
      "12672/13303 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0829 - mse: 0.0131\n",
      "Epoch 00014: val_loss did not improve from 0.01638\n",
      "13303/13303 [==============================] - 1s 94us/sample - loss: 0.0131 - mae: 0.0829 - mse: 0.0131 - val_loss: 0.0172 - val_mae: 0.0931 - val_mse: 0.0172\n",
      "Epoch 15/1000\n",
      "12736/13303 [===========================>..] - ETA: 0s - loss: 0.0130 - mae: 0.0816 - mse: 0.0130\n",
      "Epoch 00015: val_loss improved from 0.01638 to 0.01475, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 95us/sample - loss: 0.0130 - mae: 0.0815 - mse: 0.0130 - val_loss: 0.0148 - val_mae: 0.0853 - val_mse: 0.0148\n",
      "Epoch 16/1000\n",
      "12768/13303 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0819 - mse: 0.0131\n",
      "Epoch 00016: val_loss did not improve from 0.01475\n",
      "13303/13303 [==============================] - 1s 93us/sample - loss: 0.0130 - mae: 0.0818 - mse: 0.0130 - val_loss: 0.0148 - val_mae: 0.0833 - val_mse: 0.0148\n",
      "Epoch 17/1000\n",
      "12672/13303 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0776 - mse: 0.0119\n",
      "Epoch 00017: val_loss did not improve from 0.01475\n",
      "13303/13303 [==============================] - 1s 93us/sample - loss: 0.0118 - mae: 0.0774 - mse: 0.0118 - val_loss: 0.0148 - val_mae: 0.0820 - val_mse: 0.0148\n",
      "Epoch 18/1000\n",
      "12864/13303 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0791 - mse: 0.0121\n",
      "Epoch 00018: val_loss improved from 0.01475 to 0.01416, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 95us/sample - loss: 0.0121 - mae: 0.0790 - mse: 0.0121 - val_loss: 0.0142 - val_mae: 0.0810 - val_mse: 0.0142\n",
      "Epoch 19/1000\n",
      "12928/13303 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0762 - mse: 0.0115\n",
      "Epoch 00019: val_loss did not improve from 0.01416\n",
      "13303/13303 [==============================] - 1s 93us/sample - loss: 0.0114 - mae: 0.0762 - mse: 0.0114 - val_loss: 0.0159 - val_mae: 0.0877 - val_mse: 0.0159\n",
      "Epoch 20/1000\n",
      "13184/13303 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0764 - mse: 0.0114\n",
      "Epoch 00020: val_loss did not improve from 0.01416\n",
      "13303/13303 [==============================] - 1s 108us/sample - loss: 0.0114 - mae: 0.0764 - mse: 0.0114 - val_loss: 0.0162 - val_mae: 0.0896 - val_mse: 0.0162\n",
      "Epoch 21/1000\n",
      "12768/13303 [===========================>..] - ETA: 0s - loss: 0.0112 - mae: 0.0751 - mse: 0.0112\n",
      "Epoch 00021: val_loss did not improve from 0.01416\n",
      "13303/13303 [==============================] - 2s 119us/sample - loss: 0.0112 - mae: 0.0752 - mse: 0.0112 - val_loss: 0.0149 - val_mae: 0.0820 - val_mse: 0.0149\n",
      "Epoch 22/1000\n",
      "13056/13303 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0754 - mse: 0.0112\n",
      "Epoch 00022: val_loss improved from 0.01416 to 0.01414, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 2s 125us/sample - loss: 0.0112 - mae: 0.0754 - mse: 0.0112 - val_loss: 0.0141 - val_mae: 0.0840 - val_mse: 0.0141\n",
      "Epoch 23/1000\n",
      "12608/13303 [===========================>..] - ETA: 0s - loss: 0.0107 - mae: 0.0735 - mse: 0.0107\n",
      "Epoch 00023: val_loss did not improve from 0.01414\n",
      "13303/13303 [==============================] - 1s 94us/sample - loss: 0.0107 - mae: 0.0734 - mse: 0.0107 - val_loss: 0.0146 - val_mae: 0.0832 - val_mse: 0.0146\n",
      "Epoch 24/1000\n",
      "13280/13303 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0731 - mse: 0.0105\n",
      "Epoch 00024: val_loss did not improve from 0.01414\n",
      "13303/13303 [==============================] - 1s 96us/sample - loss: 0.0105 - mae: 0.0731 - mse: 0.0105 - val_loss: 0.0142 - val_mae: 0.0818 - val_mse: 0.0142\n",
      "Epoch 25/1000\n",
      "13152/13303 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0720 - mse: 0.0102\n",
      "Epoch 00025: val_loss did not improve from 0.01414\n",
      "13303/13303 [==============================] - 2s 118us/sample - loss: 0.0102 - mae: 0.0720 - mse: 0.0102 - val_loss: 0.0148 - val_mae: 0.0844 - val_mse: 0.0148\n",
      "Epoch 26/1000\n",
      "13184/13303 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0690 - mse: 0.0095\n",
      "Epoch 00026: val_loss did not improve from 0.01414\n",
      "13303/13303 [==============================] - 1s 110us/sample - loss: 0.0095 - mae: 0.0690 - mse: 0.0095 - val_loss: 0.0144 - val_mae: 0.0835 - val_mse: 0.0144\n",
      "Epoch 27/1000\n",
      "12832/13303 [===========================>..] - ETA: 0s - loss: 0.0097 - mae: 0.0704 - mse: 0.0097\n",
      "Epoch 00027: val_loss did not improve from 0.01414\n",
      "13303/13303 [==============================] - 1s 92us/sample - loss: 0.0096 - mae: 0.0703 - mse: 0.0096 - val_loss: 0.0143 - val_mae: 0.0803 - val_mse: 0.0143\n",
      "Epoch 28/1000\n",
      "12640/13303 [===========================>..] - ETA: 0s - loss: 0.0093 - mae: 0.0681 - mse: 0.0093\n",
      "Epoch 00028: val_loss improved from 0.01414 to 0.01369, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 96us/sample - loss: 0.0093 - mae: 0.0683 - mse: 0.0093 - val_loss: 0.0137 - val_mae: 0.0782 - val_mse: 0.0137\n",
      "Epoch 29/1000\n",
      "12992/13303 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0698 - mse: 0.0097\n",
      "Epoch 00029: val_loss did not improve from 0.01369\n",
      "13303/13303 [==============================] - 1s 91us/sample - loss: 0.0096 - mae: 0.0697 - mse: 0.0096 - val_loss: 0.0145 - val_mae: 0.0846 - val_mse: 0.0145\n",
      "Epoch 30/1000\n",
      "12608/13303 [===========================>..] - ETA: 0s - loss: 0.0091 - mae: 0.0673 - mse: 0.0091\n",
      "Epoch 00030: val_loss improved from 0.01369 to 0.01295, saving model to model_checkpoint.h5\n",
      "13303/13303 [==============================] - 1s 96us/sample - loss: 0.0091 - mae: 0.0673 - mse: 0.0091 - val_loss: 0.0130 - val_mae: 0.0771 - val_mse: 0.0130\n",
      "Epoch 31/1000\n",
      "12672/13303 [===========================>..] - ETA: 0s - loss: 0.0088 - mae: 0.0662 - mse: 0.0088\n",
      "Epoch 00031: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 94us/sample - loss: 0.0088 - mae: 0.0663 - mse: 0.0088 - val_loss: 0.0168 - val_mae: 0.0910 - val_mse: 0.0168\n",
      "Epoch 32/1000\n",
      "12672/13303 [===========================>..] - ETA: 0s - loss: 0.0094 - mae: 0.0687 - mse: 0.0094\n",
      "Epoch 00032: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 93us/sample - loss: 0.0094 - mae: 0.0690 - mse: 0.0094 - val_loss: 0.0147 - val_mae: 0.0862 - val_mse: 0.0147\n",
      "Epoch 33/1000\n",
      "12992/13303 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0644 - mse: 0.0083\n",
      "Epoch 00033: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 96us/sample - loss: 0.0083 - mae: 0.0645 - mse: 0.0083 - val_loss: 0.0171 - val_mae: 0.0921 - val_mse: 0.0171\n",
      "Epoch 34/1000\n",
      "13024/13303 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0651 - mse: 0.0085\n",
      "Epoch 00034: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 104us/sample - loss: 0.0084 - mae: 0.0651 - mse: 0.0084 - val_loss: 0.0131 - val_mae: 0.0770 - val_mse: 0.0131\n",
      "Epoch 35/1000\n",
      "12832/13303 [===========================>..] - ETA: 0s - loss: 0.0083 - mae: 0.0646 - mse: 0.0083\n",
      "Epoch 00035: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 98us/sample - loss: 0.0082 - mae: 0.0645 - mse: 0.0082 - val_loss: 0.0139 - val_mae: 0.0781 - val_mse: 0.0139\n",
      "Epoch 36/1000\n",
      "13088/13303 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0657 - mse: 0.0087\n",
      "Epoch 00036: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 2s 125us/sample - loss: 0.0086 - mae: 0.0656 - mse: 0.0086 - val_loss: 0.0149 - val_mae: 0.0811 - val_mse: 0.0149\n",
      "Epoch 37/1000\n",
      "12960/13303 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0612 - mse: 0.0076\n",
      "Epoch 00037: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 100us/sample - loss: 0.0076 - mae: 0.0613 - mse: 0.0076 - val_loss: 0.0139 - val_mae: 0.0790 - val_mse: 0.0139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000\n",
      "12896/13303 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0609 - mse: 0.0074\n",
      "Epoch 00038: val_loss did not improve from 0.01295\n",
      "13303/13303 [==============================] - 1s 98us/sample - loss: 0.0074 - mae: 0.0608 - mse: 0.0074 - val_loss: 0.0133 - val_mae: 0.0793 - val_mse: 0.0133\n",
      "Elapsed time during model training:  51.78126239776611\n",
      "Size of training set 18691\n",
      "Train on 18040 samples, validate on 651 samples\n",
      "Epoch 1/1000\n",
      "17892/18040 [============================>.] - ETA: 0s - loss: 0.0403 - mae: 0.1425 - mse: 0.0403\n",
      "Epoch 00001: val_loss improved from inf to 0.02614, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0402,  mae:0.1422,  mse:0.0402,  val_loss:0.0261,  val_mae:0.1205,  val_mse:0.0261,  \n",
      "18040/18040 [==============================] - 3s 156us/sample - loss: 0.0402 - mae: 0.1422 - mse: 0.0402 - val_loss: 0.0261 - val_mae: 0.1205 - val_mse: 0.0261\n",
      "Epoch 2/1000\n",
      "17619/18040 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.1099 - mse: 0.0224\n",
      "Epoch 00002: val_loss improved from 0.02614 to 0.02049, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 113us/sample - loss: 0.0225 - mae: 0.1101 - mse: 0.0225 - val_loss: 0.0205 - val_mae: 0.1031 - val_mse: 0.0205\n",
      "Epoch 3/1000\n",
      "17850/18040 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.1050 - mse: 0.0208\n",
      "Epoch 00003: val_loss did not improve from 0.02049\n",
      "18040/18040 [==============================] - 2s 114us/sample - loss: 0.0207 - mae: 0.1049 - mse: 0.0207 - val_loss: 0.0209 - val_mae: 0.1041 - val_mse: 0.0209\n",
      "Epoch 4/1000\n",
      "18039/18040 [============================>.] - ETA: 0s - loss: 0.0188 - mae: 0.1000 - mse: 0.0188\n",
      "Epoch 00004: val_loss did not improve from 0.02049\n",
      "18040/18040 [==============================] - 2s 112us/sample - loss: 0.0188 - mae: 0.1000 - mse: 0.0188 - val_loss: 0.0232 - val_mae: 0.1099 - val_mse: 0.0232\n",
      "Epoch 5/1000\n",
      "17829/18040 [============================>.] - ETA: 0s - loss: 0.0182 - mae: 0.0982 - mse: 0.0182\n",
      "Epoch 00005: val_loss improved from 0.02049 to 0.01981, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 111us/sample - loss: 0.0182 - mae: 0.0982 - mse: 0.0182 - val_loss: 0.0198 - val_mae: 0.0987 - val_mse: 0.0198\n",
      "Epoch 6/1000\n",
      "17724/18040 [============================>.] - ETA: 0s - loss: 0.0174 - mae: 0.0957 - mse: 0.0174\n",
      "Epoch 00006: val_loss did not improve from 0.01981\n",
      "18040/18040 [==============================] - 3s 154us/sample - loss: 0.0174 - mae: 0.0956 - mse: 0.0174 - val_loss: 0.0291 - val_mae: 0.1282 - val_mse: 0.0291\n",
      "Epoch 7/1000\n",
      "18018/18040 [============================>.] - ETA: 0s - loss: 0.0168 - mae: 0.0930 - mse: 0.0168\n",
      "Epoch 00007: val_loss improved from 0.01981 to 0.01719, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 3s 155us/sample - loss: 0.0168 - mae: 0.0930 - mse: 0.0168 - val_loss: 0.0172 - val_mae: 0.0901 - val_mse: 0.0172\n",
      "Epoch 8/1000\n",
      "17892/18040 [============================>.] - ETA: 0s - loss: 0.0159 - mae: 0.0901 - mse: 0.0159\n",
      "Epoch 00008: val_loss did not improve from 0.01719\n",
      "18040/18040 [==============================] - 3s 139us/sample - loss: 0.0159 - mae: 0.0901 - mse: 0.0159 - val_loss: 0.0219 - val_mae: 0.1035 - val_mse: 0.0219\n",
      "Epoch 9/1000\n",
      "17955/18040 [============================>.] - ETA: 0s - loss: 0.0150 - mae: 0.0870 - mse: 0.0150\n",
      "Epoch 00009: val_loss improved from 0.01719 to 0.01683, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 114us/sample - loss: 0.0150 - mae: 0.0871 - mse: 0.0150 - val_loss: 0.0168 - val_mae: 0.0940 - val_mse: 0.0168\n",
      "Epoch 10/1000\n",
      "17997/18040 [============================>.] - ETA: 0s - loss: 0.0145 - mae: 0.0860 - mse: 0.0145\n",
      "Epoch 00010: val_loss did not improve from 0.01683\n",
      "18040/18040 [==============================] - 2s 118us/sample - loss: 0.0145 - mae: 0.0860 - mse: 0.0145 - val_loss: 0.0201 - val_mae: 0.0985 - val_mse: 0.0201\n",
      "Epoch 11/1000\n",
      "17871/18040 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0838 - mse: 0.0140\n",
      "Epoch 00011: val_loss did not improve from 0.01683\n",
      "18040/18040 [==============================] - 2s 118us/sample - loss: 0.0140 - mae: 0.0838 - mse: 0.0140 - val_loss: 0.0192 - val_mae: 0.0987 - val_mse: 0.0192\n",
      "Epoch 12/1000\n",
      "17955/18040 [============================>.] - ETA: 0s - loss: 0.0141 - mae: 0.0849 - mse: 0.0141\n",
      "Epoch 00012: val_loss improved from 0.01683 to 0.01583, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 117us/sample - loss: 0.0141 - mae: 0.0849 - mse: 0.0141 - val_loss: 0.0158 - val_mae: 0.0802 - val_mse: 0.0158\n",
      "Epoch 13/1000\n",
      "17787/18040 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0809 - mse: 0.0132\n",
      "Epoch 00013: val_loss did not improve from 0.01583\n",
      "18040/18040 [==============================] - 2s 117us/sample - loss: 0.0132 - mae: 0.0809 - mse: 0.0132 - val_loss: 0.0172 - val_mae: 0.0895 - val_mse: 0.0172\n",
      "Epoch 14/1000\n",
      "17913/18040 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0799 - mse: 0.0127\n",
      "Epoch 00014: val_loss improved from 0.01583 to 0.01574, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 121us/sample - loss: 0.0128 - mae: 0.0800 - mse: 0.0128 - val_loss: 0.0157 - val_mae: 0.0829 - val_mse: 0.0157\n",
      "Epoch 15/1000\n",
      "17871/18040 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0792 - mse: 0.0125\n",
      "Epoch 00015: val_loss improved from 0.01574 to 0.01491, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 3s 142us/sample - loss: 0.0125 - mae: 0.0791 - mse: 0.0125 - val_loss: 0.0149 - val_mae: 0.0832 - val_mse: 0.0149\n",
      "Epoch 16/1000\n",
      "18039/18040 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0790 - mse: 0.0126\n",
      "Epoch 00016: val_loss did not improve from 0.01491\n",
      "18040/18040 [==============================] - 2s 121us/sample - loss: 0.0126 - mae: 0.0790 - mse: 0.0126 - val_loss: 0.0166 - val_mae: 0.0888 - val_mse: 0.0166\n",
      "Epoch 17/1000\n",
      "17913/18040 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0771 - mse: 0.0119\n",
      "Epoch 00017: val_loss improved from 0.01491 to 0.01369, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 114us/sample - loss: 0.0119 - mae: 0.0771 - mse: 0.0119 - val_loss: 0.0137 - val_mae: 0.0751 - val_mse: 0.0137\n",
      "Epoch 18/1000\n",
      "17808/18040 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0755 - mse: 0.0117\n",
      "Epoch 00018: val_loss did not improve from 0.01369\n",
      "18040/18040 [==============================] - 2s 113us/sample - loss: 0.0117 - mae: 0.0755 - mse: 0.0117 - val_loss: 0.0151 - val_mae: 0.0809 - val_mse: 0.0151\n",
      "Epoch 19/1000\n",
      "18039/18040 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0745 - mse: 0.0112\n",
      "Epoch 00019: val_loss did not improve from 0.01369\n",
      "18040/18040 [==============================] - 2s 115us/sample - loss: 0.0112 - mae: 0.0745 - mse: 0.0112 - val_loss: 0.0157 - val_mae: 0.0814 - val_mse: 0.0157\n",
      "Epoch 20/1000\n",
      "17766/18040 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0740 - mse: 0.0111\n",
      "Epoch 00020: val_loss did not improve from 0.01369\n",
      "18040/18040 [==============================] - 2s 114us/sample - loss: 0.0111 - mae: 0.0740 - mse: 0.0111 - val_loss: 0.0159 - val_mae: 0.0864 - val_mse: 0.0159\n",
      "Epoch 21/1000\n",
      "17745/18040 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0716 - mse: 0.0106\n",
      "Epoch 00021: val_loss did not improve from 0.01369\n",
      "18040/18040 [==============================] - 2s 115us/sample - loss: 0.0106 - mae: 0.0716 - mse: 0.0106 - val_loss: 0.0144 - val_mae: 0.0775 - val_mse: 0.0144\n",
      "Epoch 22/1000\n",
      "17598/18040 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0727 - mse: 0.0109\n",
      "Epoch 00022: val_loss improved from 0.01369 to 0.01298, saving model to model_checkpoint.h5\n",
      "18040/18040 [==============================] - 2s 116us/sample - loss: 0.0108 - mae: 0.0727 - mse: 0.0108 - val_loss: 0.0130 - val_mae: 0.0782 - val_mse: 0.0130\n",
      "Epoch 23/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17787/18040 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0719 - mse: 0.0105\n",
      "Epoch 00023: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 120us/sample - loss: 0.0106 - mae: 0.0721 - mse: 0.0106 - val_loss: 0.0153 - val_mae: 0.0822 - val_mse: 0.0153\n",
      "Epoch 24/1000\n",
      "17766/18040 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0710 - mse: 0.0102\n",
      "Epoch 00024: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 3s 154us/sample - loss: 0.0102 - mae: 0.0709 - mse: 0.0102 - val_loss: 0.0149 - val_mae: 0.0803 - val_mse: 0.0149\n",
      "Epoch 25/1000\n",
      "17976/18040 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0700 - mse: 0.0100\n",
      "Epoch 00025: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 116us/sample - loss: 0.0100 - mae: 0.0701 - mse: 0.0100 - val_loss: 0.0145 - val_mae: 0.0762 - val_mse: 0.0145\n",
      "Epoch 26/1000\n",
      "17997/18040 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0696 - mse: 0.0099\n",
      "Epoch 00026: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 116us/sample - loss: 0.0099 - mae: 0.0695 - mse: 0.0099 - val_loss: 0.0142 - val_mae: 0.0768 - val_mse: 0.0142\n",
      "Epoch 27/1000\n",
      "17640/18040 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0696 - mse: 0.0098\n",
      "Epoch 00027: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 118us/sample - loss: 0.0098 - mae: 0.0696 - mse: 0.0098 - val_loss: 0.0140 - val_mae: 0.0759 - val_mse: 0.0140\n",
      "Epoch 28/1000\n",
      "17619/18040 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0678 - mse: 0.0095\n",
      "Epoch 00028: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 114us/sample - loss: 0.0095 - mae: 0.0677 - mse: 0.0095 - val_loss: 0.0153 - val_mae: 0.0795 - val_mse: 0.0153\n",
      "Epoch 29/1000\n",
      "17829/18040 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0685 - mse: 0.0095\n",
      "Epoch 00029: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 116us/sample - loss: 0.0095 - mae: 0.0684 - mse: 0.0095 - val_loss: 0.0135 - val_mae: 0.0744 - val_mse: 0.0135\n",
      "Epoch 30/1000\n",
      "17955/18040 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0661 - mse: 0.0090\n",
      "Epoch 00030: val_loss did not improve from 0.01298\n",
      "18040/18040 [==============================] - 2s 113us/sample - loss: 0.0090 - mae: 0.0661 - mse: 0.0090 - val_loss: 0.0156 - val_mae: 0.0827 - val_mse: 0.0156\n",
      "Elapsed time during model training:  66.66580271720886\n",
      "Size of training set 18691\n",
      "Train on 7476 samples, validate on 11215 samples\n",
      "Epoch 1/1000\n",
      "6489/7476 [=========================>....] - ETA: 0s - loss: 0.1034 - mae: 0.2222 - mse: 0.1034\n",
      "Epoch 00001: val_loss improved from inf to 0.04046, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0945,  mae:0.2119,  mse:0.0945,  val_loss:0.0405,  val_mae:0.1573,  val_mse:0.0405,  \n",
      "7476/7476 [==============================] - 1s 116us/sample - loss: 0.0945 - mae: 0.2119 - mse: 0.0945 - val_loss: 0.0405 - val_mae: 0.1573 - val_mse: 0.0405\n",
      "Epoch 2/1000\n",
      "6489/7476 [=========================>....] - ETA: 0s - loss: 0.0236 - mae: 0.1143 - mse: 0.0236\n",
      "Epoch 00002: val_loss improved from 0.04046 to 0.01971, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 1s 70us/sample - loss: 0.0232 - mae: 0.1132 - mse: 0.0232 - val_loss: 0.0197 - val_mae: 0.1008 - val_mse: 0.0197\n",
      "Epoch 3/1000\n",
      "6695/7476 [=========================>....] - ETA: 0s - loss: 0.0177 - mae: 0.0966 - mse: 0.0177\n",
      "Epoch 00003: val_loss did not improve from 0.01971\n",
      "7476/7476 [==============================] - 0s 65us/sample - loss: 0.0179 - mae: 0.0972 - mse: 0.0179 - val_loss: 0.0203 - val_mae: 0.1015 - val_mse: 0.0203\n",
      "Epoch 4/1000\n",
      "6695/7476 [=========================>....] - ETA: 0s - loss: 0.0197 - mae: 0.1037 - mse: 0.0197\n",
      "Epoch 00004: val_loss did not improve from 0.01971\n",
      "7476/7476 [==============================] - 0s 66us/sample - loss: 0.0193 - mae: 0.1024 - mse: 0.0193 - val_loss: 0.0218 - val_mae: 0.1036 - val_mse: 0.0218\n",
      "Epoch 5/1000\n",
      "6695/7476 [=========================>....] - ETA: 0s - loss: 0.0153 - mae: 0.0897 - mse: 0.0153\n",
      "Epoch 00005: val_loss improved from 0.01971 to 0.01843, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 1s 68us/sample - loss: 0.0154 - mae: 0.0902 - mse: 0.0154 - val_loss: 0.0184 - val_mae: 0.0960 - val_mse: 0.0184\n",
      "Epoch 6/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0140 - mae: 0.0856 - mse: 0.0140\n",
      "Epoch 00006: val_loss improved from 0.01843 to 0.01796, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 1s 68us/sample - loss: 0.0143 - mae: 0.0861 - mse: 0.0143 - val_loss: 0.0180 - val_mae: 0.0942 - val_mse: 0.0180\n",
      "Epoch 7/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0144 - mae: 0.0876 - mse: 0.0144\n",
      "Epoch 00007: val_loss did not improve from 0.01796\n",
      "7476/7476 [==============================] - 0s 65us/sample - loss: 0.0144 - mae: 0.0875 - mse: 0.0144 - val_loss: 0.0185 - val_mae: 0.0969 - val_mse: 0.0185\n",
      "Epoch 8/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0133 - mae: 0.0845 - mse: 0.0133\n",
      "Epoch 00008: val_loss did not improve from 0.01796\n",
      "7476/7476 [==============================] - 0s 66us/sample - loss: 0.0136 - mae: 0.0852 - mse: 0.0136 - val_loss: 0.0210 - val_mae: 0.1086 - val_mse: 0.0210\n",
      "Epoch 9/1000\n",
      "6386/7476 [========================>.....] - ETA: 0s - loss: 0.0131 - mae: 0.0826 - mse: 0.0131\n",
      "Epoch 00009: val_loss improved from 0.01796 to 0.01747, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 1s 71us/sample - loss: 0.0127 - mae: 0.0814 - mse: 0.0127 - val_loss: 0.0175 - val_mae: 0.0925 - val_mse: 0.0175\n",
      "Epoch 10/1000\n",
      "6489/7476 [=========================>....] - ETA: 0s - loss: 0.0124 - mae: 0.0811 - mse: 0.0124\n",
      "Epoch 00010: val_loss improved from 0.01747 to 0.01737, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 0s 64us/sample - loss: 0.0123 - mae: 0.0809 - mse: 0.0123 - val_loss: 0.0174 - val_mae: 0.0915 - val_mse: 0.0174\n",
      "Epoch 11/1000\n",
      "5871/7476 [======================>.......] - ETA: 0s - loss: 0.0120 - mae: 0.0809 - mse: 0.0120\n",
      "Epoch 00011: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 54us/sample - loss: 0.0123 - mae: 0.0812 - mse: 0.0123 - val_loss: 0.0197 - val_mae: 0.0964 - val_mse: 0.0197\n",
      "Epoch 12/1000\n",
      "6180/7476 [=======================>......] - ETA: 0s - loss: 0.0125 - mae: 0.0814 - mse: 0.0125\n",
      "Epoch 00012: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 52us/sample - loss: 0.0126 - mae: 0.0825 - mse: 0.0126 - val_loss: 0.0179 - val_mae: 0.0940 - val_mse: 0.0179\n",
      "Epoch 13/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0116 - mae: 0.0790 - mse: 0.0116\n",
      "Epoch 00013: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0116 - mae: 0.0789 - mse: 0.0116 - val_loss: 0.0191 - val_mae: 0.0971 - val_mse: 0.0191\n",
      "Epoch 14/1000\n",
      "6283/7476 [========================>.....] - ETA: 0s - loss: 0.0095 - mae: 0.0696 - mse: 0.0095\n",
      "Epoch 00014: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 54us/sample - loss: 0.0101 - mae: 0.0714 - mse: 0.0101 - val_loss: 0.0186 - val_mae: 0.0978 - val_mse: 0.0186\n",
      "Epoch 15/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0113 - mae: 0.0777 - mse: 0.0113\n",
      "Epoch 00015: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 51us/sample - loss: 0.0113 - mae: 0.0775 - mse: 0.0113 - val_loss: 0.0178 - val_mae: 0.0941 - val_mse: 0.0178\n",
      "Epoch 16/1000\n",
      "7107/7476 [===========================>..] - ETA: 0s - loss: 0.0099 - mae: 0.0724 - mse: 0.0099\n",
      "Epoch 00016: val_loss did not improve from 0.01737\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0100 - mae: 0.0724 - mse: 0.0100 - val_loss: 0.0189 - val_mae: 0.0992 - val_mse: 0.0189\n",
      "Epoch 17/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7107/7476 [===========================>..] - ETA: 0s - loss: 0.0112 - mae: 0.0798 - mse: 0.0112\n",
      "Epoch 00017: val_loss improved from 0.01737 to 0.01690, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 0s 52us/sample - loss: 0.0112 - mae: 0.0796 - mse: 0.0112 - val_loss: 0.0169 - val_mae: 0.0921 - val_mse: 0.0169\n",
      "Epoch 18/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0091 - mae: 0.0697 - mse: 0.0091\n",
      "Epoch 00018: val_loss did not improve from 0.01690\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0091 - mae: 0.0698 - mse: 0.0091 - val_loss: 0.0180 - val_mae: 0.0915 - val_mse: 0.0180\n",
      "Epoch 19/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0094 - mae: 0.0715 - mse: 0.0094\n",
      "Epoch 00019: val_loss did not improve from 0.01690\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0095 - mae: 0.0718 - mse: 0.0095 - val_loss: 0.0216 - val_mae: 0.1032 - val_mse: 0.0216\n",
      "Epoch 20/1000\n",
      "7004/7476 [===========================>..] - ETA: 0s - loss: 0.0103 - mae: 0.0760 - mse: 0.0103\n",
      "Epoch 00020: val_loss improved from 0.01690 to 0.01645, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 0s 53us/sample - loss: 0.0102 - mae: 0.0755 - mse: 0.0102 - val_loss: 0.0164 - val_mae: 0.0892 - val_mse: 0.0164\n",
      "Epoch 21/1000\n",
      "6695/7476 [=========================>....] - ETA: 0s - loss: 0.0089 - mae: 0.0694 - mse: 0.0089\n",
      "Epoch 00021: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 51us/sample - loss: 0.0089 - mae: 0.0697 - mse: 0.0089 - val_loss: 0.0174 - val_mae: 0.0970 - val_mse: 0.0174\n",
      "Epoch 22/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0083 - mae: 0.0671 - mse: 0.0083\n",
      "Epoch 00022: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0084 - mae: 0.0677 - mse: 0.0084 - val_loss: 0.0167 - val_mae: 0.0889 - val_mse: 0.0167\n",
      "Epoch 23/1000\n",
      "7210/7476 [===========================>..] - ETA: 0s - loss: 0.0073 - mae: 0.0620 - mse: 0.0073\n",
      "Epoch 00023: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 48us/sample - loss: 0.0072 - mae: 0.0618 - mse: 0.0072 - val_loss: 0.0172 - val_mae: 0.0919 - val_mse: 0.0172\n",
      "Epoch 24/1000\n",
      "6489/7476 [=========================>....] - ETA: 0s - loss: 0.0077 - mae: 0.0644 - mse: 0.0077\n",
      "Epoch 00024: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 60us/sample - loss: 0.0078 - mae: 0.0647 - mse: 0.0078 - val_loss: 0.0165 - val_mae: 0.0864 - val_mse: 0.0165\n",
      "Epoch 25/1000\n",
      "7210/7476 [===========================>..] - ETA: 0s - loss: 0.0078 - mae: 0.0652 - mse: 0.0078\n",
      "Epoch 00025: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 59us/sample - loss: 0.0079 - mae: 0.0655 - mse: 0.0079 - val_loss: 0.0168 - val_mae: 0.0888 - val_mse: 0.0168\n",
      "Epoch 26/1000\n",
      "6386/7476 [========================>.....] - ETA: 0s - loss: 0.0074 - mae: 0.0639 - mse: 0.0074\n",
      "Epoch 00026: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 51us/sample - loss: 0.0073 - mae: 0.0632 - mse: 0.0073 - val_loss: 0.0166 - val_mae: 0.0857 - val_mse: 0.0166\n",
      "Epoch 27/1000\n",
      "7004/7476 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0600 - mse: 0.0066\n",
      "Epoch 00027: val_loss did not improve from 0.01645\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0066 - mae: 0.0601 - mse: 0.0066 - val_loss: 0.0167 - val_mae: 0.0875 - val_mse: 0.0167\n",
      "Epoch 28/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0072 - mae: 0.0634 - mse: 0.0072\n",
      "Epoch 00028: val_loss improved from 0.01645 to 0.01613, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 0s 53us/sample - loss: 0.0073 - mae: 0.0634 - mse: 0.0073 - val_loss: 0.0161 - val_mae: 0.0877 - val_mse: 0.0161\n",
      "Epoch 29/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0065 - mae: 0.0592 - mse: 0.0065\n",
      "Epoch 00029: val_loss did not improve from 0.01613\n",
      "7476/7476 [==============================] - 0s 50us/sample - loss: 0.0065 - mae: 0.0596 - mse: 0.0065 - val_loss: 0.0182 - val_mae: 0.0946 - val_mse: 0.0182\n",
      "Epoch 30/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0066 - mae: 0.0604 - mse: 0.0066\n",
      "Epoch 00030: val_loss improved from 0.01613 to 0.01571, saving model to model_checkpoint.h5\n",
      "7476/7476 [==============================] - 0s 53us/sample - loss: 0.0066 - mae: 0.0605 - mse: 0.0066 - val_loss: 0.0157 - val_mae: 0.0845 - val_mse: 0.0157\n",
      "Epoch 31/1000\n",
      "7107/7476 [===========================>..] - ETA: 0s - loss: 0.0063 - mae: 0.0592 - mse: 0.0063\n",
      "Epoch 00031: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0064 - mae: 0.0592 - mse: 0.0064 - val_loss: 0.0157 - val_mae: 0.0831 - val_mse: 0.0157\n",
      "Epoch 32/1000\n",
      "6901/7476 [==========================>...] - ETA: 0s - loss: 0.0061 - mae: 0.0567 - mse: 0.0061\n",
      "Epoch 00032: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 49us/sample - loss: 0.0061 - mae: 0.0565 - mse: 0.0061 - val_loss: 0.0166 - val_mae: 0.0856 - val_mse: 0.0166\n",
      "Epoch 33/1000\n",
      "7210/7476 [===========================>..] - ETA: 0s - loss: 0.0060 - mae: 0.0574 - mse: 0.0060\n",
      "Epoch 00033: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 48us/sample - loss: 0.0059 - mae: 0.0570 - mse: 0.0059 - val_loss: 0.0160 - val_mae: 0.0837 - val_mse: 0.0160\n",
      "Epoch 34/1000\n",
      "6798/7476 [==========================>...] - ETA: 0s - loss: 0.0056 - mae: 0.0560 - mse: 0.0056\n",
      "Epoch 00034: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 50us/sample - loss: 0.0057 - mae: 0.0561 - mse: 0.0057 - val_loss: 0.0168 - val_mae: 0.0882 - val_mse: 0.0168\n",
      "Epoch 35/1000\n",
      "5768/7476 [======================>.......] - ETA: 0s - loss: 0.0055 - mae: 0.0555 - mse: 0.0055\n",
      "Epoch 00035: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 54us/sample - loss: 0.0055 - mae: 0.0554 - mse: 0.0055 - val_loss: 0.0167 - val_mae: 0.0867 - val_mse: 0.0167\n",
      "Epoch 36/1000\n",
      "6180/7476 [=======================>......] - ETA: 0s - loss: 0.0063 - mae: 0.0596 - mse: 0.0063\n",
      "Epoch 00036: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 52us/sample - loss: 0.0064 - mae: 0.0597 - mse: 0.0064 - val_loss: 0.0167 - val_mae: 0.0877 - val_mse: 0.0167\n",
      "Epoch 37/1000\n",
      "6592/7476 [=========================>....] - ETA: 0s - loss: 0.0055 - mae: 0.0549 - mse: 0.0055\n",
      "Epoch 00037: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 56us/sample - loss: 0.0054 - mae: 0.0548 - mse: 0.0054 - val_loss: 0.0159 - val_mae: 0.0843 - val_mse: 0.0159\n",
      "Epoch 38/1000\n",
      "7416/7476 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0531 - mse: 0.0051\n",
      "Epoch 00038: val_loss did not improve from 0.01571\n",
      "7476/7476 [==============================] - 0s 63us/sample - loss: 0.0051 - mae: 0.0532 - mse: 0.0051 - val_loss: 0.0163 - val_mae: 0.0862 - val_mse: 0.0163\n",
      "Elapsed time during model training:  16.59030795097351\n",
      "Size of training set 18691\n",
      "Train on 14262 samples, validate on 4429 samples\n",
      "Epoch 1/1000\n",
      "14125/14262 [============================>.] - ETA: 0s - loss: 0.0439 - mae: 0.1493 - mse: 0.0439\n",
      "Epoch 00001: val_loss improved from inf to 0.02100, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0437,  mae:0.1489,  mse:0.0437,  val_loss:0.0210,  val_mae:0.1062,  val_mse:0.0210,  \n",
      "14262/14262 [==============================] - 3s 187us/sample - loss: 0.0437 - mae: 0.1489 - mse: 0.0437 - val_loss: 0.0210 - val_mae: 0.1062 - val_mse: 0.0210\n",
      "Epoch 2/1000\n",
      "13775/14262 [===========================>..] - ETA: 0s - loss: 0.0250 - mae: 0.1176 - mse: 0.0250\n",
      "Epoch 00002: val_loss improved from 0.02100 to 0.01934, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 147us/sample - loss: 0.0249 - mae: 0.1173 - mse: 0.0249 - val_loss: 0.0193 - val_mae: 0.0990 - val_mse: 0.0193\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000/14262 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.1071 - mse: 0.0213\n",
      "Epoch 00003: val_loss improved from 0.01934 to 0.01795, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 115us/sample - loss: 0.0212 - mae: 0.1071 - mse: 0.0212 - val_loss: 0.0180 - val_mae: 0.0930 - val_mse: 0.0180\n",
      "Epoch 4/1000\n",
      "14250/14262 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.1074 - mse: 0.0208\n",
      "Epoch 00004: val_loss did not improve from 0.01795\n",
      "14262/14262 [==============================] - 2s 120us/sample - loss: 0.0208 - mae: 0.1074 - mse: 0.0208 - val_loss: 0.0181 - val_mae: 0.0926 - val_mse: 0.0181\n",
      "Epoch 5/1000\n",
      "14000/14262 [============================>.] - ETA: 0s - loss: 0.0183 - mae: 0.0994 - mse: 0.0183\n",
      "Epoch 00005: val_loss did not improve from 0.01795\n",
      "14262/14262 [==============================] - 2s 120us/sample - loss: 0.0184 - mae: 0.0996 - mse: 0.0184 - val_loss: 0.0186 - val_mae: 0.0989 - val_mse: 0.0186\n",
      "Epoch 6/1000\n",
      "13950/14262 [============================>.] - ETA: 0s - loss: 0.0178 - mae: 0.0974 - mse: 0.0178\n",
      "Epoch 00006: val_loss improved from 0.01795 to 0.01535, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 121us/sample - loss: 0.0177 - mae: 0.0971 - mse: 0.0177 - val_loss: 0.0153 - val_mae: 0.0838 - val_mse: 0.0153\n",
      "Epoch 7/1000\n",
      "14000/14262 [============================>.] - ETA: 0s - loss: 0.0174 - mae: 0.0963 - mse: 0.0174\n",
      "Epoch 00007: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 113us/sample - loss: 0.0175 - mae: 0.0964 - mse: 0.0175 - val_loss: 0.0176 - val_mae: 0.0979 - val_mse: 0.0176\n",
      "Epoch 8/1000\n",
      "13725/14262 [===========================>..] - ETA: 0s - loss: 0.0164 - mae: 0.0929 - mse: 0.0164\n",
      "Epoch 00008: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 116us/sample - loss: 0.0165 - mae: 0.0932 - mse: 0.0165 - val_loss: 0.0176 - val_mae: 0.0947 - val_mse: 0.0176\n",
      "Epoch 9/1000\n",
      "13900/14262 [============================>.] - ETA: 0s - loss: 0.0165 - mae: 0.0929 - mse: 0.0165\n",
      "Epoch 00009: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 108us/sample - loss: 0.0164 - mae: 0.0926 - mse: 0.0164 - val_loss: 0.0157 - val_mae: 0.0851 - val_mse: 0.0157\n",
      "Epoch 10/1000\n",
      "14025/14262 [============================>.] - ETA: 0s - loss: 0.0160 - mae: 0.0920 - mse: 0.0160\n",
      "Epoch 00010: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 111us/sample - loss: 0.0161 - mae: 0.0922 - mse: 0.0161 - val_loss: 0.0191 - val_mae: 0.1064 - val_mse: 0.0191\n",
      "Epoch 11/1000\n",
      "13975/14262 [============================>.] - ETA: 0s - loss: 0.0149 - mae: 0.0884 - mse: 0.0149\n",
      "Epoch 00011: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 113us/sample - loss: 0.0150 - mae: 0.0884 - mse: 0.0150 - val_loss: 0.0157 - val_mae: 0.0876 - val_mse: 0.0157\n",
      "Epoch 12/1000\n",
      "14125/14262 [============================>.] - ETA: 0s - loss: 0.0149 - mae: 0.0881 - mse: 0.0149\n",
      "Epoch 00012: val_loss did not improve from 0.01535\n",
      "14262/14262 [==============================] - 2s 153us/sample - loss: 0.0149 - mae: 0.0881 - mse: 0.0149 - val_loss: 0.0200 - val_mae: 0.1045 - val_mse: 0.0200\n",
      "Epoch 13/1000\n",
      "14000/14262 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0838 - mse: 0.0137\n",
      "Epoch 00013: val_loss improved from 0.01535 to 0.01463, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 154us/sample - loss: 0.0138 - mae: 0.0840 - mse: 0.0138 - val_loss: 0.0146 - val_mae: 0.0822 - val_mse: 0.0146\n",
      "Epoch 14/1000\n",
      "13900/14262 [============================>.] - ETA: 0s - loss: 0.0138 - mae: 0.0844 - mse: 0.0138\n",
      "Epoch 00014: val_loss did not improve from 0.01463\n",
      "14262/14262 [==============================] - 2s 153us/sample - loss: 0.0139 - mae: 0.0846 - mse: 0.0139 - val_loss: 0.0189 - val_mae: 0.0917 - val_mse: 0.0189\n",
      "Epoch 15/1000\n",
      "14150/14262 [============================>.] - ETA: 0s - loss: 0.0139 - mae: 0.0845 - mse: 0.0139\n",
      "Epoch 00015: val_loss did not improve from 0.01463\n",
      "14262/14262 [==============================] - 2s 139us/sample - loss: 0.0139 - mae: 0.0845 - mse: 0.0139 - val_loss: 0.0186 - val_mae: 0.1008 - val_mse: 0.0186\n",
      "Epoch 16/1000\n",
      "14050/14262 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0829 - mse: 0.0134\n",
      "Epoch 00016: val_loss did not improve from 0.01463\n",
      "14262/14262 [==============================] - 2s 115us/sample - loss: 0.0134 - mae: 0.0829 - mse: 0.0134 - val_loss: 0.0181 - val_mae: 0.0996 - val_mse: 0.0181\n",
      "Epoch 17/1000\n",
      "14000/14262 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0803 - mse: 0.0127\n",
      "Epoch 00017: val_loss did not improve from 0.01463\n",
      "14262/14262 [==============================] - 2s 123us/sample - loss: 0.0127 - mae: 0.0803 - mse: 0.0127 - val_loss: 0.0151 - val_mae: 0.0832 - val_mse: 0.0151\n",
      "Epoch 18/1000\n",
      "13725/14262 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0782 - mse: 0.0119\n",
      "Epoch 00018: val_loss did not improve from 0.01463\n",
      "14262/14262 [==============================] - 2s 114us/sample - loss: 0.0120 - mae: 0.0785 - mse: 0.0120 - val_loss: 0.0170 - val_mae: 0.0967 - val_mse: 0.0170\n",
      "Epoch 19/1000\n",
      "13925/14262 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0780 - mse: 0.0119\n",
      "Epoch 00019: val_loss improved from 0.01463 to 0.01444, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 115us/sample - loss: 0.0119 - mae: 0.0782 - mse: 0.0119 - val_loss: 0.0144 - val_mae: 0.0830 - val_mse: 0.0144\n",
      "Epoch 20/1000\n",
      "14050/14262 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0761 - mse: 0.0114\n",
      "Epoch 00020: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 115us/sample - loss: 0.0114 - mae: 0.0762 - mse: 0.0114 - val_loss: 0.0149 - val_mae: 0.0817 - val_mse: 0.0149\n",
      "Epoch 21/1000\n",
      "13750/14262 [===========================>..] - ETA: 0s - loss: 0.0113 - mae: 0.0758 - mse: 0.0113\n",
      "Epoch 00021: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 110us/sample - loss: 0.0113 - mae: 0.0760 - mse: 0.0113 - val_loss: 0.0157 - val_mae: 0.0878 - val_mse: 0.0157\n",
      "Epoch 22/1000\n",
      "14050/14262 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0744 - mse: 0.0109\n",
      "Epoch 00022: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 111us/sample - loss: 0.0109 - mae: 0.0743 - mse: 0.0109 - val_loss: 0.0165 - val_mae: 0.0907 - val_mse: 0.0165\n",
      "Epoch 23/1000\n",
      "13925/14262 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0745 - mse: 0.0109\n",
      "Epoch 00023: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 157us/sample - loss: 0.0109 - mae: 0.0745 - mse: 0.0109 - val_loss: 0.0147 - val_mae: 0.0869 - val_mse: 0.0147\n",
      "Epoch 24/1000\n",
      "13925/14262 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0715 - mse: 0.0101\n",
      "Epoch 00024: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 156us/sample - loss: 0.0102 - mae: 0.0715 - mse: 0.0102 - val_loss: 0.0146 - val_mae: 0.0816 - val_mse: 0.0146\n",
      "Epoch 25/1000\n",
      "14050/14262 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0733 - mse: 0.0105\n",
      "Epoch 00025: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 150us/sample - loss: 0.0105 - mae: 0.0733 - mse: 0.0105 - val_loss: 0.0154 - val_mae: 0.0876 - val_mse: 0.0154\n",
      "Epoch 26/1000\n",
      "14200/14262 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0722 - mse: 0.0103\n",
      "Epoch 00026: val_loss did not improve from 0.01444\n",
      "14262/14262 [==============================] - 2s 114us/sample - loss: 0.0104 - mae: 0.0722 - mse: 0.0104 - val_loss: 0.0171 - val_mae: 0.0893 - val_mse: 0.0171\n",
      "Epoch 27/1000\n",
      "13775/14262 [===========================>..] - ETA: 0s - loss: 0.0103 - mae: 0.0722 - mse: 0.0103\n",
      "Epoch 00027: val_loss improved from 0.01444 to 0.01325, saving model to model_checkpoint.h5\n",
      "14262/14262 [==============================] - 2s 111us/sample - loss: 0.0103 - mae: 0.0723 - mse: 0.0103 - val_loss: 0.0133 - val_mae: 0.0788 - val_mse: 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "13950/14262 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0694 - mse: 0.0095\n",
      "Epoch 00028: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 117us/sample - loss: 0.0095 - mae: 0.0693 - mse: 0.0095 - val_loss: 0.0136 - val_mae: 0.0785 - val_mse: 0.0136\n",
      "Epoch 29/1000\n",
      "13950/14262 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0690 - mse: 0.0094\n",
      "Epoch 00029: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 155us/sample - loss: 0.0094 - mae: 0.0690 - mse: 0.0094 - val_loss: 0.0141 - val_mae: 0.0816 - val_mse: 0.0141\n",
      "Epoch 30/1000\n",
      "13725/14262 [===========================>..] - ETA: 0s - loss: 0.0090 - mae: 0.0676 - mse: 0.0090\n",
      "Epoch 00030: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 113us/sample - loss: 0.0090 - mae: 0.0679 - mse: 0.0090 - val_loss: 0.0156 - val_mae: 0.0869 - val_mse: 0.0156\n",
      "Epoch 31/1000\n",
      "13975/14262 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0675 - mse: 0.0089\n",
      "Epoch 00031: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 108us/sample - loss: 0.0090 - mae: 0.0677 - mse: 0.0090 - val_loss: 0.0139 - val_mae: 0.0809 - val_mse: 0.0139\n",
      "Epoch 32/1000\n",
      "14125/14262 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0679 - mse: 0.0092\n",
      "Epoch 00032: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 107us/sample - loss: 0.0092 - mae: 0.0678 - mse: 0.0092 - val_loss: 0.0145 - val_mae: 0.0800 - val_mse: 0.0145\n",
      "Epoch 33/1000\n",
      "14025/14262 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0680 - mse: 0.0091\n",
      "Epoch 00033: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 107us/sample - loss: 0.0091 - mae: 0.0680 - mse: 0.0091 - val_loss: 0.0146 - val_mae: 0.0838 - val_mse: 0.0146\n",
      "Epoch 34/1000\n",
      "13950/14262 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0661 - mse: 0.0087\n",
      "Epoch 00034: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 152us/sample - loss: 0.0087 - mae: 0.0662 - mse: 0.0087 - val_loss: 0.0152 - val_mae: 0.0853 - val_mse: 0.0152\n",
      "Epoch 35/1000\n",
      "14075/14262 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0670 - mse: 0.0087\n",
      "Epoch 00035: val_loss did not improve from 0.01325\n",
      "14262/14262 [==============================] - 2s 126us/sample - loss: 0.0087 - mae: 0.0670 - mse: 0.0087 - val_loss: 0.0134 - val_mae: 0.0781 - val_mse: 0.0134\n",
      "Elapsed time during model training:  63.75837540626526\n",
      "Size of training set 18691\n",
      "Train on 17121 samples, validate on 1570 samples\n",
      "Epoch 1/1000\n",
      "16072/17121 [===========================>..] - ETA: 0s - loss: 0.0540 - mae: 0.1573 - mse: 0.0540\n",
      "Epoch 00001: val_loss improved from inf to 0.02255, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0521,  mae:0.1546,  mse:0.0521,  val_loss:0.0226,  val_mae:0.1112,  val_mse:0.0226,  \n",
      "17121/17121 [==============================] - 1s 64us/sample - loss: 0.0521 - mae: 0.1546 - mse: 0.0521 - val_loss: 0.0226 - val_mae: 0.1112 - val_mse: 0.0226\n",
      "Epoch 2/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0201 - mae: 0.1028 - mse: 0.0201\n",
      "Epoch 00002: val_loss did not improve from 0.02255\n",
      "17121/17121 [==============================] - 1s 46us/sample - loss: 0.0200 - mae: 0.1025 - mse: 0.0200 - val_loss: 0.0266 - val_mae: 0.1212 - val_mse: 0.0266\n",
      "Epoch 3/1000\n",
      "15990/17121 [===========================>..] - ETA: 0s - loss: 0.0180 - mae: 0.0969 - mse: 0.0180\n",
      "Epoch 00003: val_loss improved from 0.02255 to 0.02070, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 47us/sample - loss: 0.0179 - mae: 0.0968 - mse: 0.0179 - val_loss: 0.0207 - val_mae: 0.1039 - val_mse: 0.0207\n",
      "Epoch 4/1000\n",
      "15744/17121 [==========================>...] - ETA: 0s - loss: 0.0165 - mae: 0.0916 - mse: 0.0165\n",
      "Epoch 00004: val_loss improved from 0.02070 to 0.01943, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 38us/sample - loss: 0.0163 - mae: 0.0911 - mse: 0.0163 - val_loss: 0.0194 - val_mae: 0.0994 - val_mse: 0.0194\n",
      "Epoch 5/1000\n",
      "15990/17121 [===========================>..] - ETA: 0s - loss: 0.0157 - mae: 0.0903 - mse: 0.0157\n",
      "Epoch 00005: val_loss did not improve from 0.01943\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0158 - mae: 0.0904 - mse: 0.0158 - val_loss: 0.0209 - val_mae: 0.1068 - val_mse: 0.0209\n",
      "Epoch 6/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0150 - mae: 0.0871 - mse: 0.0150\n",
      "Epoch 00006: val_loss improved from 0.01943 to 0.01653, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0149 - mae: 0.0870 - mse: 0.0149 - val_loss: 0.0165 - val_mae: 0.0875 - val_mse: 0.0165\n",
      "Epoch 7/1000\n",
      "16482/17121 [===========================>..] - ETA: 0s - loss: 0.0143 - mae: 0.0852 - mse: 0.0143\n",
      "Epoch 00007: val_loss did not improve from 0.01653\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0143 - mae: 0.0853 - mse: 0.0143 - val_loss: 0.0180 - val_mae: 0.0946 - val_mse: 0.0180\n",
      "Epoch 8/1000\n",
      "16400/17121 [===========================>..] - ETA: 0s - loss: 0.0150 - mae: 0.0881 - mse: 0.0150\n",
      "Epoch 00008: val_loss did not improve from 0.01653\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0150 - mae: 0.0879 - mse: 0.0150 - val_loss: 0.0186 - val_mae: 0.0939 - val_mse: 0.0186\n",
      "Epoch 9/1000\n",
      "16154/17121 [===========================>..] - ETA: 0s - loss: 0.0137 - mae: 0.0841 - mse: 0.0137\n",
      "Epoch 00009: val_loss did not improve from 0.01653\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0137 - mae: 0.0840 - mse: 0.0137 - val_loss: 0.0166 - val_mae: 0.0893 - val_mse: 0.0166\n",
      "Epoch 10/1000\n",
      "16564/17121 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0831 - mse: 0.0133\n",
      "Epoch 00010: val_loss did not improve from 0.01653\n",
      "17121/17121 [==============================] - 1s 31us/sample - loss: 0.0134 - mae: 0.0832 - mse: 0.0134 - val_loss: 0.0190 - val_mae: 0.1005 - val_mse: 0.0190\n",
      "Epoch 11/1000\n",
      "15826/17121 [==========================>...] - ETA: 0s - loss: 0.0131 - mae: 0.0821 - mse: 0.0131\n",
      "Epoch 00011: val_loss did not improve from 0.01653\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0130 - mae: 0.0821 - mse: 0.0130 - val_loss: 0.0190 - val_mae: 0.0978 - val_mse: 0.0190\n",
      "Epoch 12/1000\n",
      "16154/17121 [===========================>..] - ETA: 0s - loss: 0.0130 - mae: 0.0817 - mse: 0.0130\n",
      "Epoch 00012: val_loss improved from 0.01653 to 0.01590, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 34us/sample - loss: 0.0129 - mae: 0.0813 - mse: 0.0129 - val_loss: 0.0159 - val_mae: 0.0869 - val_mse: 0.0159\n",
      "Epoch 13/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0824 - mse: 0.0131\n",
      "Epoch 00013: val_loss did not improve from 0.01590\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0130 - mae: 0.0822 - mse: 0.0130 - val_loss: 0.0175 - val_mae: 0.0906 - val_mse: 0.0175\n",
      "Epoch 14/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0116 - mae: 0.0768 - mse: 0.0116\n",
      "Epoch 00014: val_loss did not improve from 0.01590\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0116 - mae: 0.0767 - mse: 0.0116 - val_loss: 0.0183 - val_mae: 0.0979 - val_mse: 0.0183\n",
      "Epoch 15/1000\n",
      "16236/17121 [===========================>..] - ETA: 0s - loss: 0.0118 - mae: 0.0776 - mse: 0.0118\n",
      "Epoch 00015: val_loss improved from 0.01590 to 0.01535, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0117 - mae: 0.0775 - mse: 0.0117 - val_loss: 0.0153 - val_mae: 0.0843 - val_mse: 0.0153\n",
      "Epoch 16/1000\n",
      "16154/17121 [===========================>..] - ETA: 0s - loss: 0.0106 - mae: 0.0728 - mse: 0.0106\n",
      "Epoch 00016: val_loss improved from 0.01535 to 0.01496, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 34us/sample - loss: 0.0106 - mae: 0.0730 - mse: 0.0106 - val_loss: 0.0150 - val_mae: 0.0820 - val_mse: 0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0111 - mae: 0.0750 - mse: 0.0111\n",
      "Epoch 00017: val_loss did not improve from 0.01496\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0111 - mae: 0.0753 - mse: 0.0111 - val_loss: 0.0179 - val_mae: 0.0983 - val_mse: 0.0179\n",
      "Epoch 18/1000\n",
      "16154/17121 [===========================>..] - ETA: 0s - loss: 0.0104 - mae: 0.0723 - mse: 0.0104\n",
      "Epoch 00018: val_loss did not improve from 0.01496\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0105 - mae: 0.0724 - mse: 0.0105 - val_loss: 0.0164 - val_mae: 0.0895 - val_mse: 0.0164\n",
      "Epoch 19/1000\n",
      "16236/17121 [===========================>..] - ETA: 0s - loss: 0.0106 - mae: 0.0738 - mse: 0.0106\n",
      "Epoch 00019: val_loss did not improve from 0.01496\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0107 - mae: 0.0739 - mse: 0.0107 - val_loss: 0.0169 - val_mae: 0.0901 - val_mse: 0.0169\n",
      "Epoch 20/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0104 - mae: 0.0727 - mse: 0.0104\n",
      "Epoch 00020: val_loss improved from 0.01496 to 0.01467, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0104 - mae: 0.0729 - mse: 0.0104 - val_loss: 0.0147 - val_mae: 0.0843 - val_mse: 0.0147\n",
      "Epoch 21/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0097 - mae: 0.0707 - mse: 0.0097\n",
      "Epoch 00021: val_loss did not improve from 0.01467\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0100 - mae: 0.0709 - mse: 0.0100 - val_loss: 0.0173 - val_mae: 0.0925 - val_mse: 0.0173\n",
      "Epoch 22/1000\n",
      "16810/17121 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0701 - mse: 0.0097\n",
      "Epoch 00022: val_loss did not improve from 0.01467\n",
      "17121/17121 [==============================] - 1s 31us/sample - loss: 0.0097 - mae: 0.0701 - mse: 0.0097 - val_loss: 0.0148 - val_mae: 0.0866 - val_mse: 0.0148\n",
      "Epoch 23/1000\n",
      "16728/17121 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0727 - mse: 0.0102\n",
      "Epoch 00023: val_loss did not improve from 0.01467\n",
      "17121/17121 [==============================] - 1s 31us/sample - loss: 0.0102 - mae: 0.0729 - mse: 0.0102 - val_loss: 0.0156 - val_mae: 0.0879 - val_mse: 0.0156\n",
      "Epoch 24/1000\n",
      "16564/17121 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0701 - mse: 0.0097\n",
      "Epoch 00024: val_loss did not improve from 0.01467\n",
      "17121/17121 [==============================] - 1s 31us/sample - loss: 0.0096 - mae: 0.0698 - mse: 0.0096 - val_loss: 0.0172 - val_mae: 0.0889 - val_mse: 0.0172\n",
      "Epoch 25/1000\n",
      "16810/17121 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0679 - mse: 0.0091\n",
      "Epoch 00025: val_loss improved from 0.01467 to 0.01404, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0092 - mae: 0.0680 - mse: 0.0092 - val_loss: 0.0140 - val_mae: 0.0807 - val_mse: 0.0140\n",
      "Epoch 26/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0089 - mae: 0.0671 - mse: 0.0089\n",
      "Epoch 00026: val_loss improved from 0.01404 to 0.01396, saving model to model_checkpoint.h5\n",
      "17121/17121 [==============================] - 1s 38us/sample - loss: 0.0089 - mae: 0.0668 - mse: 0.0089 - val_loss: 0.0140 - val_mae: 0.0782 - val_mse: 0.0140\n",
      "Epoch 27/1000\n",
      "16236/17121 [===========================>..] - ETA: 0s - loss: 0.0088 - mae: 0.0662 - mse: 0.0088\n",
      "Epoch 00027: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 45us/sample - loss: 0.0089 - mae: 0.0666 - mse: 0.0089 - val_loss: 0.0151 - val_mae: 0.0837 - val_mse: 0.0151\n",
      "Epoch 28/1000\n",
      "15744/17121 [==========================>...] - ETA: 0s - loss: 0.0082 - mae: 0.0644 - mse: 0.0082\n",
      "Epoch 00028: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 36us/sample - loss: 0.0082 - mae: 0.0646 - mse: 0.0082 - val_loss: 0.0142 - val_mae: 0.0800 - val_mse: 0.0142\n",
      "Epoch 29/1000\n",
      "16154/17121 [===========================>..] - ETA: 0s - loss: 0.0085 - mae: 0.0653 - mse: 0.0085\n",
      "Epoch 00029: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0085 - mae: 0.0652 - mse: 0.0085 - val_loss: 0.0148 - val_mae: 0.0800 - val_mse: 0.0148\n",
      "Epoch 30/1000\n",
      "15416/17121 [==========================>...] - ETA: 0s - loss: 0.0083 - mae: 0.0648 - mse: 0.0083\n",
      "Epoch 00030: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 33us/sample - loss: 0.0082 - mae: 0.0647 - mse: 0.0082 - val_loss: 0.0141 - val_mae: 0.0819 - val_mse: 0.0141\n",
      "Epoch 31/1000\n",
      "16482/17121 [===========================>..] - ETA: 0s - loss: 0.0078 - mae: 0.0626 - mse: 0.0078\n",
      "Epoch 00031: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0078 - mae: 0.0627 - mse: 0.0078 - val_loss: 0.0141 - val_mae: 0.0798 - val_mse: 0.0141\n",
      "Epoch 32/1000\n",
      "16236/17121 [===========================>..] - ETA: 0s - loss: 0.0077 - mae: 0.0632 - mse: 0.0077\n",
      "Epoch 00032: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0077 - mae: 0.0634 - mse: 0.0077 - val_loss: 0.0144 - val_mae: 0.0820 - val_mse: 0.0144\n",
      "Epoch 33/1000\n",
      "16072/17121 [===========================>..] - ETA: 0s - loss: 0.0076 - mae: 0.0629 - mse: 0.0076\n",
      "Epoch 00033: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 32us/sample - loss: 0.0077 - mae: 0.0632 - mse: 0.0077 - val_loss: 0.0146 - val_mae: 0.0777 - val_mse: 0.0146\n",
      "Epoch 34/1000\n",
      "16318/17121 [===========================>..] - ETA: 0s - loss: 0.0076 - mae: 0.0622 - mse: 0.0076\n",
      "Epoch 00034: val_loss did not improve from 0.01396\n",
      "17121/17121 [==============================] - 1s 36us/sample - loss: 0.0077 - mae: 0.0624 - mse: 0.0077 - val_loss: 0.0159 - val_mae: 0.0852 - val_mse: 0.0159\n",
      "Elapsed time during model training:  20.719156980514526\n",
      "Size of training set 18691\n",
      "Train on 12987 samples, validate on 5704 samples\n",
      "Epoch 1/1000\n",
      "12716/12987 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.1478 - mse: 0.0450\n",
      "Epoch 00001: val_loss improved from inf to 0.02425, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0445,  mae:0.1469,  mse:0.0445,  val_loss:0.0243,  val_mae:0.1153,  val_mse:0.0243,  \n",
      "12987/12987 [==============================] - 2s 158us/sample - loss: 0.0445 - mae: 0.1469 - mse: 0.0445 - val_loss: 0.0243 - val_mae: 0.1153 - val_mse: 0.0243\n",
      "Epoch 2/1000\n",
      "12546/12987 [===========================>..] - ETA: 0s - loss: 0.0232 - mae: 0.1136 - mse: 0.0232\n",
      "Epoch 00002: val_loss improved from 0.02425 to 0.02374, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 106us/sample - loss: 0.0233 - mae: 0.1138 - mse: 0.0233 - val_loss: 0.0237 - val_mae: 0.1164 - val_mse: 0.0237\n",
      "Epoch 3/1000\n",
      "12308/12987 [===========================>..] - ETA: 0s - loss: 0.0207 - mae: 0.1068 - mse: 0.0207\n",
      "Epoch 00003: val_loss improved from 0.02374 to 0.02247, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 95us/sample - loss: 0.0209 - mae: 0.1073 - mse: 0.0209 - val_loss: 0.0225 - val_mae: 0.1065 - val_mse: 0.0225\n",
      "Epoch 4/1000\n",
      "12682/12987 [============================>.] - ETA: 0s - loss: 0.0187 - mae: 0.1015 - mse: 0.0187\n",
      "Epoch 00004: val_loss did not improve from 0.02247\n",
      "12987/12987 [==============================] - 1s 107us/sample - loss: 0.0188 - mae: 0.1016 - mse: 0.0188 - val_loss: 0.0247 - val_mae: 0.1112 - val_mse: 0.0247\n",
      "Epoch 5/1000\n",
      "12716/12987 [============================>.] - ETA: 0s - loss: 0.0181 - mae: 0.0990 - mse: 0.0181\n",
      "Epoch 00005: val_loss did not improve from 0.02247\n",
      "12987/12987 [==============================] - 1s 90us/sample - loss: 0.0182 - mae: 0.0991 - mse: 0.0182 - val_loss: 0.0270 - val_mae: 0.1124 - val_mse: 0.0270\n",
      "Epoch 6/1000\n",
      "12240/12987 [===========================>..] - ETA: 0s - loss: 0.0174 - mae: 0.0970 - mse: 0.0174\n",
      "Epoch 00006: val_loss improved from 0.02247 to 0.02188, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 90us/sample - loss: 0.0173 - mae: 0.0966 - mse: 0.0173 - val_loss: 0.0219 - val_mae: 0.1082 - val_mse: 0.0219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000\n",
      "12240/12987 [===========================>..] - ETA: 0s - loss: 0.0161 - mae: 0.0920 - mse: 0.0161\n",
      "Epoch 00007: val_loss improved from 0.02188 to 0.01884, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 90us/sample - loss: 0.0164 - mae: 0.0932 - mse: 0.0164 - val_loss: 0.0188 - val_mae: 0.0997 - val_mse: 0.0188\n",
      "Epoch 8/1000\n",
      "12342/12987 [===========================>..] - ETA: 0s - loss: 0.0161 - mae: 0.0937 - mse: 0.0161\n",
      "Epoch 00008: val_loss did not improve from 0.01884\n",
      "12987/12987 [==============================] - 1s 88us/sample - loss: 0.0161 - mae: 0.0938 - mse: 0.0161 - val_loss: 0.0227 - val_mae: 0.1085 - val_mse: 0.0227\n",
      "Epoch 9/1000\n",
      "12852/12987 [============================>.] - ETA: 0s - loss: 0.0152 - mae: 0.0900 - mse: 0.0152\n",
      "Epoch 00009: val_loss did not improve from 0.01884\n",
      "12987/12987 [==============================] - 1s 89us/sample - loss: 0.0151 - mae: 0.0899 - mse: 0.0151 - val_loss: 0.0191 - val_mae: 0.0938 - val_mse: 0.0191\n",
      "Epoch 10/1000\n",
      "12580/12987 [============================>.] - ETA: 0s - loss: 0.0148 - mae: 0.0884 - mse: 0.0148\n",
      "Epoch 00010: val_loss improved from 0.01884 to 0.01764, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 2s 120us/sample - loss: 0.0147 - mae: 0.0883 - mse: 0.0147 - val_loss: 0.0176 - val_mae: 0.0923 - val_mse: 0.0176\n",
      "Epoch 11/1000\n",
      "12546/12987 [===========================>..] - ETA: 0s - loss: 0.0147 - mae: 0.0881 - mse: 0.0147\n",
      "Epoch 00011: val_loss did not improve from 0.01764\n",
      "12987/12987 [==============================] - 2s 125us/sample - loss: 0.0147 - mae: 0.0881 - mse: 0.0147 - val_loss: 0.0178 - val_mae: 0.0914 - val_mse: 0.0178\n",
      "Epoch 12/1000\n",
      "12784/12987 [============================>.] - ETA: 0s - loss: 0.0136 - mae: 0.0848 - mse: 0.0136\n",
      "Epoch 00012: val_loss improved from 0.01764 to 0.01750, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 2s 128us/sample - loss: 0.0136 - mae: 0.0848 - mse: 0.0136 - val_loss: 0.0175 - val_mae: 0.0903 - val_mse: 0.0175\n",
      "Epoch 13/1000\n",
      "12614/12987 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0825 - mse: 0.0130\n",
      "Epoch 00013: val_loss improved from 0.01750 to 0.01728, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 109us/sample - loss: 0.0131 - mae: 0.0828 - mse: 0.0131 - val_loss: 0.0173 - val_mae: 0.0859 - val_mse: 0.0173\n",
      "Epoch 14/1000\n",
      "12546/12987 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0837 - mse: 0.0131\n",
      "Epoch 00014: val_loss improved from 0.01728 to 0.01538, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 93us/sample - loss: 0.0130 - mae: 0.0835 - mse: 0.0130 - val_loss: 0.0154 - val_mae: 0.0808 - val_mse: 0.0154\n",
      "Epoch 15/1000\n",
      "12512/12987 [===========================>..] - ETA: 0s - loss: 0.0122 - mae: 0.0799 - mse: 0.0122\n",
      "Epoch 00015: val_loss did not improve from 0.01538\n",
      "12987/12987 [==============================] - 1s 96us/sample - loss: 0.0122 - mae: 0.0800 - mse: 0.0122 - val_loss: 0.0189 - val_mae: 0.0948 - val_mse: 0.0189\n",
      "Epoch 16/1000\n",
      "12580/12987 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0823 - mse: 0.0126\n",
      "Epoch 00016: val_loss did not improve from 0.01538\n",
      "12987/12987 [==============================] - 1s 90us/sample - loss: 0.0126 - mae: 0.0823 - mse: 0.0126 - val_loss: 0.0158 - val_mae: 0.0837 - val_mse: 0.0158\n",
      "Epoch 17/1000\n",
      "12478/12987 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0786 - mse: 0.0119\n",
      "Epoch 00017: val_loss improved from 0.01538 to 0.01516, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 89us/sample - loss: 0.0119 - mae: 0.0787 - mse: 0.0119 - val_loss: 0.0152 - val_mae: 0.0812 - val_mse: 0.0152\n",
      "Epoch 18/1000\n",
      "12648/12987 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0766 - mse: 0.0113\n",
      "Epoch 00018: val_loss did not improve from 0.01516\n",
      "12987/12987 [==============================] - 1s 100us/sample - loss: 0.0114 - mae: 0.0767 - mse: 0.0114 - val_loss: 0.0189 - val_mae: 0.0935 - val_mse: 0.0189\n",
      "Epoch 19/1000\n",
      "12920/12987 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0766 - mse: 0.0110\n",
      "Epoch 00019: val_loss improved from 0.01516 to 0.01461, saving model to model_checkpoint.h5\n",
      "12987/12987 [==============================] - 1s 98us/sample - loss: 0.0110 - mae: 0.0766 - mse: 0.0110 - val_loss: 0.0146 - val_mae: 0.0806 - val_mse: 0.0146\n",
      "Epoch 20/1000\n",
      "12444/12987 [===========================>..] - ETA: 0s - loss: 0.0107 - mae: 0.0743 - mse: 0.0107\n",
      "Epoch 00020: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 95us/sample - loss: 0.0107 - mae: 0.0742 - mse: 0.0107 - val_loss: 0.0149 - val_mae: 0.0789 - val_mse: 0.0149\n",
      "Epoch 21/1000\n",
      "12308/12987 [===========================>..] - ETA: 0s - loss: 0.0110 - mae: 0.0766 - mse: 0.0110\n",
      "Epoch 00021: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 89us/sample - loss: 0.0109 - mae: 0.0765 - mse: 0.0109 - val_loss: 0.0177 - val_mae: 0.0937 - val_mse: 0.0177\n",
      "Epoch 22/1000\n",
      "12886/12987 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0749 - mse: 0.0106\n",
      "Epoch 00022: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 92us/sample - loss: 0.0107 - mae: 0.0750 - mse: 0.0107 - val_loss: 0.0154 - val_mae: 0.0807 - val_mse: 0.0154\n",
      "Epoch 23/1000\n",
      "12682/12987 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0719 - mse: 0.0099\n",
      "Epoch 00023: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 103us/sample - loss: 0.0099 - mae: 0.0719 - mse: 0.0099 - val_loss: 0.0165 - val_mae: 0.0889 - val_mse: 0.0165\n",
      "Epoch 24/1000\n",
      "12682/12987 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0734 - mse: 0.0103\n",
      "Epoch 00024: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 90us/sample - loss: 0.0102 - mae: 0.0732 - mse: 0.0102 - val_loss: 0.0156 - val_mae: 0.0804 - val_mse: 0.0156\n",
      "Epoch 25/1000\n",
      "12546/12987 [===========================>..] - ETA: 0s - loss: 0.0096 - mae: 0.0710 - mse: 0.0096\n",
      "Epoch 00025: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 103us/sample - loss: 0.0097 - mae: 0.0710 - mse: 0.0097 - val_loss: 0.0160 - val_mae: 0.0858 - val_mse: 0.0160\n",
      "Epoch 26/1000\n",
      "12342/12987 [===========================>..] - ETA: 0s - loss: 0.0098 - mae: 0.0726 - mse: 0.0098\n",
      "Epoch 00026: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 1s 97us/sample - loss: 0.0097 - mae: 0.0724 - mse: 0.0097 - val_loss: 0.0154 - val_mae: 0.0830 - val_mse: 0.0154\n",
      "Epoch 27/1000\n",
      "12954/12987 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0698 - mse: 0.0094\n",
      "Epoch 00027: val_loss did not improve from 0.01461\n",
      "12987/12987 [==============================] - 2s 122us/sample - loss: 0.0094 - mae: 0.0698 - mse: 0.0094 - val_loss: 0.0156 - val_mae: 0.0823 - val_mse: 0.0156\n",
      "Elapsed time during model training:  36.077392578125\n",
      "Size of training set 18691\n",
      "Train on 16472 samples, validate on 2219 samples\n",
      "Epoch 1/1000\n",
      "15947/16472 [============================>.] - ETA: 0s - loss: 0.0446 - mae: 0.1442 - mse: 0.0446\n",
      "Epoch 00001: val_loss improved from inf to 0.02133, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0439,  mae:0.1430,  mse:0.0439,  val_loss:0.0213,  val_mae:0.1087,  val_mse:0.0213,  \n",
      "16472/16472 [==============================] - 2s 117us/sample - loss: 0.0439 - mae: 0.1430 - mse: 0.0439 - val_loss: 0.0213 - val_mae: 0.1087 - val_mse: 0.0213\n",
      "Epoch 2/1000\n",
      "16021/16472 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.1104 - mse: 0.0226\n",
      "Epoch 00002: val_loss did not improve from 0.02133\n",
      "16472/16472 [==============================] - 2s 92us/sample - loss: 0.0224 - mae: 0.1100 - mse: 0.0224 - val_loss: 0.0220 - val_mae: 0.1088 - val_mse: 0.0220\n",
      "Epoch 3/1000\n",
      "16206/16472 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.1013 - mse: 0.0195\n",
      "Epoch 00003: val_loss did not improve from 0.02133\n",
      "16472/16472 [==============================] - 1s 76us/sample - loss: 0.0194 - mae: 0.1013 - mse: 0.0194 - val_loss: 0.0214 - val_mae: 0.1059 - val_mse: 0.0214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0185 - mae: 0.0980 - mse: 0.0185\n",
      "Epoch 00004: val_loss did not improve from 0.02133\n",
      "16472/16472 [==============================] - 1s 71us/sample - loss: 0.0185 - mae: 0.0979 - mse: 0.0185 - val_loss: 0.0231 - val_mae: 0.1138 - val_mse: 0.0231\n",
      "Epoch 5/1000\n",
      "16391/16472 [============================>.] - ETA: 0s - loss: 0.0178 - mae: 0.0978 - mse: 0.0178\n",
      "Epoch 00005: val_loss improved from 0.02133 to 0.01889, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 74us/sample - loss: 0.0178 - mae: 0.0977 - mse: 0.0178 - val_loss: 0.0189 - val_mae: 0.0966 - val_mse: 0.0189\n",
      "Epoch 6/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0166 - mae: 0.0925 - mse: 0.0166\n",
      "Epoch 00006: val_loss improved from 0.01889 to 0.01803, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 69us/sample - loss: 0.0165 - mae: 0.0921 - mse: 0.0165 - val_loss: 0.0180 - val_mae: 0.0934 - val_mse: 0.0180\n",
      "Epoch 7/1000\n",
      "16132/16472 [============================>.] - ETA: 0s - loss: 0.0164 - mae: 0.0919 - mse: 0.0164\n",
      "Epoch 00007: val_loss did not improve from 0.01803\n",
      "16472/16472 [==============================] - 1s 76us/sample - loss: 0.0164 - mae: 0.0918 - mse: 0.0164 - val_loss: 0.0191 - val_mae: 0.0976 - val_mse: 0.0191\n",
      "Epoch 8/1000\n",
      "15873/16472 [===========================>..] - ETA: 0s - loss: 0.0156 - mae: 0.0900 - mse: 0.0156\n",
      "Epoch 00008: val_loss did not improve from 0.01803\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0155 - mae: 0.0897 - mse: 0.0155 - val_loss: 0.0190 - val_mae: 0.1056 - val_mse: 0.0190\n",
      "Epoch 9/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0148 - mae: 0.0876 - mse: 0.0148\n",
      "Epoch 00009: val_loss did not improve from 0.01803\n",
      "16472/16472 [==============================] - 1s 68us/sample - loss: 0.0147 - mae: 0.0874 - mse: 0.0147 - val_loss: 0.0181 - val_mae: 0.0957 - val_mse: 0.0181\n",
      "Epoch 10/1000\n",
      "16428/16472 [============================>.] - ETA: 0s - loss: 0.0150 - mae: 0.0879 - mse: 0.0150\n",
      "Epoch 00010: val_loss improved from 0.01803 to 0.01580, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 80us/sample - loss: 0.0150 - mae: 0.0879 - mse: 0.0150 - val_loss: 0.0158 - val_mae: 0.0892 - val_mse: 0.0158\n",
      "Epoch 11/1000\n",
      "16391/16472 [============================>.] - ETA: 0s - loss: 0.0139 - mae: 0.0842 - mse: 0.0139\n",
      "Epoch 00011: val_loss improved from 0.01580 to 0.01549, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 71us/sample - loss: 0.0139 - mae: 0.0842 - mse: 0.0139 - val_loss: 0.0155 - val_mae: 0.0848 - val_mse: 0.0155\n",
      "Epoch 12/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0135 - mae: 0.0825 - mse: 0.0135\n",
      "Epoch 00012: val_loss did not improve from 0.01549\n",
      "16472/16472 [==============================] - 1s 75us/sample - loss: 0.0135 - mae: 0.0827 - mse: 0.0135 - val_loss: 0.0187 - val_mae: 0.0982 - val_mse: 0.0187\n",
      "Epoch 13/1000\n",
      "16169/16472 [============================>.] - ETA: 0s - loss: 0.0136 - mae: 0.0841 - mse: 0.0136\n",
      "Epoch 00013: val_loss did not improve from 0.01549\n",
      "16472/16472 [==============================] - 1s 74us/sample - loss: 0.0136 - mae: 0.0842 - mse: 0.0136 - val_loss: 0.0164 - val_mae: 0.0892 - val_mse: 0.0164\n",
      "Epoch 14/1000\n",
      "16243/16472 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0795 - mse: 0.0126\n",
      "Epoch 00014: val_loss did not improve from 0.01549\n",
      "16472/16472 [==============================] - 1s 69us/sample - loss: 0.0127 - mae: 0.0797 - mse: 0.0127 - val_loss: 0.0177 - val_mae: 0.0950 - val_mse: 0.0177\n",
      "Epoch 15/1000\n",
      "16206/16472 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0814 - mse: 0.0130\n",
      "Epoch 00015: val_loss improved from 0.01549 to 0.01440, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 89us/sample - loss: 0.0130 - mae: 0.0813 - mse: 0.0130 - val_loss: 0.0144 - val_mae: 0.0801 - val_mse: 0.0144\n",
      "Epoch 16/1000\n",
      "15836/16472 [===========================>..] - ETA: 0s - loss: 0.0128 - mae: 0.0808 - mse: 0.0128\n",
      "Epoch 00016: val_loss did not improve from 0.01440\n",
      "16472/16472 [==============================] - 1s 78us/sample - loss: 0.0127 - mae: 0.0808 - mse: 0.0127 - val_loss: 0.0160 - val_mae: 0.0873 - val_mse: 0.0160\n",
      "Epoch 17/1000\n",
      "16058/16472 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0771 - mse: 0.0119\n",
      "Epoch 00017: val_loss did not improve from 0.01440\n",
      "16472/16472 [==============================] - 1s 74us/sample - loss: 0.0118 - mae: 0.0769 - mse: 0.0118 - val_loss: 0.0154 - val_mae: 0.0858 - val_mse: 0.0154\n",
      "Epoch 18/1000\n",
      "15984/16472 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0779 - mse: 0.0119\n",
      "Epoch 00018: val_loss did not improve from 0.01440\n",
      "16472/16472 [==============================] - 1s 75us/sample - loss: 0.0119 - mae: 0.0780 - mse: 0.0119 - val_loss: 0.0159 - val_mae: 0.0869 - val_mse: 0.0159\n",
      "Epoch 19/1000\n",
      "16280/16472 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0740 - mse: 0.0110\n",
      "Epoch 00019: val_loss did not improve from 0.01440\n",
      "16472/16472 [==============================] - 1s 70us/sample - loss: 0.0110 - mae: 0.0741 - mse: 0.0110 - val_loss: 0.0165 - val_mae: 0.0889 - val_mse: 0.0165\n",
      "Epoch 20/1000\n",
      "15984/16472 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0757 - mse: 0.0115\n",
      "Epoch 00020: val_loss did not improve from 0.01440\n",
      "16472/16472 [==============================] - 1s 71us/sample - loss: 0.0114 - mae: 0.0757 - mse: 0.0114 - val_loss: 0.0162 - val_mae: 0.0889 - val_mse: 0.0162\n",
      "Epoch 21/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0110 - mae: 0.0744 - mse: 0.0110\n",
      "Epoch 00021: val_loss improved from 0.01440 to 0.01341, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 72us/sample - loss: 0.0110 - mae: 0.0746 - mse: 0.0110 - val_loss: 0.0134 - val_mae: 0.0772 - val_mse: 0.0134\n",
      "Epoch 22/1000\n",
      "15688/16472 [===========================>..] - ETA: 0s - loss: 0.0104 - mae: 0.0723 - mse: 0.0104\n",
      "Epoch 00022: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 1s 72us/sample - loss: 0.0104 - mae: 0.0723 - mse: 0.0104 - val_loss: 0.0168 - val_mae: 0.0969 - val_mse: 0.0168\n",
      "Epoch 23/1000\n",
      "16243/16472 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0728 - mse: 0.0106\n",
      "Epoch 00023: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 1s 70us/sample - loss: 0.0106 - mae: 0.0729 - mse: 0.0106 - val_loss: 0.0165 - val_mae: 0.0880 - val_mse: 0.0165\n",
      "Epoch 24/1000\n",
      "16243/16472 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0694 - mse: 0.0097\n",
      "Epoch 00024: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 1s 69us/sample - loss: 0.0098 - mae: 0.0695 - mse: 0.0098 - val_loss: 0.0162 - val_mae: 0.0865 - val_mse: 0.0162\n",
      "Epoch 25/1000\n",
      "16132/16472 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0720 - mse: 0.0103\n",
      "Epoch 00025: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 2s 99us/sample - loss: 0.0103 - mae: 0.0720 - mse: 0.0103 - val_loss: 0.0149 - val_mae: 0.0833 - val_mse: 0.0149\n",
      "Epoch 26/1000\n",
      "16021/16472 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0703 - mse: 0.0097\n",
      "Epoch 00026: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 1s 77us/sample - loss: 0.0098 - mae: 0.0707 - mse: 0.0098 - val_loss: 0.0163 - val_mae: 0.0850 - val_mse: 0.0163\n",
      "Epoch 27/1000\n",
      "16465/16472 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0702 - mse: 0.0101\n",
      "Epoch 00027: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 1s 77us/sample - loss: 0.0101 - mae: 0.0703 - mse: 0.0101 - val_loss: 0.0157 - val_mae: 0.0869 - val_mse: 0.0157\n",
      "Epoch 28/1000\n",
      "16132/16472 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0697 - mse: 0.0097\n",
      "Epoch 00028: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 2s 99us/sample - loss: 0.0097 - mae: 0.0696 - mse: 0.0097 - val_loss: 0.0136 - val_mae: 0.0764 - val_mse: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "16465/16472 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0678 - mse: 0.0092\n",
      "Epoch 00029: val_loss did not improve from 0.01341\n",
      "16472/16472 [==============================] - 2s 99us/sample - loss: 0.0092 - mae: 0.0678 - mse: 0.0092 - val_loss: 0.0148 - val_mae: 0.0796 - val_mse: 0.0148\n",
      "Epoch 30/1000\n",
      "16132/16472 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0659 - mse: 0.0087\n",
      "Epoch 00030: val_loss improved from 0.01341 to 0.01305, saving model to model_checkpoint.h5\n",
      "16472/16472 [==============================] - 1s 91us/sample - loss: 0.0087 - mae: 0.0658 - mse: 0.0087 - val_loss: 0.0130 - val_mae: 0.0749 - val_mse: 0.0130\n",
      "Epoch 31/1000\n",
      "16317/16472 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0694 - mse: 0.0094\n",
      "Epoch 00031: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 74us/sample - loss: 0.0094 - mae: 0.0693 - mse: 0.0094 - val_loss: 0.0137 - val_mae: 0.0789 - val_mse: 0.0137\n",
      "Epoch 32/1000\n",
      "15762/16472 [===========================>..] - ETA: 0s - loss: 0.0086 - mae: 0.0654 - mse: 0.0086\n",
      "Epoch 00032: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 69us/sample - loss: 0.0085 - mae: 0.0654 - mse: 0.0085 - val_loss: 0.0155 - val_mae: 0.0857 - val_mse: 0.0155\n",
      "Epoch 33/1000\n",
      "15910/16472 [===========================>..] - ETA: 0s - loss: 0.0087 - mae: 0.0661 - mse: 0.0087\n",
      "Epoch 00033: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 68us/sample - loss: 0.0087 - mae: 0.0663 - mse: 0.0087 - val_loss: 0.0192 - val_mae: 0.0991 - val_mse: 0.0192\n",
      "Epoch 34/1000\n",
      "16169/16472 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0655 - mse: 0.0086\n",
      "Epoch 00034: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 70us/sample - loss: 0.0085 - mae: 0.0653 - mse: 0.0085 - val_loss: 0.0139 - val_mae: 0.0761 - val_mse: 0.0139\n",
      "Epoch 35/1000\n",
      "15984/16472 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0660 - mse: 0.0085\n",
      "Epoch 00035: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 68us/sample - loss: 0.0085 - mae: 0.0660 - mse: 0.0085 - val_loss: 0.0132 - val_mae: 0.0755 - val_mse: 0.0132\n",
      "Epoch 36/1000\n",
      "16021/16472 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0630 - mse: 0.0079\n",
      "Epoch 00036: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0079 - mae: 0.0630 - mse: 0.0079 - val_loss: 0.0139 - val_mae: 0.0771 - val_mse: 0.0139\n",
      "Epoch 37/1000\n",
      "16021/16472 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0636 - mse: 0.0080\n",
      "Epoch 00037: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0080 - mae: 0.0639 - mse: 0.0080 - val_loss: 0.0144 - val_mae: 0.0824 - val_mse: 0.0144\n",
      "Epoch 38/1000\n",
      "16169/16472 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0623 - mse: 0.0079\n",
      "Epoch 00038: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0079 - mae: 0.0624 - mse: 0.0079 - val_loss: 0.0160 - val_mae: 0.0855 - val_mse: 0.0160\n",
      "Epoch 39/1000\n",
      "16095/16472 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0613 - mse: 0.0075\n",
      "Epoch 00039: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 70us/sample - loss: 0.0075 - mae: 0.0614 - mse: 0.0075 - val_loss: 0.0131 - val_mae: 0.0763 - val_mse: 0.0131\n",
      "Epoch 40/1000\n",
      "16391/16472 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0603 - mse: 0.0073\n",
      "Epoch 00040: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 69us/sample - loss: 0.0073 - mae: 0.0604 - mse: 0.0073 - val_loss: 0.0147 - val_mae: 0.0818 - val_mse: 0.0147\n",
      "Epoch 41/1000\n",
      "15984/16472 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0624 - mse: 0.0077\n",
      "Epoch 00041: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0077 - mae: 0.0624 - mse: 0.0077 - val_loss: 0.0152 - val_mae: 0.0854 - val_mse: 0.0152\n",
      "Epoch 42/1000\n",
      "16132/16472 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0601 - mse: 0.0072\n",
      "Epoch 00042: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 67us/sample - loss: 0.0073 - mae: 0.0603 - mse: 0.0073 - val_loss: 0.0185 - val_mae: 0.0966 - val_mse: 0.0185\n",
      "Epoch 43/1000\n",
      "16021/16472 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0617 - mse: 0.0074\n",
      "Epoch 00043: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 1s 90us/sample - loss: 0.0078 - mae: 0.0619 - mse: 0.0078 - val_loss: 0.0147 - val_mae: 0.0847 - val_mse: 0.0147\n",
      "Epoch 44/1000\n",
      "15947/16472 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0600 - mse: 0.0072\n",
      "Epoch 00044: val_loss did not improve from 0.01305\n",
      "16472/16472 [==============================] - 2s 96us/sample - loss: 0.0072 - mae: 0.0602 - mse: 0.0072 - val_loss: 0.0137 - val_mae: 0.0768 - val_mse: 0.0137\n",
      "Elapsed time during model training:  55.86262488365173\n",
      "Size of training set 18691\n",
      "Train on 17544 samples, validate on 1147 samples\n",
      "Epoch 1/1000\n",
      "17319/17544 [============================>.] - ETA: 0s - loss: 0.0402 - mae: 0.1416 - mse: 0.0402\n",
      "Epoch 00001: val_loss improved from inf to 0.01994, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0399,  mae:0.1411,  mse:0.0399,  val_loss:0.0199,  val_mae:0.0991,  val_mse:0.0199,  \n",
      "17544/17544 [==============================] - 3s 167us/sample - loss: 0.0399 - mae: 0.1411 - mse: 0.0399 - val_loss: 0.0199 - val_mae: 0.0991 - val_mse: 0.0199\n",
      "Epoch 2/1000\n",
      "17043/17544 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1084 - mse: 0.0220\n",
      "Epoch 00002: val_loss did not improve from 0.01994\n",
      "17544/17544 [==============================] - 2s 119us/sample - loss: 0.0220 - mae: 0.1085 - mse: 0.0220 - val_loss: 0.0235 - val_mae: 0.1179 - val_mse: 0.0235\n",
      "Epoch 3/1000\n",
      "17457/17544 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.1044 - mse: 0.0203\n",
      "Epoch 00003: val_loss did not improve from 0.01994\n",
      "17544/17544 [==============================] - 2s 102us/sample - loss: 0.0203 - mae: 0.1044 - mse: 0.0203 - val_loss: 0.0205 - val_mae: 0.0971 - val_mse: 0.0205\n",
      "Epoch 4/1000\n",
      "17250/17544 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.1011 - mse: 0.0191\n",
      "Epoch 00004: val_loss did not improve from 0.01994\n",
      "17544/17544 [==============================] - 2s 103us/sample - loss: 0.0192 - mae: 0.1013 - mse: 0.0192 - val_loss: 0.0243 - val_mae: 0.1147 - val_mse: 0.0243\n",
      "Epoch 5/1000\n",
      "17342/17544 [============================>.] - ETA: 0s - loss: 0.0173 - mae: 0.0950 - mse: 0.0173\n",
      "Epoch 00005: val_loss improved from 0.01994 to 0.01938, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 111us/sample - loss: 0.0173 - mae: 0.0951 - mse: 0.0173 - val_loss: 0.0194 - val_mae: 0.1024 - val_mse: 0.0194\n",
      "Epoch 6/1000\n",
      "17503/17544 [============================>.] - ETA: 0s - loss: 0.0165 - mae: 0.0924 - mse: 0.0165\n",
      "Epoch 00006: val_loss did not improve from 0.01938\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0165 - mae: 0.0924 - mse: 0.0165 - val_loss: 0.0210 - val_mae: 0.1040 - val_mse: 0.0210\n",
      "Epoch 7/1000\n",
      "17089/17544 [============================>.] - ETA: 0s - loss: 0.0169 - mae: 0.0935 - mse: 0.0169\n",
      "Epoch 00007: val_loss did not improve from 0.01938\n",
      "17544/17544 [==============================] - 2s 104us/sample - loss: 0.0168 - mae: 0.0933 - mse: 0.0168 - val_loss: 0.0207 - val_mae: 0.0995 - val_mse: 0.0207\n",
      "Epoch 8/1000\n",
      "17526/17544 [============================>.] - ETA: 0s - loss: 0.0157 - mae: 0.0901 - mse: 0.0157\n",
      "Epoch 00008: val_loss improved from 0.01938 to 0.01756, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 131us/sample - loss: 0.0157 - mae: 0.0901 - mse: 0.0157 - val_loss: 0.0176 - val_mae: 0.0931 - val_mse: 0.0176\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17434/17544 [============================>.] - ETA: 0s - loss: 0.0153 - mae: 0.0886 - mse: 0.0153\n",
      "Epoch 00009: val_loss improved from 0.01756 to 0.01594, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 113us/sample - loss: 0.0153 - mae: 0.0887 - mse: 0.0153 - val_loss: 0.0159 - val_mae: 0.0869 - val_mse: 0.0159\n",
      "Epoch 10/1000\n",
      "17411/17544 [============================>.] - ETA: 0s - loss: 0.0144 - mae: 0.0856 - mse: 0.0144\n",
      "Epoch 00010: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0144 - mae: 0.0856 - mse: 0.0144 - val_loss: 0.0218 - val_mae: 0.1119 - val_mse: 0.0218\n",
      "Epoch 11/1000\n",
      "17457/17544 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0835 - mse: 0.0140\n",
      "Epoch 00011: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0140 - mae: 0.0836 - mse: 0.0140 - val_loss: 0.0206 - val_mae: 0.1091 - val_mse: 0.0206\n",
      "Epoch 12/1000\n",
      "17273/17544 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0836 - mse: 0.0140\n",
      "Epoch 00012: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 107us/sample - loss: 0.0140 - mae: 0.0836 - mse: 0.0140 - val_loss: 0.0164 - val_mae: 0.0882 - val_mse: 0.0164\n",
      "Epoch 13/1000\n",
      "17250/17544 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0808 - mse: 0.0130\n",
      "Epoch 00013: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 107us/sample - loss: 0.0131 - mae: 0.0809 - mse: 0.0131 - val_loss: 0.0189 - val_mae: 0.0960 - val_mse: 0.0189\n",
      "Epoch 14/1000\n",
      "17388/17544 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0816 - mse: 0.0132\n",
      "Epoch 00014: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 109us/sample - loss: 0.0132 - mae: 0.0817 - mse: 0.0132 - val_loss: 0.0169 - val_mae: 0.0866 - val_mse: 0.0169\n",
      "Epoch 15/1000\n",
      "17135/17544 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0791 - mse: 0.0125\n",
      "Epoch 00015: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 107us/sample - loss: 0.0125 - mae: 0.0791 - mse: 0.0125 - val_loss: 0.0171 - val_mae: 0.0834 - val_mse: 0.0171\n",
      "Epoch 16/1000\n",
      "17135/17544 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0776 - mse: 0.0121\n",
      "Epoch 00016: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 107us/sample - loss: 0.0121 - mae: 0.0775 - mse: 0.0121 - val_loss: 0.0166 - val_mae: 0.0852 - val_mse: 0.0166\n",
      "Epoch 17/1000\n",
      "17089/17544 [============================>.] - ETA: 0s - loss: 0.0118 - mae: 0.0765 - mse: 0.0118\n",
      "Epoch 00017: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0118 - mae: 0.0764 - mse: 0.0118 - val_loss: 0.0164 - val_mae: 0.0889 - val_mse: 0.0164\n",
      "Epoch 18/1000\n",
      "17434/17544 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0756 - mse: 0.0114\n",
      "Epoch 00018: val_loss did not improve from 0.01594\n",
      "17544/17544 [==============================] - 2s 102us/sample - loss: 0.0115 - mae: 0.0756 - mse: 0.0115 - val_loss: 0.0163 - val_mae: 0.0905 - val_mse: 0.0163\n",
      "Epoch 19/1000\n",
      "17227/17544 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0758 - mse: 0.0114\n",
      "Epoch 00019: val_loss improved from 0.01594 to 0.01503, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 135us/sample - loss: 0.0114 - mae: 0.0760 - mse: 0.0114 - val_loss: 0.0150 - val_mae: 0.0852 - val_mse: 0.0150\n",
      "Epoch 20/1000\n",
      "17434/17544 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0756 - mse: 0.0114\n",
      "Epoch 00020: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 111us/sample - loss: 0.0113 - mae: 0.0756 - mse: 0.0113 - val_loss: 0.0162 - val_mae: 0.0882 - val_mse: 0.0162\n",
      "Epoch 21/1000\n",
      "17296/17544 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0743 - mse: 0.0111\n",
      "Epoch 00021: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 109us/sample - loss: 0.0111 - mae: 0.0744 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0810 - val_mse: 0.0153\n",
      "Epoch 22/1000\n",
      "17388/17544 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0732 - mse: 0.0107\n",
      "Epoch 00022: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 112us/sample - loss: 0.0107 - mae: 0.0733 - mse: 0.0107 - val_loss: 0.0167 - val_mae: 0.0911 - val_mse: 0.0167\n",
      "Epoch 23/1000\n",
      "17434/17544 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0719 - mse: 0.0105\n",
      "Epoch 00023: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 111us/sample - loss: 0.0105 - mae: 0.0719 - mse: 0.0105 - val_loss: 0.0152 - val_mae: 0.0804 - val_mse: 0.0152\n",
      "Epoch 24/1000\n",
      "17181/17544 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0729 - mse: 0.0106\n",
      "Epoch 00024: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 107us/sample - loss: 0.0106 - mae: 0.0730 - mse: 0.0106 - val_loss: 0.0158 - val_mae: 0.0865 - val_mse: 0.0158\n",
      "Epoch 25/1000\n",
      "17296/17544 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0695 - mse: 0.0098\n",
      "Epoch 00025: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 106us/sample - loss: 0.0097 - mae: 0.0694 - mse: 0.0097 - val_loss: 0.0160 - val_mae: 0.0827 - val_mse: 0.0160\n",
      "Epoch 26/1000\n",
      "17273/17544 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0695 - mse: 0.0098\n",
      "Epoch 00026: val_loss did not improve from 0.01503\n",
      "17544/17544 [==============================] - 2s 105us/sample - loss: 0.0098 - mae: 0.0694 - mse: 0.0098 - val_loss: 0.0158 - val_mae: 0.0858 - val_mse: 0.0158\n",
      "Epoch 27/1000\n",
      "17365/17544 [============================>.] - ETA: 0s - loss: 0.0096 - mae: 0.0690 - mse: 0.0096\n",
      "Epoch 00027: val_loss improved from 0.01503 to 0.01494, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0096 - mae: 0.0689 - mse: 0.0096 - val_loss: 0.0149 - val_mae: 0.0803 - val_mse: 0.0149\n",
      "Epoch 28/1000\n",
      "17503/17544 [============================>.] - ETA: 0s - loss: 0.0096 - mae: 0.0695 - mse: 0.0096\n",
      "Epoch 00028: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 108us/sample - loss: 0.0096 - mae: 0.0696 - mse: 0.0096 - val_loss: 0.0159 - val_mae: 0.0831 - val_mse: 0.0159\n",
      "Epoch 29/1000\n",
      "17457/17544 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0685 - mse: 0.0094\n",
      "Epoch 00029: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 110us/sample - loss: 0.0094 - mae: 0.0685 - mse: 0.0094 - val_loss: 0.0153 - val_mae: 0.0828 - val_mse: 0.0153\n",
      "Epoch 30/1000\n",
      "17319/17544 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0661 - mse: 0.0088\n",
      "Epoch 00030: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 3s 148us/sample - loss: 0.0089 - mae: 0.0661 - mse: 0.0089 - val_loss: 0.0150 - val_mae: 0.0820 - val_mse: 0.0150\n",
      "Epoch 31/1000\n",
      "17181/17544 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0666 - mse: 0.0089\n",
      "Epoch 00031: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 3s 148us/sample - loss: 0.0089 - mae: 0.0664 - mse: 0.0089 - val_loss: 0.0152 - val_mae: 0.0822 - val_mse: 0.0152\n",
      "Epoch 32/1000\n",
      "17296/17544 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0660 - mse: 0.0088\n",
      "Epoch 00032: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 109us/sample - loss: 0.0088 - mae: 0.0660 - mse: 0.0088 - val_loss: 0.0157 - val_mae: 0.0840 - val_mse: 0.0157\n",
      "Epoch 33/1000\n",
      "17411/17544 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0662 - mse: 0.0088\n",
      "Epoch 00033: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 106us/sample - loss: 0.0088 - mae: 0.0663 - mse: 0.0088 - val_loss: 0.0185 - val_mae: 0.0952 - val_mse: 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000\n",
      "17227/17544 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0646 - mse: 0.0086\n",
      "Epoch 00034: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 116us/sample - loss: 0.0086 - mae: 0.0646 - mse: 0.0086 - val_loss: 0.0152 - val_mae: 0.0806 - val_mse: 0.0152\n",
      "Epoch 35/1000\n",
      "17319/17544 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0637 - mse: 0.0083\n",
      "Epoch 00035: val_loss did not improve from 0.01494\n",
      "17544/17544 [==============================] - 2s 115us/sample - loss: 0.0083 - mae: 0.0637 - mse: 0.0083 - val_loss: 0.0158 - val_mae: 0.0853 - val_mse: 0.0158\n",
      "Epoch 36/1000\n",
      "17526/17544 [============================>.] - ETA: 0s - loss: 0.0084 - mae: 0.0644 - mse: 0.0084\n",
      "Epoch 00036: val_loss improved from 0.01494 to 0.01449, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 127us/sample - loss: 0.0084 - mae: 0.0644 - mse: 0.0084 - val_loss: 0.0145 - val_mae: 0.0764 - val_mse: 0.0145\n",
      "Epoch 37/1000\n",
      "17480/17544 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0627 - mse: 0.0079\n",
      "Epoch 00037: val_loss improved from 0.01449 to 0.01437, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 118us/sample - loss: 0.0079 - mae: 0.0627 - mse: 0.0079 - val_loss: 0.0144 - val_mae: 0.0790 - val_mse: 0.0144\n",
      "Epoch 38/1000\n",
      "17411/17544 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0643 - mse: 0.0083\n",
      "Epoch 00038: val_loss improved from 0.01437 to 0.01314, saving model to model_checkpoint.h5\n",
      "17544/17544 [==============================] - 2s 113us/sample - loss: 0.0083 - mae: 0.0642 - mse: 0.0083 - val_loss: 0.0131 - val_mae: 0.0755 - val_mse: 0.0131\n",
      "Epoch 39/1000\n",
      "17319/17544 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0629 - mse: 0.0079\n",
      "Epoch 00039: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 125us/sample - loss: 0.0079 - mae: 0.0630 - mse: 0.0079 - val_loss: 0.0142 - val_mae: 0.0768 - val_mse: 0.0142\n",
      "Epoch 40/1000\n",
      "17388/17544 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0615 - mse: 0.0077\n",
      "Epoch 00040: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 3s 150us/sample - loss: 0.0077 - mae: 0.0615 - mse: 0.0077 - val_loss: 0.0147 - val_mae: 0.0806 - val_mse: 0.0147\n",
      "Epoch 41/1000\n",
      "17250/17544 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0622 - mse: 0.0078\n",
      "Epoch 00041: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 127us/sample - loss: 0.0077 - mae: 0.0621 - mse: 0.0077 - val_loss: 0.0148 - val_mae: 0.0795 - val_mse: 0.0148\n",
      "Epoch 42/1000\n",
      "17388/17544 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0614 - mse: 0.0076\n",
      "Epoch 00042: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 103us/sample - loss: 0.0076 - mae: 0.0614 - mse: 0.0076 - val_loss: 0.0141 - val_mae: 0.0820 - val_mse: 0.0141\n",
      "Epoch 43/1000\n",
      "17365/17544 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0617 - mse: 0.0077\n",
      "Epoch 00043: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 103us/sample - loss: 0.0077 - mae: 0.0619 - mse: 0.0077 - val_loss: 0.0147 - val_mae: 0.0808 - val_mse: 0.0147\n",
      "Epoch 44/1000\n",
      "17227/17544 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0612 - mse: 0.0076\n",
      "Epoch 00044: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 112us/sample - loss: 0.0076 - mae: 0.0612 - mse: 0.0076 - val_loss: 0.0144 - val_mae: 0.0779 - val_mse: 0.0144\n",
      "Epoch 45/1000\n",
      "17319/17544 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0602 - mse: 0.0073\n",
      "Epoch 00045: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 103us/sample - loss: 0.0073 - mae: 0.0602 - mse: 0.0073 - val_loss: 0.0146 - val_mae: 0.0845 - val_mse: 0.0146\n",
      "Epoch 46/1000\n",
      "17158/17544 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0588 - mse: 0.0069\n",
      "Epoch 00046: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 110us/sample - loss: 0.0069 - mae: 0.0590 - mse: 0.0069 - val_loss: 0.0154 - val_mae: 0.0839 - val_mse: 0.0154\n",
      "Epoch 47/1000\n",
      "17388/17544 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0602 - mse: 0.0072\n",
      "Epoch 00047: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 105us/sample - loss: 0.0072 - mae: 0.0602 - mse: 0.0072 - val_loss: 0.0166 - val_mae: 0.0906 - val_mse: 0.0166\n",
      "Epoch 48/1000\n",
      "17273/17544 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0598 - mse: 0.0072\n",
      "Epoch 00048: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 112us/sample - loss: 0.0072 - mae: 0.0597 - mse: 0.0072 - val_loss: 0.0148 - val_mae: 0.0771 - val_mse: 0.0148\n",
      "Epoch 49/1000\n",
      "17181/17544 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0574 - mse: 0.0066\n",
      "Epoch 00049: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 3s 152us/sample - loss: 0.0066 - mae: 0.0573 - mse: 0.0066 - val_loss: 0.0141 - val_mae: 0.0768 - val_mse: 0.0141\n",
      "Epoch 50/1000\n",
      "17342/17544 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0585 - mse: 0.0069\n",
      "Epoch 00050: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 142us/sample - loss: 0.0070 - mae: 0.0587 - mse: 0.0070 - val_loss: 0.0142 - val_mae: 0.0784 - val_mse: 0.0142\n",
      "Epoch 51/1000\n",
      "17273/17544 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0580 - mse: 0.0067\n",
      "Epoch 00051: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 109us/sample - loss: 0.0067 - mae: 0.0579 - mse: 0.0067 - val_loss: 0.0143 - val_mae: 0.0760 - val_mse: 0.0143\n",
      "Epoch 52/1000\n",
      "17250/17544 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0571 - mse: 0.0065\n",
      "Epoch 00052: val_loss did not improve from 0.01314\n",
      "17544/17544 [==============================] - 2s 109us/sample - loss: 0.0065 - mae: 0.0572 - mse: 0.0065 - val_loss: 0.0133 - val_mae: 0.0745 - val_mse: 0.0133\n",
      "Elapsed time during model training:  105.67573308944702\n",
      "\n",
      "Time elapsed for hp opt: 996.4497489929199\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#r = gp_minimize(objective, space, n_calls=20, n_jobs=n_core)\n",
    "r = gp_minimize(objective, space, n_calls=20)\n",
    "end = time.time()\n",
    "print(\"\\nTime elapsed for hp opt: %s\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[212, 0.0016765613721795554, 14, 23, 0.06133593626865962]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot hp opt results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1304892358>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(r, yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use best hps to train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following must be defined\n",
    "algo = 'nn' #am I using XGBoost (xgb) or Neural Nets (nn)?\n",
    "total_frac = 1 #total fraction of data set to work with\n",
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = True #make True if the training data should be chosen randomly\n",
    "n_remote = 10000 #the n_remote most remote points will be added to training set if random_split = False\n",
    "USE_PCA = True #should I use PCA?\n",
    "N_COMPONENTS=400 #how many PCA Components should I use?\n",
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "cat_si_sd = False #make True if you want to concatenate size-indep and size-dep fps\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature$\n",
    "stacked = True #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 18 #number of cores to use\n",
    "if not stacked:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv' #path to size-dep data\n",
    "else:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/stacked.csv'\n",
    "if not stacked:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv' #path to size-indep data\n",
    "else:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/stacked.csv'\n",
    "if not stacked:\n",
    "    start_str_sd = 'CH4_v/v_248_bar'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "else:\n",
    "    start_str_sd = 'Density'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "\n",
    "start_str_si = 'filename'\n",
    "end_str_si = 'valence_pa'\n",
    "del_geometric_fp = False #make True if you want to ignore the geometric features\n",
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns\n",
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data\n",
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}\n",
    "n_trees = 50 #number of weak learners. Bigger is better until 5000\n",
    "save_pp = False #make True if you want to save the parity plot\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using following 420 features\n",
      "Mafp_Br1_C2_C1\n",
      "Mafp_Br1_C2_C2\n",
      "Mafp_Br1_C2_C3\n",
      "Mafp_Br1_C3_Br1\n",
      "Mafp_Br1_C3_C1\n",
      "Mafp_Br1_C3_C2\n",
      "Mafp_Br1_C3_C3\n",
      "Mafp_Br1_C3_C4\n",
      "Mafp_Br1_C3_N1\n",
      "Mafp_Br1_C3_N2\n",
      "Mafp_Br1_C3_N3\n",
      "Mafp_Br1_C3_O1\n",
      "Mafp_Br1_C4_Br1\n",
      "Mafp_Br1_C4_C2\n",
      "Mafp_Br1_C4_C3\n",
      "Mafp_Br1_C4_C4\n",
      "Mafp_Br1_C4_H1\n",
      "Mafp_Br1_C4_N1\n",
      "Mafp_Br1_C4_N2\n",
      "Mafp_Br1_C4_N3\n",
      "Mafp_Br1_C4_O1\n",
      "Mafp_Br1_C4_O2\n",
      "Mafp_Br1_N2_C2\n",
      "Mafp_Br1_N2_C3\n",
      "Mafp_Br1_N2_C4\n",
      "Mafp_Br1_N2_N1\n",
      "Mafp_Br1_N2_N2\n",
      "Mafp_Br1_N2_N3\n",
      "Mafp_Br1_N3_Br1\n",
      "Mafp_Br1_N3_C2\n",
      "Mafp_Br1_N3_C3\n",
      "Mafp_Br1_N3_H1\n",
      "Mafp_Br1_N3_N2\n",
      "Mafp_Br1_N3_O2\n",
      "Mafp_Br1_O2_C2\n",
      "Mafp_Br1_O2_C3\n",
      "Mafp_Br1_O2_C4\n",
      "Mafp_C1_C2_C2\n",
      "Mafp_C1_C2_C3\n",
      "Mafp_C1_C2_C4\n",
      "Mafp_C1_C2_F1\n",
      "Mafp_C1_C2_H1\n",
      "Mafp_C1_C2_O1\n",
      "Mafp_C1_C2_O2\n",
      "Mafp_C1_C3_C2\n",
      "Mafp_C1_C3_C3\n",
      "Mafp_C1_C3_C4\n",
      "Mafp_C1_C3_Cl1\n",
      "Mafp_C1_C3_F1\n",
      "Mafp_C1_C3_H1\n",
      "Mafp_C1_C3_N2\n",
      "Mafp_C1_C3_N3\n",
      "Mafp_C1_C3_O1\n",
      "Mafp_C1_C3_O2\n",
      "Mafp_C1_C4_C2\n",
      "Mafp_C1_C4_C3\n",
      "Mafp_C1_C4_C4\n",
      "Mafp_C1_C4_H1\n",
      "Mafp_C1_C4_O1\n",
      "Mafp_C1_C4_O2\n",
      "Mafp_C1_N2_C2\n",
      "Mafp_C1_N2_C3\n",
      "Mafp_C1_N2_N2\n",
      "Mafp_C1_N2_N3\n",
      "Mafp_C1_N3_C3\n",
      "Mafp_C1_N3_C4\n",
      "Mafp_C1_N3_N2\n",
      "Mafp_C1_N3_N3\n",
      "Mafp_C1_N3_O1\n",
      "Mafp_C1_O2_C3\n",
      "Mafp_C1_O2_C4\n",
      "Mafp_C2_C2_C2\n",
      "Mafp_C2_C2_C3\n",
      "Mafp_C2_C2_C4\n",
      "Mafp_C2_C2_Cl1\n",
      "Mafp_C2_C2_F1\n",
      "Mafp_C2_C2_H1\n",
      "Mafp_C2_C2_N1\n",
      "Mafp_C2_C2_N2\n",
      "Mafp_C2_C2_N3\n",
      "Mafp_C2_C2_O1\n",
      "Mafp_C2_C2_O2\n",
      "Mafp_C2_C3_C2\n",
      "Mafp_C2_C3_C3\n",
      "Mafp_C2_C3_C4\n",
      "Mafp_C2_C3_Cl1\n",
      "Mafp_C2_C3_F1\n",
      "Mafp_C2_C3_H1\n",
      "Mafp_C2_C3_N1\n",
      "Mafp_C2_C3_N2\n",
      "Mafp_C2_C3_N3\n",
      "Mafp_C2_C3_O1\n",
      "Mafp_C2_C3_O2\n",
      "Mafp_C2_C4_C2\n",
      "Mafp_C2_C4_C3\n",
      "Mafp_C2_C4_C4\n",
      "Mafp_C2_C4_Cl1\n",
      "Mafp_C2_C4_F1\n",
      "Mafp_C2_C4_H1\n",
      "Mafp_C2_C4_N2\n",
      "Mafp_C2_C4_N3\n",
      "Mafp_C2_C4_O1\n",
      "Mafp_C2_C4_O2\n",
      "Mafp_C2_N2_C2\n",
      "Mafp_C2_N2_C3\n",
      "Mafp_C2_N2_C4\n",
      "Mafp_C2_N2_Cl1\n",
      "Mafp_C2_N2_F1\n",
      "Mafp_C2_N2_H1\n",
      "Mafp_C2_N2_N1\n",
      "Mafp_C2_N2_N2\n",
      "Mafp_C2_N2_N3\n",
      "Mafp_C2_N2_O1\n",
      "Mafp_C2_N2_O2\n",
      "Mafp_C2_N3_C2\n",
      "Mafp_C2_N3_C3\n",
      "Mafp_C2_N3_C4\n",
      "Mafp_C2_N3_F1\n",
      "Mafp_C2_N3_H1\n",
      "Mafp_C2_N3_N2\n",
      "Mafp_C2_N3_N3\n",
      "Mafp_C2_N3_O1\n",
      "Mafp_C2_N3_O2\n",
      "Mafp_C2_O2_C2\n",
      "Mafp_C2_O2_C3\n",
      "Mafp_C2_O2_C4\n",
      "Mafp_C2_O2_Cl1\n",
      "Mafp_C2_O2_H1\n",
      "Mafp_C2_O2_N2\n",
      "Mafp_C2_O2_N3\n",
      "Mafp_C3_C2_C3\n",
      "Mafp_C3_C2_C4\n",
      "Mafp_C3_C2_Cl1\n",
      "Mafp_C3_C2_F1\n",
      "Mafp_C3_C2_H1\n",
      "Mafp_C3_C2_N1\n",
      "Mafp_C3_C2_N2\n",
      "Mafp_C3_C2_N3\n",
      "Mafp_C3_C2_O1\n",
      "Mafp_C3_C2_O2\n",
      "Mafp_C3_C3_C3\n",
      "Mafp_C3_C3_C4\n",
      "Mafp_C3_C3_Cl1\n",
      "Mafp_C3_C3_F1\n",
      "Mafp_C3_C3_H1\n",
      "Mafp_C3_C3_N1\n",
      "Mafp_C3_C3_N2\n",
      "Mafp_C3_C3_N3\n",
      "Mafp_C3_C3_O1\n",
      "Mafp_C3_C3_O2\n",
      "Mafp_C3_C4_C3\n",
      "Mafp_C3_C4_C4\n",
      "Mafp_C3_C4_Cl1\n",
      "Mafp_C3_C4_F1\n",
      "Mafp_C3_C4_H1\n",
      "Mafp_C3_C4_N1\n",
      "Mafp_C3_C4_N2\n",
      "Mafp_C3_C4_N3\n",
      "Mafp_C3_C4_O1\n",
      "Mafp_C3_C4_O2\n",
      "Mafp_C3_N2_C3\n",
      "Mafp_C3_N2_C4\n",
      "Mafp_C3_N2_Cl1\n",
      "Mafp_C3_N2_F1\n",
      "Mafp_C3_N2_H1\n",
      "Mafp_C3_N2_N1\n",
      "Mafp_C3_N2_N2\n",
      "Mafp_C3_N2_N3\n",
      "Mafp_C3_N2_O1\n",
      "Mafp_C3_N2_O2\n",
      "Mafp_C3_N3_C3\n",
      "Mafp_C3_N3_C4\n",
      "Mafp_C3_N3_Cl1\n",
      "Mafp_C3_N3_F1\n",
      "Mafp_C3_N3_H1\n",
      "Mafp_C3_N3_N1\n",
      "Mafp_C3_N3_N2\n",
      "Mafp_C3_N3_N3\n",
      "Mafp_C3_N3_O2\n",
      "Mafp_C3_O2_C3\n",
      "Mafp_C3_O2_C4\n",
      "Mafp_C3_O2_Cl1\n",
      "Mafp_C3_O2_F1\n",
      "Mafp_C3_O2_H1\n",
      "Mafp_C3_O2_N2\n",
      "Mafp_C3_O2_N3\n",
      "Mafp_C3_O2_O1\n",
      "Mafp_C3_O2_O2\n",
      "Mafp_C4_C2_C4\n",
      "Mafp_C4_C2_H1\n",
      "Mafp_C4_C2_N1\n",
      "Mafp_C4_C2_N2\n",
      "Mafp_C4_C2_N3\n",
      "Mafp_C4_C2_O1\n",
      "Mafp_C4_C2_O2\n",
      "Mafp_C4_C3_C4\n",
      "Mafp_C4_C3_Cl1\n",
      "Mafp_C4_C3_F1\n",
      "Mafp_C4_C3_H1\n",
      "Mafp_C4_C3_N1\n",
      "Mafp_C4_C3_N2\n",
      "Mafp_C4_C3_N3\n",
      "Mafp_C4_C3_O1\n",
      "Mafp_C4_C3_O2\n",
      "Mafp_C4_C4_C4\n",
      "Mafp_C4_C4_Cl1\n",
      "Mafp_C4_C4_F1\n",
      "Mafp_C4_C4_H1\n",
      "Mafp_C4_C4_N1\n",
      "Mafp_C4_C4_N2\n",
      "Mafp_C4_C4_N3\n",
      "Mafp_C4_C4_O1\n",
      "Mafp_C4_C4_O2\n",
      "Mafp_C4_N2_C4\n",
      "Mafp_C4_N2_H1\n",
      "Mafp_C4_N2_N1\n",
      "Mafp_C4_N2_N2\n",
      "Mafp_C4_N2_N3\n",
      "Mafp_C4_N2_O1\n",
      "Mafp_C4_N3_C4\n",
      "Mafp_C4_N3_F1\n",
      "Mafp_C4_N3_H1\n",
      "Mafp_C4_N3_N1\n",
      "Mafp_C4_N3_N2\n",
      "Mafp_C4_N3_N3\n",
      "Mafp_C4_N3_O1\n",
      "Mafp_C4_N3_O2\n",
      "Mafp_C4_O2_C4\n",
      "Mafp_C4_O2_Cl1\n",
      "Mafp_C4_O2_F1\n",
      "Mafp_C4_O2_H1\n",
      "Mafp_C4_O2_N2\n",
      "Mafp_C4_O2_N3\n",
      "Mafp_C4_O2_O1\n",
      "Mafp_C4_O2_O2\n",
      "Mafp_Cl1_C3_Cl1\n",
      "Mafp_Cl1_C3_H1\n",
      "Mafp_Cl1_C3_N1\n",
      "Mafp_Cl1_C3_N2\n",
      "Mafp_Cl1_C3_N3\n",
      "Mafp_Cl1_C3_O1\n",
      "Mafp_Cl1_C4_Cl1\n",
      "Mafp_Cl1_C4_H1\n",
      "Mafp_Cl1_C4_N2\n",
      "Mafp_Cl1_C4_N3\n",
      "Mafp_Cl1_C4_O1\n",
      "Mafp_Cl1_C4_O2\n",
      "Mafp_Cl1_N2_N1\n",
      "Mafp_Cl1_N2_N2\n",
      "Mafp_Cl1_N3_Cl1\n",
      "Mafp_Cl1_N3_H1\n",
      "Mafp_Cl1_N3_N2\n",
      "Mafp_F1_C3_F1\n",
      "Mafp_F1_C3_N1\n",
      "Mafp_F1_C3_N2\n",
      "Mafp_F1_C3_N3\n",
      "Mafp_F1_C3_O1\n",
      "Mafp_F1_C3_O2\n",
      "Mafp_F1_C4_F1\n",
      "Mafp_F1_C4_H1\n",
      "Mafp_F1_C4_N2\n",
      "Mafp_F1_C4_N3\n",
      "Mafp_F1_C4_O1\n",
      "Mafp_F1_C4_O2\n",
      "Mafp_F1_N2_N2\n",
      "Mafp_F1_N3_F1\n",
      "Mafp_F1_N3_H1\n",
      "Mafp_F1_N3_N1\n",
      "Mafp_F1_N3_N2\n",
      "Mafp_F1_N3_N3\n",
      "Mafp_H1_C2_H1\n",
      "Mafp_H1_C2_N1\n",
      "Mafp_H1_C2_N2\n",
      "Mafp_H1_C2_N3\n",
      "Mafp_H1_C2_O1\n",
      "Mafp_H1_C2_O2\n",
      "Mafp_H1_C3_H1\n",
      "Mafp_H1_C3_N1\n",
      "Mafp_H1_C3_N2\n",
      "Mafp_H1_C3_N3\n",
      "Mafp_H1_C3_O1\n",
      "Mafp_H1_C3_O2\n",
      "Mafp_H1_C4_H1\n",
      "Mafp_H1_C4_N2\n",
      "Mafp_H1_C4_N3\n",
      "Mafp_H1_C4_O1\n",
      "Mafp_H1_C4_O2\n",
      "Mafp_H1_N2_H1\n",
      "Mafp_H1_N2_N1\n",
      "Mafp_H1_N2_N2\n",
      "Mafp_H1_N2_N3\n",
      "Mafp_H1_N3_H1\n",
      "Mafp_H1_N3_N1\n",
      "Mafp_H1_N3_N2\n",
      "Mafp_H1_N3_N3\n",
      "Mafp_H1_N3_O2\n",
      "Mafp_H1_O2_H1\n",
      "Mafp_H1_O2_N2\n",
      "Mafp_H1_O2_O1\n",
      "Mafp_H1_O2_O2\n",
      "Mafp_N1_C2_N2\n",
      "Mafp_N1_C2_N3\n",
      "Mafp_N1_C2_O2\n",
      "Mafp_N1_C3_N2\n",
      "Mafp_N1_C3_N3\n",
      "Mafp_N1_C3_O1\n",
      "Mafp_N1_C3_O2\n",
      "Mafp_N1_C4_N2\n",
      "Mafp_N1_C4_N3\n",
      "Mafp_N1_C4_O2\n",
      "Mafp_N1_N2_N3\n",
      "Mafp_N1_N3_N2\n",
      "Mafp_N1_N3_N3\n",
      "Mafp_N2_C2_N2\n",
      "Mafp_N2_C2_N3\n",
      "Mafp_N2_C2_O2\n",
      "Mafp_N2_C3_N2\n",
      "Mafp_N2_C3_N3\n",
      "Mafp_N2_C3_O1\n",
      "Mafp_N2_C3_O2\n",
      "Mafp_N2_C4_N2\n",
      "Mafp_N2_C4_N3\n",
      "Mafp_N2_C4_O1\n",
      "Mafp_N2_C4_O2\n",
      "Mafp_N2_N2_N2\n",
      "Mafp_N2_N2_N3\n",
      "Mafp_N2_N2_O2\n",
      "Mafp_N2_N3_N2\n",
      "Mafp_N2_N3_N3\n",
      "Mafp_N2_N3_O2\n",
      "Mafp_N2_O2_N3\n",
      "Mafp_N3_C2_O1\n",
      "Mafp_N3_C3_N3\n",
      "Mafp_N3_C3_O1\n",
      "Mafp_N3_C3_O2\n",
      "Mafp_N3_C4_N3\n",
      "Mafp_N3_C4_O1\n",
      "Mafp_N3_C4_O2\n",
      "Mafp_N3_N2_N3\n",
      "Mafp_N3_N2_O1\n",
      "Mafp_N3_N2_O2\n",
      "Mafp_N3_N3_N3\n",
      "Mafp_N3_N3_O1\n",
      "Mafp_N3_N3_O2\n",
      "Mafp_N3_O2_N3\n",
      "Mafp_O1_C2_O1\n",
      "Mafp_O1_C2_O2\n",
      "Mafp_O1_C3_O1\n",
      "Mafp_O1_C3_O2\n",
      "Mafp_O1_C4_O1\n",
      "Mafp_O1_C4_O2\n",
      "Mafp_O1_N3_O2\n",
      "Mafp_O2_C2_O2\n",
      "Mafp_O2_C3_O2\n",
      "Mafp_O2_C4_O2\n",
      "Mafp_O2_N3_O2\n",
      "Mefp_fam_acrylate\n",
      "Mefp_fam_carbonateester\n",
      "Mefp_fam_ketone\n",
      "Mefp_fam_polyamides\n",
      "Mefp_fam_single\n",
      "Mefp_norm_mol_wt\n",
      "Mefp_numatoms_none_H\n",
      "Mefp_ring\n",
      "Mmfp_Chi0n\n",
      "Mmfp_Chi0v\n",
      "Mmfp_Chi1n\n",
      "Mmfp_Chi1v\n",
      "Mmfp_Chi2n\n",
      "Mmfp_Chi2v\n",
      "Mmfp_HallKierAlpha\n",
      "Mmfp_MQNs13\n",
      "Mmfp_MQNs14\n",
      "Mmfp_MQNs15\n",
      "Mmfp_MQNs16\n",
      "Mmfp_MQNs17\n",
      "Mmfp_MQNs18\n",
      "Mmfp_MQNs19\n",
      "Mmfp_MQNs20\n",
      "Mmfp_MQNs21\n",
      "Mmfp_MQNs22\n",
      "Mmfp_MQNs23\n",
      "Mmfp_MQNs24\n",
      "Mmfp_MQNs25\n",
      "Mmfp_MQNs26\n",
      "Mmfp_MQNs27\n",
      "Mmfp_MQNs28\n",
      "Mmfp_MQNs29\n",
      "Mmfp_MQNs30\n",
      "Mmfp_MQNs31\n",
      "Mmfp_MQNs32\n",
      "Mmfp_MQNs33\n",
      "Mmfp_MQNs34\n",
      "Mmfp_MQNs35\n",
      "Mmfp_MQNs36\n",
      "Mmfp_MQNs37\n",
      "Mmfp_MQNs38\n",
      "Mmfp_MQNs39\n",
      "Mmfp_MQNs40\n",
      "Mmfp_MQNs41\n",
      "Mmfp_MQNs42\n",
      "Mmfp_NumAliphaticRings\n",
      "Mmfp_NumAromaticRings\n",
      "Mmfp_tpsa\n",
      "norm_valence_pa\n",
      "norm_atomic_rad_pa_(angstroms)\n",
      "norm_affinity_pa_(eV)\n",
      "norm_ionization_potential_pa_(eV)\n",
      "norm_electronegativity_pa\n",
      "norm_Dom._Pore_(ang.)\n",
      "norm_Max._Pore_(ang.)\n",
      "norm_Void_Fraction\n",
      "norm_Surf._Area_(m2/g)\n",
      "norm_Vol._Surf._Area\n",
      "norm_Density\n",
      "oh_1\n",
      "oh_2\n",
      "oh_3\n",
      "oh_4\n",
      "norm_log_pressure\n",
      "The following columns have been dropped: []\n",
      "Total len of test_df + train_df: 533430\n"
     ]
    }
   ],
   "source": [
    "if not stacked:\n",
    "    ml_data, property_used, target_mean, target_std, features = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "if stacked:\n",
    "    ml_data, property_used, target_mean, target_std, features, p_info = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "\n",
    "ml_data.head()\n",
    "\n",
    "train_df, test_df= ml.trainTestSplit(ml_data, property_used, training_pct, stacked, \n",
    "                                     n_core, random_split, n_remote, features, USE_PCA, N_COMPONENTS)\n",
    "\n",
    "if algo == 'xgb':\n",
    "    train_d, test_d, train_label, test_label = ml.alter_dtype(train_df, test_df, property_used, n_core, algo, features)\n",
    "else:\n",
    "    train_d, test_d, train_label, test_label = ml.alter_dtype(train_df, test_df, property_used, n_core, algo, features)\n",
    "\n",
    "len(train_label) + len(test_label)\n",
    "\n",
    "# Run Single Model\n",
    "\n",
    "#Good parameters\n",
    "\n",
    "SAVE_FIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = r.x\n",
    "params = [204, 0.001, 15, 4, 0.01]\n",
    "SCALE_BATCH = False\n",
    "BATCH_IND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 369807 samples, validate on 3736 samples\n",
      "Epoch 1/1000\n",
      "369752/369807 [============================>.] - ETA: 0s - loss: 0.0177 - mae: 0.0931 - mse: 0.0177\n",
      "Epoch 00001: val_loss improved from inf to 0.01161, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.0176,  mae:0.0931,  mse:0.0176,  val_loss:0.0116,  val_mae:0.0753,  val_mse:0.0116,  \n",
      "369807/369807 [==============================] - 231s 626us/sample - loss: 0.0176 - mae: 0.0931 - mse: 0.0176 - val_loss: 0.0116 - val_mae: 0.0753 - val_mse: 0.0116\n",
      "Epoch 2/1000\n",
      "369776/369807 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0783 - mse: 0.0130\n",
      "Epoch 00002: val_loss did not improve from 0.01161\n",
      "369807/369807 [==============================] - 235s 636us/sample - loss: 0.0130 - mae: 0.0783 - mse: 0.0130 - val_loss: 0.0130 - val_mae: 0.0800 - val_mse: 0.0130\n",
      "Epoch 3/1000\n",
      "369800/369807 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0748 - mse: 0.0120\n",
      "Epoch 00003: val_loss did not improve from 0.01161\n",
      "369807/369807 [==============================] - 233s 631us/sample - loss: 0.0120 - mae: 0.0748 - mse: 0.0120 - val_loss: 0.0122 - val_mae: 0.0795 - val_mse: 0.0122\n",
      "Epoch 4/1000\n",
      "369792/369807 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0727 - mse: 0.0114\n",
      "Epoch 00004: val_loss improved from 0.01161 to 0.01036, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 228s 615us/sample - loss: 0.0114 - mae: 0.0727 - mse: 0.0114 - val_loss: 0.0104 - val_mae: 0.0695 - val_mse: 0.0104\n",
      "Epoch 5/1000\n",
      "369792/369807 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0712 - mse: 0.0110\n",
      "Epoch 00005: val_loss did not improve from 0.01036\n",
      "369807/369807 [==============================] - 225s 608us/sample - loss: 0.0110 - mae: 0.0712 - mse: 0.0110 - val_loss: 0.0109 - val_mae: 0.0735 - val_mse: 0.0109\n",
      "Epoch 6/1000\n",
      "369740/369807 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0702 - mse: 0.0107\n",
      "Epoch 00006: val_loss did not improve from 0.01036\n",
      "369807/369807 [==============================] - 221s 596us/sample - loss: 0.0107 - mae: 0.0702 - mse: 0.0107 - val_loss: 0.0114 - val_mae: 0.0737 - val_mse: 0.0114\n",
      "Epoch 7/1000\n",
      "369780/369807 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0695 - mse: 0.0105\n",
      "Epoch 00007: val_loss improved from 0.01036 to 0.00970, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 219s 593us/sample - loss: 0.0105 - mae: 0.0695 - mse: 0.0105 - val_loss: 0.0097 - val_mae: 0.0694 - val_mse: 0.0097\n",
      "Epoch 8/1000\n",
      "369804/369807 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0687 - mse: 0.0103\n",
      "Epoch 00008: val_loss did not improve from 0.00970\n",
      "369807/369807 [==============================] - 225s 609us/sample - loss: 0.0103 - mae: 0.0687 - mse: 0.0103 - val_loss: 0.0098 - val_mae: 0.0681 - val_mse: 0.0098\n",
      "Epoch 9/1000\n",
      "369800/369807 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0683 - mse: 0.0101\n",
      "Epoch 00009: val_loss improved from 0.00970 to 0.00942, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 221s 598us/sample - loss: 0.0101 - mae: 0.0683 - mse: 0.0101 - val_loss: 0.0094 - val_mae: 0.0672 - val_mse: 0.0094\n",
      "Epoch 10/1000\n",
      "369752/369807 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0678 - mse: 0.0100\n",
      "Epoch 00010: val_loss did not improve from 0.00942\n",
      "369807/369807 [==============================] - 224s 606us/sample - loss: 0.0100 - mae: 0.0678 - mse: 0.0100 - val_loss: 0.0121 - val_mae: 0.0845 - val_mse: 0.0121\n",
      "Epoch 11/1000\n",
      "369784/369807 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0673 - mse: 0.0098\n",
      "Epoch 00011: val_loss did not improve from 0.00942\n",
      "369807/369807 [==============================] - 225s 609us/sample - loss: 0.0098 - mae: 0.0673 - mse: 0.0098 - val_loss: 0.0096 - val_mae: 0.0696 - val_mse: 0.0096\n",
      "Epoch 12/1000\n",
      "369720/369807 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0671 - mse: 0.0098\n",
      "Epoch 00012: val_loss did not improve from 0.00942\n",
      "369807/369807 [==============================] - 224s 607us/sample - loss: 0.0098 - mae: 0.0671 - mse: 0.0098 - val_loss: 0.0095 - val_mae: 0.0665 - val_mse: 0.0095\n",
      "Epoch 13/1000\n",
      "369792/369807 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0669 - mse: 0.0097\n",
      "Epoch 00013: val_loss improved from 0.00942 to 0.00910, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 217s 587us/sample - loss: 0.0097 - mae: 0.0669 - mse: 0.0097 - val_loss: 0.0091 - val_mae: 0.0654 - val_mse: 0.0091\n",
      "Epoch 14/1000\n",
      "369760/369807 [============================>.] - ETA: 0s - loss: 0.0096 - mae: 0.0666 - mse: 0.0096\n",
      "Epoch 00014: val_loss did not improve from 0.00910\n",
      "369807/369807 [==============================] - 215s 580us/sample - loss: 0.0096 - mae: 0.0666 - mse: 0.0096 - val_loss: 0.0095 - val_mae: 0.0681 - val_mse: 0.0095\n",
      "Epoch 15/1000\n",
      "369732/369807 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0664 - mse: 0.0095\n",
      "Epoch 00015: val_loss did not improve from 0.00910\n",
      "369807/369807 [==============================] - 218s 589us/sample - loss: 0.0095 - mae: 0.0664 - mse: 0.0095 - val_loss: 0.0140 - val_mae: 0.0952 - val_mse: 0.0140\n",
      "Epoch 16/1000\n",
      "369732/369807 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0662 - mse: 0.0095\n",
      "Epoch 00016: val_loss did not improve from 0.00910\n",
      "369807/369807 [==============================] - 229s 620us/sample - loss: 0.0095 - mae: 0.0662 - mse: 0.0095 - val_loss: 0.0105 - val_mae: 0.0755 - val_mse: 0.0105\n",
      "Epoch 17/1000\n",
      "369744/369807 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0659 - mse: 0.0094\n",
      "Epoch 00017: val_loss improved from 0.00910 to 0.00886, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 233s 631us/sample - loss: 0.0094 - mae: 0.0659 - mse: 0.0094 - val_loss: 0.0089 - val_mae: 0.0646 - val_mse: 0.0089\n",
      "Epoch 18/1000\n",
      "369788/369807 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0658 - mse: 0.0093\n",
      "Epoch 00018: val_loss did not improve from 0.00886\n",
      "369807/369807 [==============================] - 230s 622us/sample - loss: 0.0093 - mae: 0.0658 - mse: 0.0093 - val_loss: 0.0089 - val_mae: 0.0650 - val_mse: 0.0089\n",
      "Epoch 19/1000\n",
      "369788/369807 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0656 - mse: 0.0093\n",
      "Epoch 00019: val_loss did not improve from 0.00886\n",
      "369807/369807 [==============================] - 226s 612us/sample - loss: 0.0093 - mae: 0.0656 - mse: 0.0093 - val_loss: 0.0098 - val_mae: 0.0718 - val_mse: 0.0098\n",
      "Epoch 20/1000\n",
      "369732/369807 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0654 - mse: 0.0092\n",
      "Epoch 00020: val_loss did not improve from 0.00886\n",
      "369807/369807 [==============================] - 239s 645us/sample - loss: 0.0092 - mae: 0.0654 - mse: 0.0092 - val_loss: 0.0090 - val_mae: 0.0670 - val_mse: 0.0090\n",
      "Epoch 21/1000\n",
      "369784/369807 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0653 - mse: 0.0092\n",
      "Epoch 00021: val_loss improved from 0.00886 to 0.00878, saving model to model_checkpoint.h5\n",
      "369807/369807 [==============================] - 232s 627us/sample - loss: 0.0092 - mae: 0.0653 - mse: 0.0092 - val_loss: 0.0088 - val_mae: 0.0656 - val_mse: 0.0088\n",
      "Epoch 22/1000\n",
      "369780/369807 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0651 - mse: 0.0091\n",
      "Epoch 00022: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 233s 631us/sample - loss: 0.0091 - mae: 0.0651 - mse: 0.0091 - val_loss: 0.0112 - val_mae: 0.0810 - val_mse: 0.0112\n",
      "Epoch 23/1000\n",
      "369796/369807 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0651 - mse: 0.0091\n",
      "Epoch 00023: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 226s 612us/sample - loss: 0.0091 - mae: 0.0651 - mse: 0.0091 - val_loss: 0.0112 - val_mae: 0.0799 - val_mse: 0.0112\n",
      "Epoch 24/1000\n",
      "369760/369807 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0649 - mse: 0.0091\n",
      "Epoch 00024: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 230s 623us/sample - loss: 0.0091 - mae: 0.0649 - mse: 0.0091 - val_loss: 0.0090 - val_mae: 0.0661 - val_mse: 0.0090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "369784/369807 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0647 - mse: 0.0090\n",
      "Epoch 00025: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 236s 639us/sample - loss: 0.0090 - mae: 0.0647 - mse: 0.0090 - val_loss: 0.0098 - val_mae: 0.0718 - val_mse: 0.0098\n",
      "Epoch 26/1000\n",
      "369768/369807 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0647 - mse: 0.0090\n",
      "Epoch 00026: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 228s 616us/sample - loss: 0.0090 - mae: 0.0647 - mse: 0.0090 - val_loss: 0.0091 - val_mae: 0.0666 - val_mse: 0.0091\n",
      "Epoch 27/1000\n",
      "369732/369807 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0645 - mse: 0.0090\n",
      "Epoch 00027: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 229s 620us/sample - loss: 0.0090 - mae: 0.0645 - mse: 0.0090 - val_loss: 0.0094 - val_mae: 0.0700 - val_mse: 0.0094\n",
      "Epoch 28/1000\n",
      "369776/369807 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0644 - mse: 0.0089\n",
      "Epoch 00028: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 235s 635us/sample - loss: 0.0089 - mae: 0.0644 - mse: 0.0089 - val_loss: 0.0107 - val_mae: 0.0781 - val_mse: 0.0107\n",
      "Epoch 29/1000\n",
      "369736/369807 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0643 - mse: 0.0089\n",
      "Epoch 00029: val_loss did not improve from 0.00878\n",
      "369807/369807 [==============================] - 232s 627us/sample - loss: 0.0089 - mae: 0.0643 - mse: 0.0089 - val_loss: 0.0092 - val_mae: 0.0662 - val_mse: 0.0092\n",
      "Epoch 30/1000\n",
      "144240/369807 [==========>...................] - ETA: 2:18 - loss: 0.0087 - mae: 0.0639 - mse: 0.0087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0413 16:03:53.127338 139720804841216 callbacks.py:1286] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,mse\n",
      "W0413 16:03:53.128365 139720804841216 callbacks.py:1018] Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c1ae19b0b31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSAVE_FIG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparity_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_FIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py_scripts/efrc_ml_production.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(algo, train_d, n_trees, params, n_core)\u001b[0m\n\u001b[1;32m    504\u001b[0m             early_history = model.fit(fp, label, batch_size=BS,\n\u001b[1;32m    505\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAL_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                                   callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;31m##############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[1;32m    180\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_exhausted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m           self.callbacks._call_batch_hook(\n\u001b[0;32m--> 788\u001b[0;31m               mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    789\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m       elif (not self._is_tracing and\n\u001b[0;32m-> 1697\u001b[0;31m             math_ops.equal(train_batches, self._profile_batch - 1)):\n\u001b[0m\u001b[1;32m   1698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m   \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if SCALE_BATCH:\n",
    "    params[BATCH_IND] = int(params[BATCH_IND] * (total_frac/ total_frac_hp))\n",
    "\n",
    "SAVE_FIG = False\n",
    "\n",
    "MODEL = ml.run_model(algo, train_d, n_trees, params)\n",
    "ml.parity_plot(MODEL, train_d, test_d, stacked, algo, target_mean, target_std, property_used, test_label, train_label, save=SAVE_FIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02:05PM_on_April_13_2020'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%I:%M%p_on_%B_%d_%Y\")\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_units 212\n",
      "lr 0.0016765613721795554\n",
      "patience 14\n",
      "batch size 23\n",
      "validation split 0.06133593626865962\n"
     ]
    }
   ],
   "source": [
    "print(\"h_units %s\" %params[0])\n",
    "print(\"lr %s\" %params[1])\n",
    "print(\"patience %s\" %params[2])\n",
    "print(\"batch size %s\" %params[3])\n",
    "print(\"validation split %s\" %params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FIG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE is 8.47365149278117\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml.parity_plot(MODEL, train_d, test_d, stacked, algo, target_mean, target_std, property_used, test_label, train_label, save=SAVE_FIG, fname=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run below to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo=='xgb':\n",
    "    MODEL.save_model('/data/rgur/efrc/ml/models/%s/%s.xgb' %(now, now))\n",
    "else:\n",
    "    MODEL.save('/data/rgur/efrc/ml/models/%s/%s.h5' %(now, now),save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_df['filename'].to_csv('/data/rgur/efrc/ml/models/%s/train_%s.csv' %(now, now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['filename'].to_csv('/data/rgur/efrc/ml/models/%s/test_%s.csv' %(now, now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/rgur/efrc/ml/models/%s/features_%s.pkl' %(now, now), 'wb') as f:\n",
    "    pickle.dump(features, f, protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
