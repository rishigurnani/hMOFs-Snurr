{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize\n",
    "\n",
    "import importlib\n",
    "import efrc_ml_production as ml\n",
    "importlib.reload(ml)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin hp opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following must be defined\n",
    "algo = 'nn' #am I using XGBoost (xgb) or Neural Nets (nn)?\n",
    "total_frac_hp = .05 #total fraction of data set to work with\n",
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = True #make True if the training data should be chosen randomly\n",
    "n_remote = 10000 #the n_remote most remote points will be added to training set if random_split = False\n",
    "USE_PCA = True #should I use PCA?\n",
    "N_COMPONENTS=400 #how many PCA Components should I use?\n",
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "cat_si_sd = False #make True if you want to concatenate size-indep and size-dep fps\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature$\n",
    "stacked = True #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 18 #number of cores to use\n",
    "if not stacked:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv' #path to size-dep data\n",
    "else:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/stacked.csv'\n",
    "if not stacked:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv' #path to size-indep data\n",
    "else:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/stacked.csv'\n",
    "if not stacked:\n",
    "    start_str_sd = 'CH4_v/v_248_bar'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "else:\n",
    "    start_str_sd = 'Density'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "\n",
    "start_str_si = 'filename'\n",
    "end_str_si = 'valence_pa'\n",
    "del_geometric_fp = False #make True if you want to ignore the geometric features\n",
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns\n",
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data\n",
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}\n",
    "n_trees = 50 #number of weak learners. Bigger is better until 5000\n",
    "save_pp = False #make True if you want to save the parity plot\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps before hp_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not stacked:\n",
    "    ml_data_hp, property_used, target_mean, target_std, features = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac_hp, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "if stacked:\n",
    "    ml_data_hp, property_used, target_mean, target_std, features, p_info = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac_hp, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_hp, test_df_hp= ml.trainTestSplit(ml_data_hp, property_used, training_pct, stacked, \n",
    "                                     n_core, random_split, n_remote, features, USE_PCA, N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d_hp, test_d_hp, train_label_hp, test_label_hp = ml.alter_dtype(train_df_hp, test_df_hp, \n",
    "                                                                      property_used, n_core, algo, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_label_hp) + len(test_label_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    print(\"Size of training set %s\" %len(train_label_hp))\n",
    "    MODEL = ml.run_model(algo, train_d_hp, n_trees, params)\n",
    "    return ml.model_rmse(MODEL, train_d_hp, test_d_hp, stacked, algo, target_mean, target_std, property_used, \n",
    "                         test_label_hp, train_label_hp, save=False, fname=None, subset_inds=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with two hyperparameters in the model:<br>\n",
    "<br>\n",
    "1)Number of units in the first dense layer<br>\n",
    "2)Learning rate<br>\n",
    "3)Patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [(100, 400), #n_units\n",
    "        (.001, .002),#learning rate\n",
    "        (2, 15), #patience\n",
    "        (4, 128), #batch size\n",
    "        (.01, .6)] #validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#r = gp_minimize(objective, space, n_calls=20, n_jobs=n_core)\n",
    "r = gp_minimize(objective, space, n_calls=20)\n",
    "end = time.time()\n",
    "print(\"\\nTime elapsed for hp opt: %s\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot hp opt results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(r, yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use best hps to train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following must be defined\n",
    "algo = 'nn' #am I using XGBoost (xgb) or Neural Nets (nn)?\n",
    "total_frac = 1 #total fraction of data set to work with\n",
    "training_pct = .7 #how much percent of total fraction should be used for training\n",
    "random_split = True #make True if the training data should be chosen randomly\n",
    "n_remote = 10000 #the n_remote most remote points will be added to training set if random_split = False\n",
    "USE_PCA = True #should I use PCA?\n",
    "N_COMPONENTS=400 #how many PCA Components should I use?\n",
    "del_defective_mofs = False #make True if you want to remove all MOFs which a '0' value for at least one geometric property\n",
    "cat_si_sd = False #make True if you want to concatenate size-indep and size-dep fps\n",
    "add_size_fp = False #make True if you want to add 20 feature columns, where each feature is the number of atoms in a linker\n",
    "size_dependent = False #make True if the input ML-ready data contains fingerprint which does not normalize each PG feature$\n",
    "stacked = True #make True if the input ML-ready data contains pressure as feature\n",
    "n_core = 18 #number of cores to use\n",
    "if not stacked:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/ml_data.csv' #path to size-dep data\n",
    "else:\n",
    "    SD_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_no_norm/stacked.csv'\n",
    "if not stacked:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/ml_data.csv' #path to size-indep data\n",
    "else:\n",
    "    SI_ML_DATA_PATH = '/data/rgur/efrc/prep_data/all_v1/stacked.csv'\n",
    "if not stacked:\n",
    "    start_str_sd = 'CH4_v/v_248_bar'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "else:\n",
    "    start_str_sd = 'Density'\n",
    "    end_str_sd = 'norm_Dom._Pore_(ang.)'\n",
    "\n",
    "start_str_si = 'filename'\n",
    "end_str_si = 'valence_pa'\n",
    "del_geometric_fp = False #make True if you want to ignore the geometric features\n",
    "cat_col_names = ['oh_1', 'oh_2', 'oh_3', 'oh_4'] #names for interpenetration columns\n",
    "Y_DATA_PATH = '/data/rgur/efrc/data_DONOTTOUCH/hMOF_allData_March25_2013.xlsx' #path to original hMOF data\n",
    "default_params = {'objective':'reg:linear', 'colsample_bytree':0.3, 'learning_rate':0.1,\n",
    "                'max_depth':15, 'alpha':10, 'n_estimators':10}\n",
    "n_trees = 50 #number of weak learners. Bigger is better until 5000\n",
    "save_pp = False #make True if you want to save the parity plot\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stacked:\n",
    "    ml_data, property_used, target_mean, target_std, features = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "if stacked:\n",
    "    ml_data, property_used, target_mean, target_std, features, p_info = ml.prepToSplit(cat_si_sd, SD_ML_DATA_PATH, \n",
    "                                            SI_ML_DATA_PATH, start_str_sd, end_str_sd, start_str_si, end_str_si, \n",
    "                                            total_frac, del_defective_mofs, add_size_fp, size_dependent, stacked, n_core, \n",
    "                                            del_geometric_fp, cat_col_names, Y_DATA_PATH)\n",
    "\n",
    "ml_data.head()\n",
    "\n",
    "train_df, test_df= ml.trainTestSplit(ml_data, property_used, training_pct, stacked, \n",
    "                                     n_core, random_split, n_remote, features, USE_PCA, N_COMPONENTS)\n",
    "\n",
    "if algo == 'xgb':\n",
    "    train_d, test_d, train_label, test_label = ml.alter_dtype(train_df, test_df, property_used, n_core, algo, features)\n",
    "else:\n",
    "    train_d, test_d, train_label, test_label = ml.alter_dtype(train_df, test_df, property_used, n_core, algo, features)\n",
    "\n",
    "len(train_label) + len(test_label)\n",
    "\n",
    "# Run Single Model\n",
    "\n",
    "#Good parameters\n",
    "\n",
    "SAVE_FIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = r.x\n",
    "params = [204, 0.001, 15, 4, 0.01]\n",
    "SCALE_BATCH = False\n",
    "BATCH_IND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALE_BATCH:\n",
    "    params[BATCH_IND] = int(params[BATCH_IND] * (total_frac/ total_frac_hp))\n",
    "\n",
    "SAVE_FIG = False\n",
    "\n",
    "MODEL = ml.run_model(algo, train_d, n_trees, params)\n",
    "ml.parity_plot(MODEL, train_d, test_d, stacked, algo, target_mean, target_std, property_used, test_label, train_label, save=SAVE_FIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%I:%M%p_on_%B_%d_%Y\")\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"h_units %s\" %params[0])\n",
    "print(\"lr %s\" %params[1])\n",
    "print(\"patience %s\" %params[2])\n",
    "print(\"batch size %s\" %params[3])\n",
    "print(\"validation split %s\" %params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FIG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.parity_plot(MODEL, train_d, test_d, stacked, algo, target_mean, target_std, property_used, test_label, train_label, save=SAVE_FIG, fname=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run below to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo=='xgb':\n",
    "    MODEL.save_model('/data/rgur/efrc/ml/models/%s/%s.xgb' %(now, now))\n",
    "else:\n",
    "    MODEL.save('/data/rgur/efrc/ml/models/%s/%s.h5' %(now, now),save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['filename'].to_csv('/data/rgur/efrc/ml/models/%s/train_%s.csv' %(now, now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['filename'].to_csv('/data/rgur/efrc/ml/models/%s/test_%s.csv' %(now, now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/rgur/efrc/ml/models/%s/features_%s.pkl' %(now, now), 'wb') as f:\n",
    "    pickle.dump(features, f, protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}