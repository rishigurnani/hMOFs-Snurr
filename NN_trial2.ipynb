{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os import path\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "tickfontsize=20\n",
    "labelfontsize = tickfontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ml_data = pd.read_csv('~/efrc/prep_data/no_cat_v1/data_DONOTOUCH/ml_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>Metal_ID</th>\n",
       "      <th>#_of_Linkers</th>\n",
       "      <th>L0_Smiles</th>\n",
       "      <th>L1_Smiles</th>\n",
       "      <th>L2_Smiles</th>\n",
       "      <th>L3_Smiles</th>\n",
       "      <th>L4_Smiles</th>\n",
       "      <th>L5_Smiles</th>\n",
       "      <th>...</th>\n",
       "      <th>std_CH4_v/v_35_bar</th>\n",
       "      <th>norm_CH4_v/v_65_bar</th>\n",
       "      <th>mean_CH4_v/v_65_bar</th>\n",
       "      <th>std_CH4_v/v_65_bar</th>\n",
       "      <th>norm_CH4_v/v_100_bar</th>\n",
       "      <th>mean_CH4_v/v_100_bar</th>\n",
       "      <th>std_CH4_v/v_100_bar</th>\n",
       "      <th>norm_CH4_v/v_248_bar</th>\n",
       "      <th>mean_CH4_v/v_248_bar</th>\n",
       "      <th>std_CH4_v/v_248_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hypotheticalMOF_32526_i_2_j_11_k_9_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\Br</td>\n",
       "      <td>[O]C(=O)/C=C/C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.404660</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.395204</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.963777</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hypotheticalMOF_32003_i_2_j_11_k_2_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C(/C=C(/C(=O)[O])\\F)\\F</td>\n",
       "      <td>[O]C(=O)/C=C(/C(=C(/C(=O)[O])\\F)/F)\\F</td>\n",
       "      <td>Fc1nccc(c1F)c1ccc(cc1)c1c(F)c(F)nc(c1F)F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.075329</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hypotheticalMOF_1003468_i_4_j_6_k_2_m_10.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>N#Cc1c(C#N)c(ccc1c1cc(C#N)c(c(c1)C#N)C(=O)[O])...</td>\n",
       "      <td>N#Cc1cc(cc(c1C#N)C#N)C(=O)[O]</td>\n",
       "      <td>N#Cc1cc(C#N)c(c(c1C#N)C(=O)[O])C#N</td>\n",
       "      <td>N#Cc1c(c2ccc(cc2)C(=O)[O])c(C#N)c(c(c1C#N)c1cc...</td>\n",
       "      <td>N#Cc1c(C#N)cc(c(c1C#N)C(=O)[O])C#N</td>\n",
       "      <td>N#Cc1cc(ccc1C#Cc1ccc(cc1)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.133157</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hypotheticalMOF_3001711_i_2_j_25_k_25_m_13.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>CCCOC(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.255085</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hypotheticalMOF_3498_i_0_j_7_k_3_m_0.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.457618</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        filename  Metal_ID  \\\n",
       "0           0      hypotheticalMOF_32526_i_2_j_11_k_9_m_3.cif         2   \n",
       "1           1      hypotheticalMOF_32003_i_2_j_11_k_2_m_1.cif         2   \n",
       "2           2    hypotheticalMOF_1003468_i_4_j_6_k_2_m_10.cif         4   \n",
       "3           3  hypotheticalMOF_3001711_i_2_j_25_k_25_m_13.cif         2   \n",
       "4           4        hypotheticalMOF_3498_i_0_j_7_k_3_m_0.cif         0   \n",
       "\n",
       "   #_of_Linkers                                          L0_Smiles  \\\n",
       "0             2                     [O]C(=O)/C=C/C=C(/C(=O)[O])\\Br   \n",
       "1             3                  [O]C(=O)/C=C(/C=C(/C(=O)[O])\\F)\\F   \n",
       "2             9  N#Cc1c(C#N)c(ccc1c1cc(C#N)c(c(c1)C#N)C(=O)[O])...   \n",
       "3             9                                       CCCOC(=O)[O]   \n",
       "4             3                     [O]C(=O)c1ccc(c2c1CC2)C(=O)[O]   \n",
       "\n",
       "                               L1_Smiles  \\\n",
       "0                  [O]C(=O)/C=C/C(=O)[O]   \n",
       "1  [O]C(=O)/C=C(/C(=C(/C(=O)[O])\\F)/F)\\F   \n",
       "2          N#Cc1cc(cc(c1C#N)C#N)C(=O)[O]   \n",
       "3                           CCCOC(=O)[O]   \n",
       "4         [O]C(=O)c1ccc(c2c1CC2)C(=O)[O]   \n",
       "\n",
       "                                   L2_Smiles  \\\n",
       "0                                        NaN   \n",
       "1   Fc1nccc(c1F)c1ccc(cc1)c1c(F)c(F)nc(c1F)F   \n",
       "2         N#Cc1cc(C#N)c(c(c1C#N)C(=O)[O])C#N   \n",
       "3                               CCCOC(=O)[O]   \n",
       "4  [O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1)C(=O)[O]   \n",
       "\n",
       "                                           L3_Smiles  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  N#Cc1c(c2ccc(cc2)C(=O)[O])c(C#N)c(c(c1C#N)c1cc...   \n",
       "3                                       CCCOC(=O)[O]   \n",
       "4                                                NaN   \n",
       "\n",
       "                            L4_Smiles  \\\n",
       "0                                 NaN   \n",
       "1                                 NaN   \n",
       "2  N#Cc1c(C#N)cc(c(c1C#N)C(=O)[O])C#N   \n",
       "3                        CCCOC(=O)[O]   \n",
       "4                                 NaN   \n",
       "\n",
       "                                    L5_Smiles  ... std_CH4_v/v_35_bar  \\\n",
       "0                                         NaN  ...          38.761663   \n",
       "1                                         NaN  ...          38.761663   \n",
       "2  N#Cc1cc(ccc1C#Cc1ccc(cc1)C(=O)[O])C(=O)[O]  ...          38.761663   \n",
       "3                                CCCOC(=O)[O]  ...          38.761663   \n",
       "4                                         NaN  ...          38.761663   \n",
       "\n",
       "  norm_CH4_v/v_65_bar mean_CH4_v/v_65_bar std_CH4_v/v_65_bar  \\\n",
       "0           -0.404660          179.769656          39.064489   \n",
       "1            0.101287          179.769656          39.064489   \n",
       "2            0.098730          179.769656          39.064489   \n",
       "3                 NaN          179.769656          39.064489   \n",
       "4                 NaN          179.769656          39.064489   \n",
       "\n",
       "  norm_CH4_v/v_100_bar mean_CH4_v/v_100_bar std_CH4_v/v_100_bar  \\\n",
       "0            -0.395204           204.360203           42.586407   \n",
       "1             0.075329           204.360203           42.586407   \n",
       "2             0.133157           204.360203           42.586407   \n",
       "3             0.255085           204.360203           42.586407   \n",
       "4             0.457618           204.360203           42.586407   \n",
       "\n",
       "  norm_CH4_v/v_248_bar mean_CH4_v/v_248_bar std_CH4_v/v_248_bar  \n",
       "0            -0.963777           247.593271           56.068422  \n",
       "1             0.093458           247.593271           56.068422  \n",
       "2             0.172458           247.593271           56.068422  \n",
       "3                  NaN           247.593271           56.068422  \n",
       "4                  NaN           247.593271           56.068422  \n",
       "\n",
       "[5 rows x 469 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjustable parameters\n",
    "total_frac = 1\n",
    "start_str = 'SMILES'\n",
    "end_str = 'valence_pa'\n",
    "training_pct = .9\n",
    "patience = 10 #10 is Deepak default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>Metal_ID</th>\n",
       "      <th>#_of_Linkers</th>\n",
       "      <th>L0_Smiles</th>\n",
       "      <th>L1_Smiles</th>\n",
       "      <th>L2_Smiles</th>\n",
       "      <th>L3_Smiles</th>\n",
       "      <th>L4_Smiles</th>\n",
       "      <th>L5_Smiles</th>\n",
       "      <th>...</th>\n",
       "      <th>std_CH4_v/v_35_bar</th>\n",
       "      <th>norm_CH4_v/v_65_bar</th>\n",
       "      <th>mean_CH4_v/v_65_bar</th>\n",
       "      <th>std_CH4_v/v_65_bar</th>\n",
       "      <th>norm_CH4_v/v_100_bar</th>\n",
       "      <th>mean_CH4_v/v_100_bar</th>\n",
       "      <th>std_CH4_v/v_100_bar</th>\n",
       "      <th>norm_CH4_v/v_248_bar</th>\n",
       "      <th>mean_CH4_v/v_248_bar</th>\n",
       "      <th>std_CH4_v/v_248_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69176</th>\n",
       "      <td>69223</td>\n",
       "      <td>hypotheticalMOF_3593_i_0_j_7_k_4_m_2.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1Cl)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1Cl)/N=N/c1ccc(c(c1)Cl)C(=O)[O]</td>\n",
       "      <td>Clc1cc(C(=O)[O])c(cc1/N=N/c1c(Cl)cc(c(c1Cl)Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.354921</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21102</th>\n",
       "      <td>21110</td>\n",
       "      <td>hypotheticalMOF_5043233_i_1_j_23_k_9_m_1.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#C[C@@]12CC[C@@](C([C@H]1F)(F)F)([C@H...</td>\n",
       "      <td>[O]C(=O)/C=C(/C(=O)[O])\\F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.955686</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.810894</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.173564</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46293</th>\n",
       "      <td>46320</td>\n",
       "      <td>hypotheticalMOF_27246_i_1_j_18_k_18_m_2.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(Cl)c2c(c1)c1cc(Cl)c3c(c1cc2)cc(c(...</td>\n",
       "      <td>[O]C(=O)c1cc(Cl)c2c([c]1)c1[c]cc3c(c1cc2Cl)[c]...</td>\n",
       "      <td>Clc1cc2c(c3c1c(Cl)c(Cl)n[c]3)cc(c1c2cncc1Cl)Cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.016301</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18090</th>\n",
       "      <td>18096</td>\n",
       "      <td>hypotheticalMOF_5035712_i_1_j_20_k_0_m_5.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.465522</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.270543</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13754</th>\n",
       "      <td>13758</td>\n",
       "      <td>hypotheticalMOF_5031267_i_0_j_29_k_9_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C(=O)[O]</td>\n",
       "      <td>F/C(=C(\\C(=O)[O])/F)/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1c(F)cc(c(c1F)F)n1c(=O)c2c(F)cc3c4c2c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.005855</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.887350</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.592185</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39495</th>\n",
       "      <td>39517</td>\n",
       "      <td>hypotheticalMOF_5055665_i_1_j_28_k_21_m_4.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>C/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>N#C/C=C(/C#N)\\C</td>\n",
       "      <td>CO[C]1N(c2[c]cc(cc2C)C(=O)[O])C(=O)c2c3c1c(C)c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.090932</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.049613</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20167</td>\n",
       "      <td>hypotheticalMOF_35880_i_2_j_16_k_10_m_2.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[N]=CC(=[N])Cl.ClCl</td>\n",
       "      <td>[O]C(=O)/[C]=C/C=C/C(=O)[O].[Cl]</td>\n",
       "      <td>[O]C(=O)c1ccc2c([c]1)c1[c]cc(c(c1cc2Cl)C(=O)[O...</td>\n",
       "      <td>[CH]/[C]=C(\\Cl)/[CH]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.038009</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18876</th>\n",
       "      <td>18882</td>\n",
       "      <td>hypotheticalMOF_5008993_i_0_j_21_k_21_m_2.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>Cl/C(=C(\\C#CC(=O)[O])/Cl)/C#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-1.230404</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.556891</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.536843</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>12780</td>\n",
       "      <td>hypotheticalMOF_12646_i_0_j_16_k_1_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc2c(c1)c1ccc(c(c1c(c2F)F)C(=O)[O])F</td>\n",
       "      <td>Fc1cc(ccc1c1ccc(c(c1)F)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>FOC(=O)c1cc[c]c2c1[c]c(F)c1c2[c]c(c(c1)F)C(=O)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.441928</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65757</th>\n",
       "      <td>65799</td>\n",
       "      <td>hypotheticalMOF_8394_i_0_j_12_k_6_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCc1cc(C#Cc2cc(CC)c(c(c2CC)CC)C(=O)[O])c(c(c1C...</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>CCc1cc(ccc1C#Cc1c(CC)cc(c(c1CC)CC)C(=O)[O])C(=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.555553</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14868</th>\n",
       "      <td>14872</td>\n",
       "      <td>hypotheticalMOF_2000012_i_1_j_19_k_19_m_3.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br</td>\n",
       "      <td>[O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.126752</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7449</th>\n",
       "      <td>7450</td>\n",
       "      <td>hypotheticalMOF_1000614_i_3_j_7_k_2_m_1.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Fc1cc(cc(c1c1ccc(c(c1)F)C(=O)[O])F)c1cc(F)c(cc...</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1F)/N=N/c1ccc(c(c1F)F)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.292017</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2534</td>\n",
       "      <td>hypotheticalMOF_5010169_i_0_j_22_k_11_m_7.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\N</td>\n",
       "      <td>[O]C(=O)[C@]12[C@@H](N)[C@@H](N)[C@](C([C@H]1N...</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.512362</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.375926</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.365204</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36442</th>\n",
       "      <td>36459</td>\n",
       "      <td>hypotheticalMOF_5019737_i_0_j_26_k_0_m_9.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(O)c(c(c1)O)C(=O)[O]</td>\n",
       "      <td>O/C(=C\\c1[c]cc(c(c1O)O)C(=O)[O])/c1ccc(cc1)C(=...</td>\n",
       "      <td>[O]C(=O)c1c(O)c(O)c(c(c1O)O)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.457002</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62958</th>\n",
       "      <td>63000</td>\n",
       "      <td>hypotheticalMOF_5042527_i_1_j_23_k_0_m_10.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>N#Cc1ncc(nc1)C#N</td>\n",
       "      <td>N#C[C@@H]1[C@@H](C#N)[C@@]2(C#CC(=O)[O])[C@@H]...</td>\n",
       "      <td>N#Cc1cc(C(=O)[O])c(cc1C(=O)[O])C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.571949</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.704145</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.847480</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46722</th>\n",
       "      <td>46749</td>\n",
       "      <td>hypotheticalMOF_1000454_i_3_j_6_k_2_m_8.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.372548</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>2771</td>\n",
       "      <td>hypotheticalMOF_5013931_i_0_j_24_k_3_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CC[C@@H]1[C@@H](CC)c2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCC1(CC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CC[C@@]12[C@@H]3[C@@H]4[C@]2([C@@]2([C@H]1[C@]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.172103</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.304832</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63998</th>\n",
       "      <td>64040</td>\n",
       "      <td>hypotheticalMOF_5074221_i_2_j_26_k_6_m_9.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Oc1cc(C(=O)[O])c(cc1/C=C/c1c(O)cc(cc1O)C(=O)[O])O</td>\n",
       "      <td>OC1=N[CH]C=C([CH]1)C#Cc1ccncc1</td>\n",
       "      <td>Oc1cc(C(=O)[O])c(c(c1C#Cc1c(O)cc(c(c1O)O)C(=O)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.221266</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.162555</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65587</th>\n",
       "      <td>65629</td>\n",
       "      <td>hypotheticalMOF_5049712_i_1_j_26_k_12_m_1.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>F/C(=C\\c1cc(F)c(c(c1)F)C(=O)[O])/c1ccc(cc1)C(=...</td>\n",
       "      <td>F/C(=C\\c1ccc(cc1)C(=O)[O])/c1cc(F)c(c(c1)F)C(=...</td>\n",
       "      <td>Fc1[c]c(/C=[C]/c2cc(F)nc(c2F)F)cc(n1)F.FF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.394484</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59592</th>\n",
       "      <td>59630</td>\n",
       "      <td>hypotheticalMOF_5043805_i_1_j_23_k_18_m_4.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Cc1cn[c]c2c1ccc1c2ccc2c1[c]ncc2.[CH3].[CH3]</td>\n",
       "      <td>[O]C(=O)c1ccc2c(c1)c1c(C)cc3c(c1cc2)[c]c(c(c3C...</td>\n",
       "      <td>[O]C(=O)C#C[C@@]12[C@@H](C)[C@@H](C)[C@@](C([C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.039952</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47742</th>\n",
       "      <td>47769</td>\n",
       "      <td>hypotheticalMOF_5017542_i_0_j_25_k_9_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CC[CH]O/C(=C(\\C([O])[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CC[CH]O/C(=C\\C(=O)[O])/C(=O)O</td>\n",
       "      <td>CCCO[C@]12[C@@]3(C#CC(=O)[O])[C@H]4[C@@]2([C@]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.066911</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60628</th>\n",
       "      <td>60669</td>\n",
       "      <td>hypotheticalMOF_5007885_i_0_j_21_k_13_m_10.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>N#Cc1cc2cc(C(=O)[O])c(c(c2cc1C(=O)[O])C#N)C#N</td>\n",
       "      <td>N#Cc1cc2c(C#N)c(cc(c2cc1C(=O)[O])C#N)C(=O)[O]</td>\n",
       "      <td>N#C/C(=C(\\C#CC(=O)[O])/C#N)/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.072655</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.167162</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.383556</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75522</th>\n",
       "      <td>75574</td>\n",
       "      <td>hypotheticalMOF_8101_i_0_j_12_k_3_m_6.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>CCCc1cc(C(=O)[O])c2c(c1C(=O)[O])C([C@H]2CCC)(C...</td>\n",
       "      <td>CCCC1(CCC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-2.133011</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49203</th>\n",
       "      <td>49230</td>\n",
       "      <td>hypotheticalMOF_5053686_i_1_j_27_k_26_m_13.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>CCCOc1cc(cc(c1c1nnc(nn1)c1c(OCCC)cc(cc1OCCC)C(...</td>\n",
       "      <td>CCCOC1=N[CH]C(=C([CH]1)/[C]=C/c1c(OCCC)cnc(c1[...</td>\n",
       "      <td>CCOCOc1cc(ccc1/C(=[C]/c1[c]cc(cc1)C(=O)[O])/O[...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.481760</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36370</th>\n",
       "      <td>36387</td>\n",
       "      <td>hypotheticalMOF_18994_i_1_j_7_k_0_m_5.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>c1[c]ccnc1.CCc1[c]c(C(=O)[O])c(cc1/N=N/c1c(CC)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.286482</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.445445</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.157183</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64251</th>\n",
       "      <td>64293</td>\n",
       "      <td>hypotheticalMOF_7002430_i_4_j_28_k_3_m_9.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Oc1ccc(c(c1)O)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(c2c1CC2(O)O)C(=O)[O]</td>\n",
       "      <td>Oc1cc(O)cc(c1)C(=O)[O]</td>\n",
       "      <td>Oc1cc(O)c(c(c1)C(=O)[O])O</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1O)O)O</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)O)O</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.488360</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.827030</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.037695</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76287</th>\n",
       "      <td>76339</td>\n",
       "      <td>hypotheticalMOF_7002680_i_4_j_28_k_19_m_11.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>COc1cccc(c1)C(=O)[O]</td>\n",
       "      <td>COc1cccc(c1)C(=O)[O]</td>\n",
       "      <td>COc1cc(OC)cc(c1C(=O)[O])OC</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56573</th>\n",
       "      <td>56609</td>\n",
       "      <td>hypotheticalMOF_19577_i_1_j_7_k_6_m_8.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(c2ccccc2)c(c(c1c1ccccc1)c1ccccc1)...</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1c1ccccc1)C(=O)[O]</td>\n",
       "      <td>c1ccc(cc1)c1nccc(c1)C#Cc1ccncc1c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.626222</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31235</th>\n",
       "      <td>31248</td>\n",
       "      <td>hypotheticalMOF_1001308_i_3_j_10_k_9_m_12.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]C(/C=C/C(=O)[O])[O].[CH2]CO/C(=C(\\C(=O)[O])...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.653463</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.707547</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-1.201540</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42073</th>\n",
       "      <td>42097</td>\n",
       "      <td>hypotheticalMOF_31820_i_2_j_10_k_10_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Br/N=C\\C(=N)Br</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)/C=C/C(=[C]/C(=O)[O])/Br.[Br]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.418621</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41504</th>\n",
       "      <td>41527</td>\n",
       "      <td>hypotheticalMOF_5049272_i_1_j_26_k_5_m_11.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>N#Cc1ccc(c(c1)OC)C#N</td>\n",
       "      <td>COc1cc(C(=O)[O])c(cc1/C=C/c1c([O])c(OC)c(c(c1O...</td>\n",
       "      <td>COc1cc(C#CC(=O)[O])ccc1C#CC(=O)[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>C[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.374758</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.643970</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.839435</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52489</th>\n",
       "      <td>52520</td>\n",
       "      <td>hypotheticalMOF_30428_i_2_j_8_k_5_m_8.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#Cc1ccc(cc1c1ccccc1)[C@@]12[C@@H]3[CH...</td>\n",
       "      <td>N#Cc1ccc(c(c1)c1ccccc1)C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68755</th>\n",
       "      <td>68801</td>\n",
       "      <td>hypotheticalMOF_5020442_i_0_j_26_k_6_m_5.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCc1cc(C#Cc2ccc(cc2)C(=O)[O])ccc1C(=O)[O]</td>\n",
       "      <td>CCc1c(CC)c(/C=C/c2[c]c(CC)c(c(c2CC)CC)C(=O)[O]...</td>\n",
       "      <td>CCc1c(C#Cc2cc(CC)c(c(c2)CC)C(=O)[O])c(CC)c(c(c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.210698</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.552423</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.024328</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49811</th>\n",
       "      <td>49838</td>\n",
       "      <td>hypotheticalMOF_5045653_i_1_j_24_k_19_m_13.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#CC#CC(=O)[O]</td>\n",
       "      <td>CCCO[C@]12[C@H]3[C@H]4[C@@]2([C@H]2[C@@H]1[C@@...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.965705</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>797</td>\n",
       "      <td>hypotheticalMOF_6003141_i_3_j_24_k_21_m_11.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>CO[C@]12[C@@H]3[C@@]4([C@H]1[C@@]1([C@]2([C@]3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.481962</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31921</th>\n",
       "      <td>31934</td>\n",
       "      <td>hypotheticalMOF_3000716_i_1_j_26_k_23_m_8.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>N#CC(C#N)(C#N)C#N</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>[O]C(=O)c1ccccc1</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.143828</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24675</th>\n",
       "      <td>24685</td>\n",
       "      <td>hypotheticalMOF_6001587_i_3_j_21_k_5_m_12.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>CCOc1cc(C#CC(=O)[O])c(cc1C#CC(=O)[O])OCC</td>\n",
       "      <td>CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.535856</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>47981</td>\n",
       "      <td>hypotheticalMOF_15854_i_0_j_18_k_6_m_1.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Fc1cc(C(=O)[O])c(cc1C#Cc1c(F)cc(c(c1F)F)C(=O)[...</td>\n",
       "      <td>[O]C(=O)c1cc2c3c(F)cc4c(c3cc(c2cc1F)F)c(F)c(c(...</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)F)C#Cc1cc(F)c(cc1F)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.319036</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2497</td>\n",
       "      <td>hypotheticalMOF_5066677_i_2_j_22_k_7_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Brc1ncc(c(c1)/N=N/c1c(Br)cncc1Br)Br</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.227695</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73135</th>\n",
       "      <td>73185</td>\n",
       "      <td>hypotheticalMOF_31137_i_2_j_10_k_0_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Fc1cnccn1</td>\n",
       "      <td>[O]C(=O)/C=C/C=C/C(=O)[O]</td>\n",
       "      <td>[O]C(=O)/C=C/C=C(/C(=O)[O])\\F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.202383</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.490113</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>1872</td>\n",
       "      <td>hypotheticalMOF_5069694_i_2_j_24_k_5_m_4.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#Cc1ccc(cc1)C#CC(=O)[O]</td>\n",
       "      <td>N#Cc1cc(C)c(c(c1C)C)C#N</td>\n",
       "      <td>[O]C(=O)[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@]1(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>7878</td>\n",
       "      <td>hypotheticalMOF_6004262_i_3_j_27_k_2_m_14.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>N=Nc1cc(ccc1c1cc(N=N)c(cc1N=N)c1cc(N=N)c(cc1N=...</td>\n",
       "      <td>N=Nc1cc(C(=O)[O])c(c(c1c1nnc(nn1)c1ccc(c2c1n[n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.394362</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.388359</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.326295</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37619</th>\n",
       "      <td>37638</td>\n",
       "      <td>hypotheticalMOF_5008222_i_0_j_21_k_16_m_12.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCOc1c(O[CH2])c2c(ccc(c2c2c1cc(O[C]C)c(c2)[C][...</td>\n",
       "      <td>CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]</td>\n",
       "      <td>CCO/C(=C\\C#[C])/C#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.784153</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.470121</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70608</th>\n",
       "      <td>70656</td>\n",
       "      <td>hypotheticalMOF_5062990_i_2_j_20_k_16_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>Brc1c(Br)ncc2c1ccc1c2c(Br)ccn1.[O]C(=O)c1cc2c(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67699</th>\n",
       "      <td>67745</td>\n",
       "      <td>hypotheticalMOF_13406_i_0_j_16_k_10_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc2c1ccc1c2c(OCCC)cc(c1C(=O)[O])OCCC)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.365916</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.099079</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38804</th>\n",
       "      <td>38825</td>\n",
       "      <td>hypotheticalMOF_5046420_i_1_j_25_k_2_m_9.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Oc1[c]c(ccc1c1ccc(c(c1)O)C(=O)[O])c1c(O)c(O)c(...</td>\n",
       "      <td>[O]C(=O)C#C[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@...</td>\n",
       "      <td>Oc1ncc(c(c1)c1ccc(c(c1)O)c1cc(O)ncc1O)O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-0.440158</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.651970</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>6922</td>\n",
       "      <td>hypotheticalMOF_1003717_i_4_j_10_k_0_m_5.cif</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1c(ccc(c1CC)C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CCc1cc(C(=O)[O])c(c(c1C(=O)[O])CC)CC</td>\n",
       "      <td>CC/C(=C\\C(=O)[O])/C=C(/C(=O)[O])\\CC</td>\n",
       "      <td>CCc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-1.646670</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18983</th>\n",
       "      <td>18989</td>\n",
       "      <td>hypotheticalMOF_5044313_i_1_j_23_k_23_m_7.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C[C@]12CC[C@]([C@@H](C1)N)([C@@H]([C...</td>\n",
       "      <td>[O]C(=O)C#C[C@]12C[C@@H](N)[C@]([C@H]([C@@H]1N...</td>\n",
       "      <td>N#C[C@@]12C[C@@H](N)[C@@](C(C1)(N)N)([C@H]([C@...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.940483</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32230</th>\n",
       "      <td>32243</td>\n",
       "      <td>hypotheticalMOF_116_i_0_j_0_k_0_m_13.cif</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1)C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]</td>\n",
       "      <td>CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.558683</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.207872</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>17095</td>\n",
       "      <td>hypotheticalMOF_6001973_i_3_j_22_k_0_m_11.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>COc1cc(ccc1C(=O)[O])C(=O)[O]</td>\n",
       "      <td>CO[C@@H]1C[C@@]([CH2])(C(=O)[O])C([C@@H]([C]1C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-2.080005</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-2.395274</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-2.519903</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52620</th>\n",
       "      <td>52651</td>\n",
       "      <td>hypotheticalMOF_5072196_i_2_j_25_k_11_m_8.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N=CC(=N)c1ccccc1</td>\n",
       "      <td>[O]C(=O)/C(=C/C=C/C(=O)[O])/c1ccccc1</td>\n",
       "      <td>[O]C(=O)C#C[C@]12[C@@H]3[C@@H]4[C@H]1[C@@]1([C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.844760</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>39534</td>\n",
       "      <td>hypotheticalMOF_23807_i_1_j_14_k_1_m_10.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N#Cc1cc(C(=O)[O])c(cc1c1ccc(cc1C#N)C(=O)[O])C#N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.459399</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48600</th>\n",
       "      <td>48627</td>\n",
       "      <td>hypotheticalMOF_5082196_i_2_j_29_k_16_m_9.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>OC1=N[CH]c2c([C]1O)ccc1c2c(O)cc(n1)O</td>\n",
       "      <td>[O]C(=O)c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)C(=O)[O])O</td>\n",
       "      <td>OOc1n(c2[c]cc(c[c]2)C(=O)[O])c(OO)c2c3c1CC=c1c...</td>\n",
       "      <td>[O][C]c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)[C][O])O</td>\n",
       "      <td>[O][C]c1c(O)ccc2c1c(O)cc1c2c[c]c(c1O)O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.562211</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.364967</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55026</th>\n",
       "      <td>55061</td>\n",
       "      <td>hypotheticalMOF_6001255_i_3_j_20_k_14_m_5.cif</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>CCc1cccc2c1c(ccc2C(=O)[O])C(=O)[O]</td>\n",
       "      <td>[O]C(=O)C#CC#CC#CC(=O)[O]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.105969</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>42017</td>\n",
       "      <td>hypotheticalMOF_5075328_i_2_j_26_k_22_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>F/C(=C\\c1cc(F)c(c(c1F)F)C(=O)[O])/c1cc(F)c(cc1...</td>\n",
       "      <td>F[C@@H]1CN2[C@@H](CN1[C@H]([C@H]2F)F)F</td>\n",
       "      <td>[O]C(=O)[C@@]12CC[C@@](C(C1)(F)F)(C(C2(F)F)(F)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.346433</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>21251</td>\n",
       "      <td>hypotheticalMOF_32735_i_2_j_12_k_0_m_1.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1cc(F)c(cc1F)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)C(=O)[O]</td>\n",
       "      <td>Fc1ncc(nc1)F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>-3.095739</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-3.379274</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-3.107787</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>45917</td>\n",
       "      <td>hypotheticalMOF_18966_i_1_j_7_k_0_m_2.cif</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1)Cl)C(=O)[O]</td>\n",
       "      <td>[O]C(=O)c1ccc(cc1Cl)/N=N/c1c(Cl)cc(c(c1Cl)Cl)C...</td>\n",
       "      <td>Clc1cnccn1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.559909</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>42638</td>\n",
       "      <td>hypotheticalMOF_5064665_i_2_j_21_k_10_m_3.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)C#C/C=C/C#CC(=O)[O]</td>\n",
       "      <td>[O]C(=O)/[C]=C/C(=C/C(=O)[O])/Br.[Br]</td>\n",
       "      <td>Br/N=C(\\C(=N)Br)/Br</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>-0.128717</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>-0.232652</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>43592</td>\n",
       "      <td>hypotheticalMOF_5081795_i_2_j_29_k_10_m_2.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[O]C(=O)/C=C/C(=C/C(=O)[O])/Cl.ClO[C]1N(c2[c]c...</td>\n",
       "      <td>N=CC(=[N])Cl.[Cl]</td>\n",
       "      <td>[O][C]C=[CH]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>0.564215</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68268</th>\n",
       "      <td>68314</td>\n",
       "      <td>hypotheticalMOF_37209_i_2_j_18_k_2_m_4.cif</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Cc1c(c2ccncc2)c(C)c(c(c1C)c1c(C)cnc(c1C)C)C</td>\n",
       "      <td>[O]C(=O)c1cc(C)c2c(c1)c1cc(C)c3c(c1c(c2)C)cc(c...</td>\n",
       "      <td>[O]C(=O)c1ccc(c(c1C)C)c1[c]cc([c]c1)c1cc(C)c(c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761663</td>\n",
       "      <td>1.535577</td>\n",
       "      <td>179.769656</td>\n",
       "      <td>39.064489</td>\n",
       "      <td>1.563105</td>\n",
       "      <td>204.360203</td>\n",
       "      <td>42.586407</td>\n",
       "      <td>1.274043</td>\n",
       "      <td>247.593271</td>\n",
       "      <td>56.068422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76382 rows Ã— 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        filename  Metal_ID  \\\n",
       "69176       69223        hypotheticalMOF_3593_i_0_j_7_k_4_m_2.cif         0   \n",
       "21102       21110    hypotheticalMOF_5043233_i_1_j_23_k_9_m_1.cif         1   \n",
       "46293       46320     hypotheticalMOF_27246_i_1_j_18_k_18_m_2.cif         1   \n",
       "18090       18096    hypotheticalMOF_5035712_i_1_j_20_k_0_m_5.cif         1   \n",
       "13754       13758    hypotheticalMOF_5031267_i_0_j_29_k_9_m_1.cif         0   \n",
       "39495       39517   hypotheticalMOF_5055665_i_1_j_28_k_21_m_4.cif         1   \n",
       "20161       20167     hypotheticalMOF_35880_i_2_j_16_k_10_m_2.cif         2   \n",
       "18876       18882   hypotheticalMOF_5008993_i_0_j_21_k_21_m_2.cif         0   \n",
       "12776       12780      hypotheticalMOF_12646_i_0_j_16_k_1_m_1.cif         0   \n",
       "65757       65799       hypotheticalMOF_8394_i_0_j_12_k_6_m_5.cif         0   \n",
       "14868       14872   hypotheticalMOF_2000012_i_1_j_19_k_19_m_3.cif         1   \n",
       "7449         7450     hypotheticalMOF_1000614_i_3_j_7_k_2_m_1.cif         3   \n",
       "2533         2534   hypotheticalMOF_5010169_i_0_j_22_k_11_m_7.cif         0   \n",
       "36442       36459    hypotheticalMOF_5019737_i_0_j_26_k_0_m_9.cif         0   \n",
       "62958       63000   hypotheticalMOF_5042527_i_1_j_23_k_0_m_10.cif         1   \n",
       "46722       46749     hypotheticalMOF_1000454_i_3_j_6_k_2_m_8.cif         3   \n",
       "2770         2771    hypotheticalMOF_5013931_i_0_j_24_k_3_m_5.cif         0   \n",
       "63998       64040    hypotheticalMOF_5074221_i_2_j_26_k_6_m_9.cif         2   \n",
       "65587       65629   hypotheticalMOF_5049712_i_1_j_26_k_12_m_1.cif         1   \n",
       "59592       59630   hypotheticalMOF_5043805_i_1_j_23_k_18_m_4.cif         1   \n",
       "47742       47769   hypotheticalMOF_5017542_i_0_j_25_k_9_m_13.cif         0   \n",
       "60628       60669  hypotheticalMOF_5007885_i_0_j_21_k_13_m_10.cif         0   \n",
       "75522       75574       hypotheticalMOF_8101_i_0_j_12_k_3_m_6.cif         0   \n",
       "49203       49230  hypotheticalMOF_5053686_i_1_j_27_k_26_m_13.cif         1   \n",
       "36370       36387       hypotheticalMOF_18994_i_1_j_7_k_0_m_5.cif         1   \n",
       "64251       64293    hypotheticalMOF_7002430_i_4_j_28_k_3_m_9.cif         4   \n",
       "76287       76339  hypotheticalMOF_7002680_i_4_j_28_k_19_m_11.cif         4   \n",
       "56573       56609       hypotheticalMOF_19577_i_1_j_7_k_6_m_8.cif         1   \n",
       "31235       31248   hypotheticalMOF_1001308_i_3_j_10_k_9_m_12.cif         3   \n",
       "42073       42097     hypotheticalMOF_31820_i_2_j_10_k_10_m_3.cif         2   \n",
       "...           ...                                             ...       ...   \n",
       "41504       41527   hypotheticalMOF_5049272_i_1_j_26_k_5_m_11.cif         1   \n",
       "52489       52520       hypotheticalMOF_30428_i_2_j_8_k_5_m_8.cif         2   \n",
       "68755       68801    hypotheticalMOF_5020442_i_0_j_26_k_6_m_5.cif         0   \n",
       "49811       49838  hypotheticalMOF_5045653_i_1_j_24_k_19_m_13.cif         1   \n",
       "797           797  hypotheticalMOF_6003141_i_3_j_24_k_21_m_11.cif         3   \n",
       "31921       31934   hypotheticalMOF_3000716_i_1_j_26_k_23_m_8.cif         1   \n",
       "24675       24685   hypotheticalMOF_6001587_i_3_j_21_k_5_m_12.cif         3   \n",
       "47954       47981      hypotheticalMOF_15854_i_0_j_18_k_6_m_1.cif         0   \n",
       "2496         2497    hypotheticalMOF_5066677_i_2_j_22_k_7_m_3.cif         2   \n",
       "73135       73185      hypotheticalMOF_31137_i_2_j_10_k_0_m_1.cif         2   \n",
       "1871         1872    hypotheticalMOF_5069694_i_2_j_24_k_5_m_4.cif         2   \n",
       "7877         7878   hypotheticalMOF_6004262_i_3_j_27_k_2_m_14.cif         3   \n",
       "37619       37638  hypotheticalMOF_5008222_i_0_j_21_k_16_m_12.cif         0   \n",
       "70608       70656   hypotheticalMOF_5062990_i_2_j_20_k_16_m_3.cif         2   \n",
       "67699       67745    hypotheticalMOF_13406_i_0_j_16_k_10_m_13.cif         0   \n",
       "38804       38825    hypotheticalMOF_5046420_i_1_j_25_k_2_m_9.cif         1   \n",
       "6921         6922    hypotheticalMOF_1003717_i_4_j_10_k_0_m_5.cif         4   \n",
       "18983       18989   hypotheticalMOF_5044313_i_1_j_23_k_23_m_7.cif         1   \n",
       "32230       32243        hypotheticalMOF_116_i_0_j_0_k_0_m_13.cif         0   \n",
       "17089       17095   hypotheticalMOF_6001973_i_3_j_22_k_0_m_11.cif         3   \n",
       "52620       52651   hypotheticalMOF_5072196_i_2_j_25_k_11_m_8.cif         2   \n",
       "39512       39534     hypotheticalMOF_23807_i_1_j_14_k_1_m_10.cif         1   \n",
       "48600       48627   hypotheticalMOF_5082196_i_2_j_29_k_16_m_9.cif         2   \n",
       "55026       55061   hypotheticalMOF_6001255_i_3_j_20_k_14_m_5.cif         3   \n",
       "41993       42017   hypotheticalMOF_5075328_i_2_j_26_k_22_m_1.cif         2   \n",
       "21243       21251      hypotheticalMOF_32735_i_2_j_12_k_0_m_1.cif         2   \n",
       "45891       45917       hypotheticalMOF_18966_i_1_j_7_k_0_m_2.cif         1   \n",
       "42613       42638   hypotheticalMOF_5064665_i_2_j_21_k_10_m_3.cif         2   \n",
       "43567       43592   hypotheticalMOF_5081795_i_2_j_29_k_10_m_2.cif         2   \n",
       "68268       68314      hypotheticalMOF_37209_i_2_j_18_k_2_m_4.cif         2   \n",
       "\n",
       "       #_of_Linkers                                          L0_Smiles  \\\n",
       "69176             3        [O]C(=O)c1ccc(cc1)/N=N/c1ccc(cc1Cl)C(=O)[O]   \n",
       "21102             2  [O]C(=O)C#C[C@@]12CC[C@@](C([C@H]1F)(F)F)([C@H...   \n",
       "46293             3  [O]C(=O)c1cc(Cl)c2c(c1)c1cc(Cl)c3c(c1cc2)cc(c(...   \n",
       "18090             1                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "13754             3                              [O]C(=O)/C=C/C(=O)[O]   \n",
       "39495             3                    C/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "20161             4                                [N]=CC(=[N])Cl.ClCl   \n",
       "18876             3                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "12776             3    [O]C(=O)c1ccc2c(c1)c1ccc(c(c1c(c2F)F)C(=O)[O])F   \n",
       "65757             3  CCc1cc(C#Cc2cc(CC)c(c(c2CC)CC)C(=O)[O])c(c(c1C...   \n",
       "14868             8           [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449              2  Fc1cc(cc(c1c1ccc(c(c1)F)C(=O)[O])F)c1cc(F)c(cc...   \n",
       "2533              3                      [O]C(=O)/C=C/C=C(/C(=O)[O])\\N   \n",
       "36442             3                   [O]C(=O)c1cc(O)c(c(c1)O)C(=O)[O]   \n",
       "62958             3                                   N#Cc1ncc(nc1)C#N   \n",
       "46722             2            [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]   \n",
       "2770              3     CC[C@@H]1[C@@H](CC)c2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "63998             3  Oc1cc(C(=O)[O])c(cc1/C=C/c1c(O)cc(cc1O)C(=O)[O])O   \n",
       "65587             3  F/C(=C\\c1cc(F)c(c(c1)F)C(=O)[O])/c1ccc(cc1)C(=...   \n",
       "59592             3        Cc1cn[c]c2c1ccc1c2ccc2c1[c]ncc2.[CH3].[CH3]   \n",
       "47742             3            CC[CH]O/C(=C(\\C([O])[O])/OCCC)/C(=O)[O]   \n",
       "60628             3      N#Cc1cc2cc(C(=O)[O])c(c(c2cc1C(=O)[O])C#N)C#N   \n",
       "75522             3                                   [O]C(=O)C(=O)[O]   \n",
       "49203             3  CCCOc1cc(cc(c1c1nnc(nn1)c1c(OCCC)cc(cc1OCCC)C(...   \n",
       "36370             1  c1[c]ccnc1.CCc1[c]c(C(=O)[O])c(cc1/N=N/c1c(CC)...   \n",
       "64251             8                             Oc1ccc(c(c1)O)C(=O)[O]   \n",
       "76287             8                                   [O]C(=O)c1ccccc1   \n",
       "56573             3  [O]C(=O)c1cc(c2ccccc2)c(c(c1c1ccccc1)c1ccccc1)...   \n",
       "31235             1  [O]C(/C=C/C(=O)[O])[O].[CH2]CO/C(=C(\\C(=O)[O])...   \n",
       "42073             3                                     Br/N=C\\C(=N)Br   \n",
       "...             ...                                                ...   \n",
       "41504             6                               N#Cc1ccc(c(c1)OC)C#N   \n",
       "52489             2  [O]C(=O)C#Cc1ccc(cc1c1ccccc1)[C@@]12[C@@H]3[CH...   \n",
       "68755             3          CCc1cc(C#Cc2ccc(cc2)C(=O)[O])ccc1C(=O)[O]   \n",
       "49811             2                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "797               2                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "31921            10                                   [O]C(=O)c1ccccc1   \n",
       "24675             2           CCOc1cc(C#CC(=O)[O])c(cc1C#CC(=O)[O])OCC   \n",
       "47954             3  Fc1cc(C(=O)[O])c(cc1C#Cc1c(F)cc(c(c1F)F)C(=O)[...   \n",
       "2496              1                Brc1ncc(c(c1)/N=N/c1c(Br)cncc1Br)Br   \n",
       "73135             3                                          Fc1cnccn1   \n",
       "1871              3                   [O]C(=O)C#Cc1ccc(cc1)C#CC(=O)[O]   \n",
       "7877              2  N=Nc1cc(ccc1c1cc(N=N)c(cc1N=N)c1cc(N=N)c(cc1N=...   \n",
       "37619             3  CCOc1c(O[CH2])c2c(ccc(c2c2c1cc(O[C]C)c(c2)[C][...   \n",
       "70608             2                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "67699             3            CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]   \n",
       "38804             3  Oc1[c]c(ccc1c1ccc(c(c1)O)C(=O)[O])c1c(O)c(O)c(...   \n",
       "6921              6                       CCc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "18983             3  [O]C(=O)C#C[C@]12CC[C@]([C@@H](C1)N)([C@@H]([C...   \n",
       "32230             3                         [O]C(=O)c1ccc(cc1)C(=O)[O]   \n",
       "17089             2                       COc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "52620             3                                   N=CC(=N)c1ccccc1   \n",
       "39512             1    N#Cc1cc(C(=O)[O])c(cc1c1ccc(cc1C#N)C(=O)[O])C#N   \n",
       "48600             5               OC1=N[CH]c2c([C]1O)ccc1c2c(O)cc(n1)O   \n",
       "55026             2                 CCc1cccc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "41993             3  F/C(=C\\c1cc(F)c(c(c1F)F)C(=O)[O])/c1cc(F)c(cc1...   \n",
       "21243             3                     [O]C(=O)c1cc(F)c(cc1F)C(=O)[O]   \n",
       "45891             3                     [O]C(=O)c1ccc(c(c1)Cl)C(=O)[O]   \n",
       "42613             3                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "43567             3  [O]C(=O)/C=C/C(=C/C(=O)[O])/Cl.ClO[C]1N(c2[c]c...   \n",
       "68268             3        Cc1c(c2ccncc2)c(C)c(c(c1C)c1c(C)cnc(c1C)C)C   \n",
       "\n",
       "                                               L1_Smiles  \\\n",
       "69176    [O]C(=O)c1ccc(cc1Cl)/N=N/c1ccc(c(c1)Cl)C(=O)[O]   \n",
       "21102                          [O]C(=O)/C=C(/C(=O)[O])\\F   \n",
       "46293  [O]C(=O)c1cc(Cl)c2c([c]1)c1[c]cc3c(c1cc2Cl)[c]...   \n",
       "18090                                                NaN   \n",
       "13754                      F/C(=C(\\C(=O)[O])/F)/C(=O)[O]   \n",
       "39495                                    N#C/C=C(/C#N)\\C   \n",
       "20161                   [O]C(=O)/[C]=C/C=C/C(=O)[O].[Cl]   \n",
       "18876              Cl/C(=C(\\C#CC(=O)[O])/Cl)/C#CC(=O)[O]   \n",
       "12776           Fc1cc(ccc1c1ccc(c(c1)F)C(=O)[O])C(=O)[O]   \n",
       "65757                                   [O]C(=O)C(=O)[O]   \n",
       "14868         [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br   \n",
       "7449      [O]C(=O)c1ccc(cc1F)/N=N/c1ccc(c(c1F)F)C(=O)[O]   \n",
       "2533   [O]C(=O)[C@]12[C@@H](N)[C@@H](N)[C@](C([C@H]1N...   \n",
       "36442  O/C(=C\\c1[c]cc(c(c1O)O)C(=O)[O])/c1ccc(cc1)C(=...   \n",
       "62958  N#C[C@@H]1[C@@H](C#N)[C@@]2(C#CC(=O)[O])[C@@H]...   \n",
       "46722            [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1)C(=O)[O]   \n",
       "2770                CCC1(CC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "63998                     OC1=N[CH]C=C([CH]1)C#Cc1ccncc1   \n",
       "65587  F/C(=C\\c1ccc(cc1)C(=O)[O])/c1cc(F)c(c(c1)F)C(=...   \n",
       "59592  [O]C(=O)c1ccc2c(c1)c1c(C)cc3c(c1cc2)[c]c(c(c3C...   \n",
       "47742                      CC[CH]O/C(=C\\C(=O)[O])/C(=O)O   \n",
       "60628      N#Cc1cc2c(C#N)c(cc(c2cc1C(=O)[O])C#N)C(=O)[O]   \n",
       "75522  CCCc1cc(C(=O)[O])c2c(c1C(=O)[O])C([C@H]2CCC)(C...   \n",
       "49203  CCCOC1=N[CH]C(=C([CH]1)/[C]=C/c1c(OCCC)cnc(c1[...   \n",
       "36370                                                NaN   \n",
       "64251                 [O]C(=O)c1ccc(c2c1CC2(O)O)C(=O)[O]   \n",
       "76287                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "56573    [O]C(=O)c1ccc(cc1)C#Cc1ccc(cc1c1ccccc1)C(=O)[O]   \n",
       "31235                                                NaN   \n",
       "42073                          [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "...                                                  ...   \n",
       "41504  COc1cc(C(=O)[O])c(cc1/C=C/c1c([O])c(OC)c(c(c1O...   \n",
       "52489                         N#Cc1ccc(c(c1)c1ccccc1)C#N   \n",
       "68755  CCc1c(CC)c(/C=C/c2[c]c(CC)c(c(c2CC)CC)C(=O)[O]...   \n",
       "49811  CCCO[C@]12[C@H]3[C@H]4[C@@]2([C@H]2[C@@H]1[C@@...   \n",
       "797    CO[C@]12[C@@H]3[C@@]4([C@H]1[C@@]1([C@]2([C@]3...   \n",
       "31921                                   [O]C(=O)c1ccccc1   \n",
       "24675                  CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "47954  [O]C(=O)c1cc2c3c(F)cc4c(c3cc(c2cc1F)F)c(F)c(c(...   \n",
       "2496                                                 NaN   \n",
       "73135                          [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "1871                             N#Cc1cc(C)c(c(c1C)C)C#N   \n",
       "7877   N=Nc1cc(C(=O)[O])c(c(c1c1nnc(nn1)c1ccc(c2c1n[n...   \n",
       "37619                  CCO/C(=C\\C#CC(=O)[O])/C#CC(=O)[O]   \n",
       "70608  Brc1c(Br)ncc2c1ccc1c2c(Br)ccn1.[O]C(=O)c1cc2c(...   \n",
       "67699            CCCO/C(=C(\\C=C\\C(=O)[O])/OCCC)/C(=O)[O]   \n",
       "38804  [O]C(=O)C#C[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@...   \n",
       "6921                        CCc1cc(ccc1C(=O)[O])C(=O)[O]   \n",
       "18983  [O]C(=O)C#C[C@]12C[C@@H](N)[C@]([C@H]([C@@H]1N...   \n",
       "32230               CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]   \n",
       "17089  CO[C@@H]1C[C@@]([CH2])(C(=O)[O])C([C@@H]([C]1C...   \n",
       "52620               [O]C(=O)/C(=C/C=C/C(=O)[O])/c1ccccc1   \n",
       "39512                                                NaN   \n",
       "48600   [O]C(=O)c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)C(=O)[O])O   \n",
       "55026                          [O]C(=O)C#CC#CC#CC(=O)[O]   \n",
       "41993             F[C@@H]1CN2[C@@H](CN1[C@H]([C@H]2F)F)F   \n",
       "21243                                   [O]C(=O)C(=O)[O]   \n",
       "45891  [O]C(=O)c1ccc(cc1Cl)/N=N/c1c(Cl)cc(c(c1Cl)Cl)C...   \n",
       "42613              [O]C(=O)/[C]=C/C(=C/C(=O)[O])/Br.[Br]   \n",
       "43567                                  N=CC(=[N])Cl.[Cl]   \n",
       "68268  [O]C(=O)c1cc(C)c2c(c1)c1cc(C)c3c(c1c(c2)C)cc(c...   \n",
       "\n",
       "                                               L2_Smiles  \\\n",
       "69176  Clc1cc(C(=O)[O])c(cc1/N=N/c1c(Cl)cc(c(c1Cl)Cl)...   \n",
       "21102                                                NaN   \n",
       "46293  Clc1cc2c(c3c1c(Cl)c(Cl)n[c]3)cc(c1c2cncc1Cl)Cl...   \n",
       "18090                                                NaN   \n",
       "13754  [O]C(=O)c1c(F)cc(c(c1F)F)n1c(=O)c2c(F)cc3c4c2c...   \n",
       "39495  CO[C]1N(c2[c]cc(cc2C)C(=O)[O])C(=O)c2c3c1c(C)c...   \n",
       "20161  [O]C(=O)c1ccc2c([c]1)c1[c]cc(c(c1cc2Cl)C(=O)[O...   \n",
       "18876                        [O]C(=O)C#C/C=C/C#CC(=O)[O]   \n",
       "12776  FOC(=O)c1cc[c]c2c1[c]c(F)c1c2[c]c(c(c1)F)C(=O)...   \n",
       "65757  CCc1cc(ccc1C#Cc1c(CC)cc(c(c1CC)CC)C(=O)[O])C(=...   \n",
       "14868         [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br   \n",
       "7449                                                 NaN   \n",
       "2533                           [O]C(=O)/C=C/C=C/C(=O)[O]   \n",
       "36442               [O]C(=O)c1c(O)c(O)c(c(c1O)O)C(=O)[O]   \n",
       "62958                 N#Cc1cc(C(=O)[O])c(cc1C(=O)[O])C#N   \n",
       "46722                                                NaN   \n",
       "2770   CC[C@@]12[C@@H]3[C@@H]4[C@]2([C@@]2([C@H]1[C@]...   \n",
       "63998  Oc1cc(C(=O)[O])c(c(c1C#Cc1c(O)cc(c(c1O)O)C(=O)...   \n",
       "65587          Fc1[c]c(/C=[C]/c2cc(F)nc(c2F)F)cc(n1)F.FF   \n",
       "59592  [O]C(=O)C#C[C@@]12[C@@H](C)[C@@H](C)[C@@](C([C...   \n",
       "47742  CCCO[C@]12[C@@]3(C#CC(=O)[O])[C@H]4[C@@]2([C@]...   \n",
       "60628            N#C/C(=C(\\C#CC(=O)[O])/C#N)/C#CC(=O)[O]   \n",
       "75522             CCCC1(CCC)Cc2c1c(ccc2C(=O)[O])C(=O)[O]   \n",
       "49203  CCOCOc1cc(ccc1/C(=[C]/c1[c]cc(cc1)C(=O)[O])/O[...   \n",
       "36370                                                NaN   \n",
       "64251                             Oc1cc(O)cc(c1)C(=O)[O]   \n",
       "76287                             [O]C(=O)C#CC#CC(=O)[O]   \n",
       "56573            c1ccc(cc1)c1nccc(c1)C#Cc1ccncc1c1ccccc1   \n",
       "31235                                                NaN   \n",
       "42073              [O]C(=O)/C=C/C(=[C]/C(=O)[O])/Br.[Br]   \n",
       "...                                                  ...   \n",
       "41504                 COc1cc(C#CC(=O)[O])ccc1C#CC(=O)[O]   \n",
       "52489                                                NaN   \n",
       "68755  CCc1c(C#Cc2cc(CC)c(c(c2)CC)C(=O)[O])c(CC)c(c(c...   \n",
       "49811                                                NaN   \n",
       "797                                                  NaN   \n",
       "31921                                  N#CC(C#N)(C#N)C#N   \n",
       "24675                                                NaN   \n",
       "47954     [O]C(=O)c1ccc(c(c1)F)C#Cc1cc(F)c(cc1F)C(=O)[O]   \n",
       "2496                                                 NaN   \n",
       "73135                      [O]C(=O)/C=C/C=C(/C(=O)[O])\\F   \n",
       "1871   [O]C(=O)[C@]12[C@H]3[C@H]4[C@@]2([C@]2([C@@]1(...   \n",
       "7877                                                 NaN   \n",
       "37619                        CCO/C(=C\\C#[C])/C#CC(=O)[O]   \n",
       "70608                                                NaN   \n",
       "67699  CCCOc1cc(cc2c1ccc1c2c(OCCC)cc(c1C(=O)[O])OCCC)...   \n",
       "38804            Oc1ncc(c(c1)c1ccc(c(c1)O)c1cc(O)ncc1O)O   \n",
       "6921                    CCc1c(ccc(c1CC)C(=O)[O])C(=O)[O]   \n",
       "18983  N#C[C@@]12C[C@@H](N)[C@@](C(C1)(N)N)([C@H]([C@...   \n",
       "32230               CCCOc1cc(cc(c1C(=O)[O])OCCC)C(=O)[O]   \n",
       "17089                                                NaN   \n",
       "52620  [O]C(=O)C#C[C@]12[C@@H]3[C@@H]4[C@H]1[C@@]1([C...   \n",
       "39512                                                NaN   \n",
       "48600  OOc1n(c2[c]cc(c[c]2)C(=O)[O])c(OO)c2c3c1CC=c1c...   \n",
       "55026                                                NaN   \n",
       "41993  [O]C(=O)[C@@]12CC[C@@](C(C1)(F)F)(C(C2(F)F)(F)...   \n",
       "21243                                       Fc1ncc(nc1)F   \n",
       "45891                                         Clc1cnccn1   \n",
       "42613                                Br/N=C(\\C(=N)Br)/Br   \n",
       "43567                                       [O][C]C=[CH]   \n",
       "68268  [O]C(=O)c1ccc(c(c1C)C)c1[c]cc([c]c1)c1cc(C)c(c...   \n",
       "\n",
       "                                          L3_Smiles  \\\n",
       "69176                                           NaN   \n",
       "21102                                           NaN   \n",
       "46293                                           NaN   \n",
       "18090                                           NaN   \n",
       "13754                                           NaN   \n",
       "39495                                           NaN   \n",
       "20161                          [CH]/[C]=C(\\Cl)/[CH]   \n",
       "18876                                           NaN   \n",
       "12776                                           NaN   \n",
       "65757                                           NaN   \n",
       "14868      [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449                                            NaN   \n",
       "2533                                            NaN   \n",
       "36442                                           NaN   \n",
       "62958                                           NaN   \n",
       "46722                                           NaN   \n",
       "2770                                            NaN   \n",
       "63998                                           NaN   \n",
       "65587                                           NaN   \n",
       "59592                                           NaN   \n",
       "47742                                           NaN   \n",
       "60628                                           NaN   \n",
       "75522                                           NaN   \n",
       "49203                                           NaN   \n",
       "36370                                           NaN   \n",
       "64251                     Oc1cc(O)c(c(c1)C(=O)[O])O   \n",
       "76287                          COc1cccc(c1)C(=O)[O]   \n",
       "56573                                           NaN   \n",
       "31235                                           NaN   \n",
       "42073                                           NaN   \n",
       "...                                             ...   \n",
       "41504                                          C[O]   \n",
       "52489                                           NaN   \n",
       "68755                                           NaN   \n",
       "49811                                           NaN   \n",
       "797                                             NaN   \n",
       "31921                              [O]C(=O)c1ccccc1   \n",
       "24675                                           NaN   \n",
       "47954                                           NaN   \n",
       "2496                                            NaN   \n",
       "73135                                           NaN   \n",
       "1871                                            NaN   \n",
       "7877                                            NaN   \n",
       "37619                                           NaN   \n",
       "70608                                           NaN   \n",
       "67699                                           NaN   \n",
       "38804                                           NaN   \n",
       "6921           CCc1cc(C(=O)[O])c(c(c1C(=O)[O])CC)CC   \n",
       "18983                                           NaN   \n",
       "32230                                           NaN   \n",
       "17089                                           NaN   \n",
       "52620                                           NaN   \n",
       "39512                                           NaN   \n",
       "48600  [O][C]c1cc2c3ccc(c(c3c(cc2c(c1O)O)O)[C][O])O   \n",
       "55026                                           NaN   \n",
       "41993                                           NaN   \n",
       "21243                                           NaN   \n",
       "45891                                           NaN   \n",
       "42613                                           NaN   \n",
       "43567                                           NaN   \n",
       "68268                                           NaN   \n",
       "\n",
       "                                      L4_Smiles  \\\n",
       "69176                                       NaN   \n",
       "21102                                       NaN   \n",
       "46293                                       NaN   \n",
       "18090                                       NaN   \n",
       "13754                                       NaN   \n",
       "39495                                       NaN   \n",
       "20161                                       NaN   \n",
       "18876                                       NaN   \n",
       "12776                                       NaN   \n",
       "65757                                       NaN   \n",
       "14868  [O]C(=O)c1cc(C(=O)[O])c(c(c1)C(=O)[O])Br   \n",
       "7449                                        NaN   \n",
       "2533                                        NaN   \n",
       "36442                                       NaN   \n",
       "62958                                       NaN   \n",
       "46722                                       NaN   \n",
       "2770                                        NaN   \n",
       "63998                                       NaN   \n",
       "65587                                       NaN   \n",
       "59592                                       NaN   \n",
       "47742                                       NaN   \n",
       "60628                                       NaN   \n",
       "75522                                       NaN   \n",
       "49203                                       NaN   \n",
       "36370                                       NaN   \n",
       "64251                   [O]C(=O)c1ccc(c(c1O)O)O   \n",
       "76287                      COc1cccc(c1)C(=O)[O]   \n",
       "56573                                       NaN   \n",
       "31235                                       NaN   \n",
       "42073                                       NaN   \n",
       "...                                         ...   \n",
       "41504                                      C[O]   \n",
       "52489                                       NaN   \n",
       "68755                                       NaN   \n",
       "49811                                       NaN   \n",
       "797                                         NaN   \n",
       "31921                          [O]C(=O)c1ccccc1   \n",
       "24675                                       NaN   \n",
       "47954                                       NaN   \n",
       "2496                                        NaN   \n",
       "73135                                       NaN   \n",
       "1871                                        NaN   \n",
       "7877                                        NaN   \n",
       "37619                                       NaN   \n",
       "70608                                       NaN   \n",
       "67699                                       NaN   \n",
       "38804                                       NaN   \n",
       "6921        CC/C(=C\\C(=O)[O])/C=C(/C(=O)[O])\\CC   \n",
       "18983                                       NaN   \n",
       "32230                                       NaN   \n",
       "17089                                       NaN   \n",
       "52620                                       NaN   \n",
       "39512                                       NaN   \n",
       "48600    [O][C]c1c(O)ccc2c1c(O)cc1c2c[c]c(c1O)O   \n",
       "55026                                       NaN   \n",
       "41993                                       NaN   \n",
       "21243                                       NaN   \n",
       "45891                                       NaN   \n",
       "42613                                       NaN   \n",
       "43567                                       NaN   \n",
       "68268                                       NaN   \n",
       "\n",
       "                                        L5_Smiles  ... std_CH4_v/v_35_bar  \\\n",
       "69176                                         NaN  ...          38.761663   \n",
       "21102                                         NaN  ...          38.761663   \n",
       "46293                                         NaN  ...          38.761663   \n",
       "18090                                         NaN  ...          38.761663   \n",
       "13754                                         NaN  ...          38.761663   \n",
       "39495                                         NaN  ...          38.761663   \n",
       "20161                                         NaN  ...          38.761663   \n",
       "18876                                         NaN  ...          38.761663   \n",
       "12776                                         NaN  ...          38.761663   \n",
       "65757                                         NaN  ...          38.761663   \n",
       "14868  [O]C(=O)c1cc(C(=O)[O])c(c(c1Br)C(=O)[O])Br  ...          38.761663   \n",
       "7449                                          NaN  ...          38.761663   \n",
       "2533                                          NaN  ...          38.761663   \n",
       "36442                                         NaN  ...          38.761663   \n",
       "62958                                         NaN  ...          38.761663   \n",
       "46722                                         NaN  ...          38.761663   \n",
       "2770                                          NaN  ...          38.761663   \n",
       "63998                                         NaN  ...          38.761663   \n",
       "65587                                         NaN  ...          38.761663   \n",
       "59592                                         NaN  ...          38.761663   \n",
       "47742                                         NaN  ...          38.761663   \n",
       "60628                                         NaN  ...          38.761663   \n",
       "75522                                         NaN  ...          38.761663   \n",
       "49203                                         NaN  ...          38.761663   \n",
       "36370                                         NaN  ...          38.761663   \n",
       "64251                      [O]C(=O)c1ccc(c(c1)O)O  ...          38.761663   \n",
       "76287                  COc1cc(OC)cc(c1C(=O)[O])OC  ...          38.761663   \n",
       "56573                                         NaN  ...          38.761663   \n",
       "31235                                         NaN  ...          38.761663   \n",
       "42073                                         NaN  ...          38.761663   \n",
       "...                                           ...  ...                ...   \n",
       "41504                                        C[O]  ...          38.761663   \n",
       "52489                                         NaN  ...          38.761663   \n",
       "68755                                         NaN  ...          38.761663   \n",
       "49811                                         NaN  ...          38.761663   \n",
       "797                                           NaN  ...          38.761663   \n",
       "31921                            [O]C(=O)c1ccccc1  ...          38.761663   \n",
       "24675                                         NaN  ...          38.761663   \n",
       "47954                                         NaN  ...          38.761663   \n",
       "2496                                          NaN  ...          38.761663   \n",
       "73135                                         NaN  ...          38.761663   \n",
       "1871                                          NaN  ...          38.761663   \n",
       "7877                                          NaN  ...          38.761663   \n",
       "37619                                         NaN  ...          38.761663   \n",
       "70608                                         NaN  ...          38.761663   \n",
       "67699                                         NaN  ...          38.761663   \n",
       "38804                                         NaN  ...          38.761663   \n",
       "6921                 CCc1cc(ccc1C(=O)[O])C(=O)[O]  ...          38.761663   \n",
       "18983                                         NaN  ...          38.761663   \n",
       "32230                                         NaN  ...          38.761663   \n",
       "17089                                         NaN  ...          38.761663   \n",
       "52620                                         NaN  ...          38.761663   \n",
       "39512                                         NaN  ...          38.761663   \n",
       "48600                                         NaN  ...          38.761663   \n",
       "55026                                         NaN  ...          38.761663   \n",
       "41993                                         NaN  ...          38.761663   \n",
       "21243                                         NaN  ...          38.761663   \n",
       "45891                                         NaN  ...          38.761663   \n",
       "42613                                         NaN  ...          38.761663   \n",
       "43567                                         NaN  ...          38.761663   \n",
       "68268                                         NaN  ...          38.761663   \n",
       "\n",
       "      norm_CH4_v/v_65_bar mean_CH4_v/v_65_bar std_CH4_v/v_65_bar  \\\n",
       "69176                 NaN          179.769656          39.064489   \n",
       "21102           -0.955686          179.769656          39.064489   \n",
       "46293                 NaN          179.769656          39.064489   \n",
       "18090                 NaN          179.769656          39.064489   \n",
       "13754            1.005855          179.769656          39.064489   \n",
       "39495                 NaN          179.769656          39.064489   \n",
       "20161                 NaN          179.769656          39.064489   \n",
       "18876           -1.230404          179.769656          39.064489   \n",
       "12776                 NaN          179.769656          39.064489   \n",
       "65757                 NaN          179.769656          39.064489   \n",
       "14868                 NaN          179.769656          39.064489   \n",
       "7449                  NaN          179.769656          39.064489   \n",
       "2533             0.512362          179.769656          39.064489   \n",
       "36442                 NaN          179.769656          39.064489   \n",
       "62958           -0.571949          179.769656          39.064489   \n",
       "46722                 NaN          179.769656          39.064489   \n",
       "2770             0.464247          179.769656          39.064489   \n",
       "63998           -0.221266          179.769656          39.064489   \n",
       "65587                 NaN          179.769656          39.064489   \n",
       "59592                 NaN          179.769656          39.064489   \n",
       "47742                 NaN          179.769656          39.064489   \n",
       "60628           -0.072655          179.769656          39.064489   \n",
       "75522                 NaN          179.769656          39.064489   \n",
       "49203                 NaN          179.769656          39.064489   \n",
       "36370            1.286482          179.769656          39.064489   \n",
       "64251           -0.488360          179.769656          39.064489   \n",
       "76287                 NaN          179.769656          39.064489   \n",
       "56573                 NaN          179.769656          39.064489   \n",
       "31235           -0.653463          179.769656          39.064489   \n",
       "42073                 NaN          179.769656          39.064489   \n",
       "...                   ...                 ...                ...   \n",
       "41504            0.374758          179.769656          39.064489   \n",
       "52489                 NaN          179.769656          39.064489   \n",
       "68755            0.210698          179.769656          39.064489   \n",
       "49811                 NaN          179.769656          39.064489   \n",
       "797                   NaN          179.769656          39.064489   \n",
       "31921                 NaN          179.769656          39.064489   \n",
       "24675                 NaN          179.769656          39.064489   \n",
       "47954                 NaN          179.769656          39.064489   \n",
       "2496                  NaN          179.769656          39.064489   \n",
       "73135                 NaN          179.769656          39.064489   \n",
       "1871                  NaN          179.769656          39.064489   \n",
       "7877            -0.394362          179.769656          39.064489   \n",
       "37619                 NaN          179.769656          39.064489   \n",
       "70608                 NaN          179.769656          39.064489   \n",
       "67699                 NaN          179.769656          39.064489   \n",
       "38804           -0.440158          179.769656          39.064489   \n",
       "6921                  NaN          179.769656          39.064489   \n",
       "18983                 NaN          179.769656          39.064489   \n",
       "32230            0.936851          179.769656          39.064489   \n",
       "17089           -2.080005          179.769656          39.064489   \n",
       "52620                 NaN          179.769656          39.064489   \n",
       "39512                 NaN          179.769656          39.064489   \n",
       "48600            1.562211          179.769656          39.064489   \n",
       "55026                 NaN          179.769656          39.064489   \n",
       "41993                 NaN          179.769656          39.064489   \n",
       "21243           -3.095739          179.769656          39.064489   \n",
       "45891                 NaN          179.769656          39.064489   \n",
       "42613            0.021731          179.769656          39.064489   \n",
       "43567                 NaN          179.769656          39.064489   \n",
       "68268            1.535577          179.769656          39.064489   \n",
       "\n",
       "      norm_CH4_v/v_100_bar mean_CH4_v/v_100_bar std_CH4_v/v_100_bar  \\\n",
       "69176            -0.354921           204.360203           42.586407   \n",
       "21102            -0.810894           204.360203           42.586407   \n",
       "46293            -0.016301           204.360203           42.586407   \n",
       "18090             0.465522           204.360203           42.586407   \n",
       "13754             0.887350           204.360203           42.586407   \n",
       "39495             1.090932           204.360203           42.586407   \n",
       "20161            -0.038009           204.360203           42.586407   \n",
       "18876            -0.556891           204.360203           42.586407   \n",
       "12776             0.441928           204.360203           42.586407   \n",
       "65757             0.555553           204.360203           42.586407   \n",
       "14868             0.126752           204.360203           42.586407   \n",
       "7449              0.292017           204.360203           42.586407   \n",
       "2533              0.375926           204.360203           42.586407   \n",
       "36442             0.457002           204.360203           42.586407   \n",
       "62958            -0.704145           204.360203           42.586407   \n",
       "46722             0.372548           204.360203           42.586407   \n",
       "2770              0.172103           204.360203           42.586407   \n",
       "63998             0.162555           204.360203           42.586407   \n",
       "65587             0.394484           204.360203           42.586407   \n",
       "59592             1.039952           204.360203           42.586407   \n",
       "47742             0.066911           204.360203           42.586407   \n",
       "60628             0.167162           204.360203           42.586407   \n",
       "75522            -2.133011           204.360203           42.586407   \n",
       "49203             1.481760           204.360203           42.586407   \n",
       "36370             1.445445           204.360203           42.586407   \n",
       "64251            -0.827030           204.360203           42.586407   \n",
       "76287            -0.016299           204.360203           42.586407   \n",
       "56573             0.626222           204.360203           42.586407   \n",
       "31235            -0.707547           204.360203           42.586407   \n",
       "42073            -0.418621           204.360203           42.586407   \n",
       "...                    ...                  ...                 ...   \n",
       "41504             0.643970           204.360203           42.586407   \n",
       "52489             0.247676           204.360203           42.586407   \n",
       "68755             0.552423           204.360203           42.586407   \n",
       "49811             0.965705           204.360203           42.586407   \n",
       "797              -0.481962           204.360203           42.586407   \n",
       "31921            -0.143828           204.360203           42.586407   \n",
       "24675             0.535856           204.360203           42.586407   \n",
       "47954            -0.319036           204.360203           42.586407   \n",
       "2496              0.227695           204.360203           42.586407   \n",
       "73135            -0.202383           204.360203           42.586407   \n",
       "1871              0.816438           204.360203           42.586407   \n",
       "7877             -0.388359           204.360203           42.586407   \n",
       "37619             0.784153           204.360203           42.586407   \n",
       "70608             0.186952           204.360203           42.586407   \n",
       "67699             0.365916           204.360203           42.586407   \n",
       "38804             0.114671           204.360203           42.586407   \n",
       "6921             -1.646670           204.360203           42.586407   \n",
       "18983             0.940483           204.360203           42.586407   \n",
       "32230             0.558683           204.360203           42.586407   \n",
       "17089            -2.395274           204.360203           42.586407   \n",
       "52620            -0.844760           204.360203           42.586407   \n",
       "39512             0.459399           204.360203           42.586407   \n",
       "48600             1.364967           204.360203           42.586407   \n",
       "55026             0.105969           204.360203           42.586407   \n",
       "41993            -0.346433           204.360203           42.586407   \n",
       "21243            -3.379274           204.360203           42.586407   \n",
       "45891             0.559909           204.360203           42.586407   \n",
       "42613            -0.128717           204.360203           42.586407   \n",
       "43567             0.564215           204.360203           42.586407   \n",
       "68268             1.563105           204.360203           42.586407   \n",
       "\n",
       "      norm_CH4_v/v_248_bar mean_CH4_v/v_248_bar std_CH4_v/v_248_bar  \n",
       "69176                  NaN           247.593271           56.068422  \n",
       "21102            -1.173564           247.593271           56.068422  \n",
       "46293                  NaN           247.593271           56.068422  \n",
       "18090             0.270543           247.593271           56.068422  \n",
       "13754             0.592185           247.593271           56.068422  \n",
       "39495             1.049613           247.593271           56.068422  \n",
       "20161                  NaN           247.593271           56.068422  \n",
       "18876             0.536843           247.593271           56.068422  \n",
       "12776                  NaN           247.593271           56.068422  \n",
       "65757                  NaN           247.593271           56.068422  \n",
       "14868                  NaN           247.593271           56.068422  \n",
       "7449                   NaN           247.593271           56.068422  \n",
       "2533              0.365204           247.593271           56.068422  \n",
       "36442                  NaN           247.593271           56.068422  \n",
       "62958            -0.847480           247.593271           56.068422  \n",
       "46722                  NaN           247.593271           56.068422  \n",
       "2770             -0.304832           247.593271           56.068422  \n",
       "63998             0.803649           247.593271           56.068422  \n",
       "65587                  NaN           247.593271           56.068422  \n",
       "59592                  NaN           247.593271           56.068422  \n",
       "47742                  NaN           247.593271           56.068422  \n",
       "60628             0.383556           247.593271           56.068422  \n",
       "75522                  NaN           247.593271           56.068422  \n",
       "49203                  NaN           247.593271           56.068422  \n",
       "36370             1.157183           247.593271           56.068422  \n",
       "64251            -1.037695           247.593271           56.068422  \n",
       "76287                  NaN           247.593271           56.068422  \n",
       "56573                  NaN           247.593271           56.068422  \n",
       "31235            -1.201540           247.593271           56.068422  \n",
       "42073                  NaN           247.593271           56.068422  \n",
       "...                    ...                  ...                 ...  \n",
       "41504             0.839435           247.593271           56.068422  \n",
       "52489                  NaN           247.593271           56.068422  \n",
       "68755             1.024328           247.593271           56.068422  \n",
       "49811                  NaN           247.593271           56.068422  \n",
       "797                    NaN           247.593271           56.068422  \n",
       "31921                  NaN           247.593271           56.068422  \n",
       "24675                  NaN           247.593271           56.068422  \n",
       "47954                  NaN           247.593271           56.068422  \n",
       "2496                   NaN           247.593271           56.068422  \n",
       "73135            -0.490113           247.593271           56.068422  \n",
       "1871                   NaN           247.593271           56.068422  \n",
       "7877             -0.326295           247.593271           56.068422  \n",
       "37619             0.470121           247.593271           56.068422  \n",
       "70608                  NaN           247.593271           56.068422  \n",
       "67699            -0.099079           247.593271           56.068422  \n",
       "38804             0.651970           247.593271           56.068422  \n",
       "6921                   NaN           247.593271           56.068422  \n",
       "18983                  NaN           247.593271           56.068422  \n",
       "32230             0.207872           247.593271           56.068422  \n",
       "17089            -2.519903           247.593271           56.068422  \n",
       "52620                  NaN           247.593271           56.068422  \n",
       "39512                  NaN           247.593271           56.068422  \n",
       "48600             0.891679           247.593271           56.068422  \n",
       "55026                  NaN           247.593271           56.068422  \n",
       "41993                  NaN           247.593271           56.068422  \n",
       "21243            -3.107787           247.593271           56.068422  \n",
       "45891                  NaN           247.593271           56.068422  \n",
       "42613            -0.232652           247.593271           56.068422  \n",
       "43567                  NaN           247.593271           56.068422  \n",
       "68268             1.274043           247.593271           56.068422  \n",
       "\n",
       "[76382 rows x 469 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)\n",
    "fp_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_space(x, y, n, force_int=False):\n",
    "    step = (y - x) / (n - 1)\n",
    "    if force_int:\n",
    "        return [int(x + step * i) for i in range(n)]\n",
    "    return [x + step * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define default params\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":2, \"n_unit\":10, \"activation\":'relu', \"loss\":'mse', \n",
    "            \"opt\":'adam', \"val_pct\":.2} #patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial grid\n",
    "init_grid = {\"patience\":eq_space(20, 1000, 5, True), \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":[.3, .5, 5]}\n",
    "#patience, training fraction, n hidden layers, n hidden units, activation, loss, optimizer, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_used = 'norm_CH4_v/v_1_bar' #column name of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dat = ml_data.sample(frac=total_frac, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fp_dat.sample(frac=training_pct,random_state=2)\n",
    "test_dataset = fp_dat.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_dataset[property_used]\n",
    "test_label = test_dataset[property_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, col in enumerate(ml_data.columns):\n",
    "    if start_str in col:\n",
    "        start_col = ind + 1\n",
    "    elif end_str == col:\n",
    "        end_col = ind\n",
    "\n",
    "\n",
    "features = list(ml_data.columns[start_col:end_col])\n",
    "other_props = ['norm_Dom._Pore_(ang.)',\n",
    " 'norm_Max._Pore_(ang.)',\n",
    " 'norm_Void_Fraction',\n",
    " 'norm_Surf._Area_(m2/g)',\n",
    " 'norm_Vol._Surf._Area',\n",
    " 'norm_Density',\n",
    "  'norm_valence_pa',\n",
    "   'norm_atomic_rad_pa_(angstroms)',\n",
    "     'norm_affinity_pa_(eV)',\n",
    "       'norm_ionization_potential_pa_(eV)',\n",
    "           'norm_electronegativity_pa']\n",
    "\n",
    "features = features + other_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = train_dataset[features]\n",
    "test_fp = test_dataset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_fp.to_numpy().astype(np.float32), \n",
    "                                                     train_label.to_numpy().astype(np.float32))).batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices((test_fp.to_numpy().astype(np.float32), \n",
    "                                                    test_label.to_numpy().astype(np.float32))).batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'norm_CH4_v/v_1_bar.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ad361e43decb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.png'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mproperty_used\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, frameon, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    523\u001b[0m                             self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'norm_CH4_v/v_1_bar.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of test and training sets\n",
    "fig,ax = plt.subplots(figsize = (8,5))\n",
    "n_bins=30\n",
    "n, bins, patches = plt.hist(train_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='#FDA65F', alpha=1,label = 'Training set')\n",
    "n, bins, patches = plt.hist(test_label, n_bins, normed=0, lw=0.5, edgecolor='k', facecolor='green', alpha=1, label = 'Test set')\n",
    "plt.xlabel('y_val',fontsize=labelfontsize)\n",
    "plt.ylabel('Count',fontsize=labelfontsize)\n",
    "#ax.set_xlim(2,12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig('%s.png'%property_used,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.266536e-07</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.168371e-06</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.409526e-05</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_Br1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.574442e-08</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.367559e-06</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.839972e-04</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.384394e-02</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C4</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>3.383940e-05</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.814522e-05</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.576132e-03</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.144057e-06</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_O1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.185981e-05</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_Br1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>3.110570e-04</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.195851e-06</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.126802e-04</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C4</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.392599e-03</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_H1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.344056e-04</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_N2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.574989e-07</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_N3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.225004e-04</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_O1</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.551543e-06</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_O2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.653037e-07</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N2_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.438410e-04</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N2_N2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>5.489657e-06</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N3_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.421798e-08</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N3_N2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>7.421798e-08</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>8.798120e-07</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.641113e-04</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C4</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.465272e-07</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_C1_C2_C2</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.360075e-03</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_C1_C2_C3</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.665950e-05</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs27</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.194563e-01</td>\n",
       "      <td>0.122628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.087071</td>\n",
       "      <td>0.178533</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs28</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.135045e-01</td>\n",
       "      <td>0.084655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.082820</td>\n",
       "      <td>0.162340</td>\n",
       "      <td>0.447222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs29</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>3.440944e-02</td>\n",
       "      <td>0.055053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069987</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs30</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.556043e-02</td>\n",
       "      <td>0.026237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs31</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.165416e-01</td>\n",
       "      <td>0.159267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.213113</td>\n",
       "      <td>0.342382</td>\n",
       "      <td>0.605273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs32</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>3.722311e-02</td>\n",
       "      <td>0.071931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs33</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.585792e-05</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs34</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.776861e-02</td>\n",
       "      <td>0.047207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs35</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>8.878340e-06</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs36</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.820615e-02</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.072740</td>\n",
       "      <td>0.182231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs37</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.599475e-04</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs38</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.631785e-05</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs39</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.924747e-06</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs40</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.037908e-05</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs41</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.315632e-02</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs42</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.505104e-02</td>\n",
       "      <td>0.103505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.086386</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_NumAliphaticRings</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.483904e-02</td>\n",
       "      <td>0.049269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_NumAromaticRings</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>4.137902e-02</td>\n",
       "      <td>0.032677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.063995</td>\n",
       "      <td>0.180958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_tpsa</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.039341e+00</td>\n",
       "      <td>0.504261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>0.973388</td>\n",
       "      <td>1.378817</td>\n",
       "      <td>3.080833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Dom._Pore_(ang.)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-2.006148e-03</td>\n",
       "      <td>0.998808</td>\n",
       "      <td>-2.011702</td>\n",
       "      <td>-0.762865</td>\n",
       "      <td>-0.195212</td>\n",
       "      <td>0.599503</td>\n",
       "      <td>3.437769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Max._Pore_(ang.)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-2.239808e-03</td>\n",
       "      <td>0.998836</td>\n",
       "      <td>-2.214139</td>\n",
       "      <td>-0.733613</td>\n",
       "      <td>-0.164180</td>\n",
       "      <td>0.633026</td>\n",
       "      <td>3.138531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Void_Fraction</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-1.150956e-03</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>-4.375711</td>\n",
       "      <td>-0.453389</td>\n",
       "      <td>0.244716</td>\n",
       "      <td>0.733876</td>\n",
       "      <td>1.561608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Surf._Area_(m2/g)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-9.043582e-04</td>\n",
       "      <td>1.000393</td>\n",
       "      <td>-2.065405</td>\n",
       "      <td>-0.749266</td>\n",
       "      <td>0.038260</td>\n",
       "      <td>0.800759</td>\n",
       "      <td>2.432178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Vol._Surf._Area</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.511273e-04</td>\n",
       "      <td>1.000388</td>\n",
       "      <td>-3.545858</td>\n",
       "      <td>-0.383383</td>\n",
       "      <td>0.207659</td>\n",
       "      <td>0.648353</td>\n",
       "      <td>2.643384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Density</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.162813e-03</td>\n",
       "      <td>1.002420</td>\n",
       "      <td>-1.681296</td>\n",
       "      <td>-0.773021</td>\n",
       "      <td>-0.196836</td>\n",
       "      <td>0.543656</td>\n",
       "      <td>8.953569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_valence_pa</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>1.231356e-03</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>-1.379504</td>\n",
       "      <td>-0.918210</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_atomic_rad_pa_(angstroms)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>6.866145e-04</td>\n",
       "      <td>1.000712</td>\n",
       "      <td>-2.006160</td>\n",
       "      <td>-0.842487</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>1.009197</td>\n",
       "      <td>1.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_affinity_pa_(eV)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>2.319463e-04</td>\n",
       "      <td>1.000480</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-0.566297</td>\n",
       "      <td>1.290621</td>\n",
       "      <td>1.290621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_ionization_potential_pa_(eV)</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-6.576941e-04</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.882982</td>\n",
       "      <td>0.882982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_electronegativity_pa</th>\n",
       "      <td>68744.0</td>\n",
       "      <td>-1.999659e-04</td>\n",
       "      <td>1.001651</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-0.193532</td>\n",
       "      <td>0.231429</td>\n",
       "      <td>2.305082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     count          mean       std       min  \\\n",
       "Mafp_Br1_C2_C1                     68744.0  6.266536e-07  0.000120  0.000000   \n",
       "Mafp_Br1_C2_C2                     68744.0  7.168371e-06  0.000603  0.000000   \n",
       "Mafp_Br1_C2_C3                     68744.0  1.409526e-05  0.000911  0.000000   \n",
       "Mafp_Br1_C3_Br1                    68744.0  4.574442e-08  0.000012  0.000000   \n",
       "Mafp_Br1_C3_C1                     68744.0  7.367559e-06  0.000676  0.000000   \n",
       "Mafp_Br1_C3_C2                     68744.0  6.839972e-04  0.005644  0.000000   \n",
       "Mafp_Br1_C3_C3                     68744.0  1.384394e-02  0.052858  0.000000   \n",
       "Mafp_Br1_C3_C4                     68744.0  3.383940e-05  0.000767  0.000000   \n",
       "Mafp_Br1_C3_N1                     68744.0  2.814522e-05  0.001393  0.000000   \n",
       "Mafp_Br1_C3_N2                     68744.0  1.576132e-03  0.012806  0.000000   \n",
       "Mafp_Br1_C3_N3                     68744.0  1.144057e-06  0.000154  0.000000   \n",
       "Mafp_Br1_C3_O1                     68744.0  1.185981e-05  0.000714  0.000000   \n",
       "Mafp_Br1_C4_Br1                    68744.0  3.110570e-04  0.003532  0.000000   \n",
       "Mafp_Br1_C4_C2                     68744.0  7.195851e-06  0.000349  0.000000   \n",
       "Mafp_Br1_C4_C3                     68744.0  4.126802e-04  0.005901  0.000000   \n",
       "Mafp_Br1_C4_C4                     68744.0  4.392599e-03  0.034286  0.000000   \n",
       "Mafp_Br1_C4_H1                     68744.0  7.344056e-04  0.007350  0.000000   \n",
       "Mafp_Br1_C4_N2                     68744.0  1.574989e-07  0.000031  0.000000   \n",
       "Mafp_Br1_C4_N3                     68744.0  1.225004e-04  0.004595  0.000000   \n",
       "Mafp_Br1_C4_O1                     68744.0  7.551543e-06  0.000546  0.000000   \n",
       "Mafp_Br1_C4_O2                     68744.0  1.653037e-07  0.000043  0.000000   \n",
       "Mafp_Br1_N2_C3                     68744.0  1.438410e-04  0.003695  0.000000   \n",
       "Mafp_Br1_N2_N2                     68744.0  5.489657e-06  0.000668  0.000000   \n",
       "Mafp_Br1_N3_C3                     68744.0  7.421798e-08  0.000019  0.000000   \n",
       "Mafp_Br1_N3_N2                     68744.0  7.421798e-08  0.000019  0.000000   \n",
       "Mafp_Br1_O2_C2                     68744.0  8.798120e-07  0.000115  0.000000   \n",
       "Mafp_Br1_O2_C3                     68744.0  2.641113e-04  0.002714  0.000000   \n",
       "Mafp_Br1_O2_C4                     68744.0  6.465272e-07  0.000095  0.000000   \n",
       "Mafp_C1_C2_C2                      68744.0  1.360075e-03  0.008589  0.000000   \n",
       "Mafp_C1_C2_C3                      68744.0  6.665950e-05  0.001327  0.000000   \n",
       "...                                    ...           ...       ...       ...   \n",
       "Mmfp_MQNs27                        68744.0  1.194563e-01  0.122628  0.000000   \n",
       "Mmfp_MQNs28                        68744.0  1.135045e-01  0.084655  0.000000   \n",
       "Mmfp_MQNs29                        68744.0  3.440944e-02  0.055053  0.000000   \n",
       "Mmfp_MQNs30                        68744.0  1.556043e-02  0.026237  0.000000   \n",
       "Mmfp_MQNs31                        68744.0  2.165416e-01  0.159267  0.000000   \n",
       "Mmfp_MQNs32                        68744.0  3.722311e-02  0.071931  0.000000   \n",
       "Mmfp_MQNs33                        68744.0  4.585792e-05  0.000858  0.000000   \n",
       "Mmfp_MQNs34                        68744.0  1.776861e-02  0.047207  0.000000   \n",
       "Mmfp_MQNs35                        68744.0  8.878340e-06  0.000365  0.000000   \n",
       "Mmfp_MQNs36                        68744.0  4.820615e-02  0.034770  0.000000   \n",
       "Mmfp_MQNs37                        68744.0  1.599475e-04  0.001341  0.000000   \n",
       "Mmfp_MQNs38                        68744.0  1.631785e-05  0.000473  0.000000   \n",
       "Mmfp_MQNs39                        68744.0  1.924747e-06  0.000138  0.000000   \n",
       "Mmfp_MQNs40                        68744.0  1.037908e-05  0.000339  0.000000   \n",
       "Mmfp_MQNs41                        68744.0  6.315632e-02  0.080369  0.000000   \n",
       "Mmfp_MQNs42                        68744.0  6.505104e-02  0.103505  0.000000   \n",
       "Mmfp_NumAliphaticRings             68744.0  2.483904e-02  0.049269  0.000000   \n",
       "Mmfp_NumAromaticRings              68744.0  4.137902e-02  0.032677  0.000000   \n",
       "Mmfp_tpsa                          68744.0  1.039341e+00  0.504261  0.000000   \n",
       "norm_Dom._Pore_(ang.)              68744.0 -2.006148e-03  0.998808 -2.011702   \n",
       "norm_Max._Pore_(ang.)              68744.0 -2.239808e-03  0.998836 -2.214139   \n",
       "norm_Void_Fraction                 68744.0 -1.150956e-03  0.999552 -4.375711   \n",
       "norm_Surf._Area_(m2/g)             68744.0 -9.043582e-04  1.000393 -2.065405   \n",
       "norm_Vol._Surf._Area               68744.0  6.511273e-04  1.000388 -3.545858   \n",
       "norm_Density                       68744.0  2.162813e-03  1.002420 -1.681296   \n",
       "norm_valence_pa                    68744.0  1.231356e-03  1.000226 -1.379504   \n",
       "norm_atomic_rad_pa_(angstroms)     68744.0  6.866145e-04  1.000712 -2.006160   \n",
       "norm_affinity_pa_(eV)              68744.0  2.319463e-04  1.000480 -1.103888   \n",
       "norm_ionization_potential_pa_(eV)  68744.0 -6.576941e-04  0.999688 -1.561358   \n",
       "norm_electronegativity_pa          68744.0 -1.999659e-04  1.001651 -1.177238   \n",
       "\n",
       "                                        25%       50%       75%       max  \n",
       "Mafp_Br1_C2_C1                     0.000000  0.000000  0.000000  0.026917  \n",
       "Mafp_Br1_C2_C2                     0.000000  0.000000  0.000000  0.100000  \n",
       "Mafp_Br1_C2_C3                     0.000000  0.000000  0.000000  0.125000  \n",
       "Mafp_Br1_C3_Br1                    0.000000  0.000000  0.000000  0.003145  \n",
       "Mafp_Br1_C3_C1                     0.000000  0.000000  0.000000  0.142857  \n",
       "Mafp_Br1_C3_C2                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_C3_C3                     0.000000  0.000000  0.000000  0.500000  \n",
       "Mafp_Br1_C3_C4                     0.000000  0.000000  0.000000  0.055556  \n",
       "Mafp_Br1_C3_N1                     0.000000  0.000000  0.000000  0.125000  \n",
       "Mafp_Br1_C3_N2                     0.000000  0.000000  0.000000  0.400000  \n",
       "Mafp_Br1_C3_N3                     0.000000  0.000000  0.000000  0.033333  \n",
       "Mafp_Br1_C3_O1                     0.000000  0.000000  0.000000  0.073260  \n",
       "Mafp_Br1_C4_Br1                    0.000000  0.000000  0.000000  0.142857  \n",
       "Mafp_Br1_C4_C2                     0.000000  0.000000  0.000000  0.035714  \n",
       "Mafp_Br1_C4_C3                     0.000000  0.000000  0.000000  0.285714  \n",
       "Mafp_Br1_C4_C4                     0.000000  0.000000  0.000000  0.900000  \n",
       "Mafp_Br1_C4_H1                     0.000000  0.000000  0.000000  0.300000  \n",
       "Mafp_Br1_C4_N2                     0.000000  0.000000  0.000000  0.007426  \n",
       "Mafp_Br1_C4_N3                     0.000000  0.000000  0.000000  0.400000  \n",
       "Mafp_Br1_C4_O1                     0.000000  0.000000  0.000000  0.066667  \n",
       "Mafp_Br1_C4_O2                     0.000000  0.000000  0.000000  0.011364  \n",
       "Mafp_Br1_N2_C3                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_N2_N2                     0.000000  0.000000  0.000000  0.142857  \n",
       "Mafp_Br1_N3_C3                     0.000000  0.000000  0.000000  0.005102  \n",
       "Mafp_Br1_N3_N2                     0.000000  0.000000  0.000000  0.005102  \n",
       "Mafp_Br1_O2_C2                     0.000000  0.000000  0.000000  0.025641  \n",
       "Mafp_Br1_O2_C3                     0.000000  0.000000  0.000000  0.090909  \n",
       "Mafp_Br1_O2_C4                     0.000000  0.000000  0.000000  0.020279  \n",
       "Mafp_C1_C2_C2                      0.000000  0.000000  0.000000  0.400000  \n",
       "Mafp_C1_C2_C3                      0.000000  0.000000  0.000000  0.066667  \n",
       "...                                     ...       ...       ...       ...  \n",
       "Mmfp_MQNs27                        0.017544  0.087071  0.178533  0.714286  \n",
       "Mmfp_MQNs28                        0.051020  0.082820  0.162340  0.447222  \n",
       "Mmfp_MQNs29                        0.000000  0.000000  0.069987  0.272727  \n",
       "Mmfp_MQNs30                        0.000000  0.000000  0.023438  0.300000  \n",
       "Mmfp_MQNs31                        0.083333  0.213113  0.342382  0.605273  \n",
       "Mmfp_MQNs32                        0.000000  0.000000  0.049383  0.444444  \n",
       "Mmfp_MQNs33                        0.000000  0.000000  0.000000  0.049020  \n",
       "Mmfp_MQNs34                        0.000000  0.000000  0.000000  0.428571  \n",
       "Mmfp_MQNs35                        0.000000  0.000000  0.000000  0.036571  \n",
       "Mmfp_MQNs36                        0.021911  0.046018  0.072740  0.182231  \n",
       "Mmfp_MQNs37                        0.000000  0.000000  0.000000  0.041667  \n",
       "Mmfp_MQNs38                        0.000000  0.000000  0.000000  0.047619  \n",
       "Mmfp_MQNs39                        0.000000  0.000000  0.000000  0.016949  \n",
       "Mmfp_MQNs40                        0.000000  0.000000  0.000000  0.033333  \n",
       "Mmfp_MQNs41                        0.000000  0.035398  0.098765  0.571429  \n",
       "Mmfp_MQNs42                        0.000000  0.021429  0.086386  0.857143  \n",
       "Mmfp_NumAliphaticRings             0.000000  0.000000  0.029023  0.428571  \n",
       "Mmfp_NumAromaticRings              0.014286  0.039216  0.063995  0.180958  \n",
       "Mmfp_tpsa                          0.627570  0.973388  1.378817  3.080833  \n",
       "norm_Dom._Pore_(ang.)             -0.762865 -0.195212  0.599503  3.437769  \n",
       "norm_Max._Pore_(ang.)             -0.733613 -0.164180  0.633026  3.138531  \n",
       "norm_Void_Fraction                -0.453389  0.244716  0.733876  1.561608  \n",
       "norm_Surf._Area_(m2/g)            -0.749266  0.038260  0.800759  2.432178  \n",
       "norm_Vol._Surf._Area              -0.383383  0.207659  0.648353  2.643384  \n",
       "norm_Density                      -0.773021 -0.196836  0.543656  8.953569  \n",
       "norm_valence_pa                   -0.918210  0.926967  0.926967  0.926967  \n",
       "norm_atomic_rad_pa_(angstroms)    -0.842487  0.754378  1.009197  1.009197  \n",
       "norm_affinity_pa_(eV)             -1.103888 -0.566297  1.290621  1.290621  \n",
       "norm_ionization_potential_pa_(eV) -1.561358  0.057798  0.882982  0.882982  \n",
       "norm_electronegativity_pa         -1.177238 -0.193532  0.231429  2.305082  \n",
       "\n",
       "[385 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of training ( and test)\n",
    "train_stats = train_fp.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.309243e-05</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C2_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.493797e-06</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_Br1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.212262e-06</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>7.759606e-04</td>\n",
       "      <td>0.005888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.453487e-02</td>\n",
       "      <td>0.053843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_C4</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.962438e-05</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.636554e-05</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.564576e-03</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_N3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C3_O1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>8.886437e-06</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_Br1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.657655e-04</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>3.401040e-06</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>4.194847e-04</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_C4</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>4.129295e-03</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_H1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>7.777665e-04</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_N2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_N3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>9.944692e-05</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_O1</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.257316e-06</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_C4_O2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N2_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.879310e-04</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N2_N2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.517775e-06</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N3_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_N3_N2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.746589e-04</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_Br1_O2_C4</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_C1_C2_C2</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.370976e-03</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mafp_C1_C2_C3</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>4.901576e-05</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs27</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.194960e-01</td>\n",
       "      <td>0.122206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs28</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.112712e-01</td>\n",
       "      <td>0.082792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050691</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>0.157783</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs29</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>3.495908e-02</td>\n",
       "      <td>0.055314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072911</td>\n",
       "      <td>0.228814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs30</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.562436e-02</td>\n",
       "      <td>0.026314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs31</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.193217e-01</td>\n",
       "      <td>0.160513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086896</td>\n",
       "      <td>0.215531</td>\n",
       "      <td>0.344723</td>\n",
       "      <td>0.602730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs32</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>3.680896e-02</td>\n",
       "      <td>0.072690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs33</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>5.247439e-05</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs34</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.732539e-02</td>\n",
       "      <td>0.047142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs35</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.006872e-05</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs36</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>4.882455e-02</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022062</td>\n",
       "      <td>0.046786</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.180578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs37</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.972792e-04</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs38</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.240361e-05</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs39</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.334601e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs40</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>7.581307e-06</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs41</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>6.336283e-02</td>\n",
       "      <td>0.081139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035385</td>\n",
       "      <td>0.099041</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_MQNs42</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>6.474942e-02</td>\n",
       "      <td>0.104280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.084686</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_NumAliphaticRings</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.457951e-02</td>\n",
       "      <td>0.049520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_NumAromaticRings</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>4.185037e-02</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.039674</td>\n",
       "      <td>0.065472</td>\n",
       "      <td>0.178897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mmfp_tpsa</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.025295e+00</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620986</td>\n",
       "      <td>0.954768</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>3.080833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Dom._Pore_(ang.)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.805586e-02</td>\n",
       "      <td>1.010555</td>\n",
       "      <td>-1.898171</td>\n",
       "      <td>-0.762865</td>\n",
       "      <td>-0.081681</td>\n",
       "      <td>0.599503</td>\n",
       "      <td>3.437769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Max._Pore_(ang.)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>2.015886e-02</td>\n",
       "      <td>1.010255</td>\n",
       "      <td>-1.986365</td>\n",
       "      <td>-0.733613</td>\n",
       "      <td>-0.164180</td>\n",
       "      <td>0.633026</td>\n",
       "      <td>3.138531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Void_Fraction</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.035890e-02</td>\n",
       "      <td>1.004029</td>\n",
       "      <td>-4.373737</td>\n",
       "      <td>-0.436701</td>\n",
       "      <td>0.269914</td>\n",
       "      <td>0.745570</td>\n",
       "      <td>1.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Surf._Area_(m2/g)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>8.139461e-03</td>\n",
       "      <td>0.996483</td>\n",
       "      <td>-2.065405</td>\n",
       "      <td>-0.747613</td>\n",
       "      <td>0.061212</td>\n",
       "      <td>0.811476</td>\n",
       "      <td>2.415071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Vol._Surf._Area</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>-5.860316e-03</td>\n",
       "      <td>0.996546</td>\n",
       "      <td>-3.545858</td>\n",
       "      <td>-0.385618</td>\n",
       "      <td>0.186248</td>\n",
       "      <td>0.642950</td>\n",
       "      <td>2.653957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_Density</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>-1.946588e-02</td>\n",
       "      <td>0.977798</td>\n",
       "      <td>-1.679026</td>\n",
       "      <td>-0.785019</td>\n",
       "      <td>-0.215904</td>\n",
       "      <td>0.542286</td>\n",
       "      <td>6.309613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_valence_pa</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>-1.108253e-02</td>\n",
       "      <td>0.997964</td>\n",
       "      <td>-1.379504</td>\n",
       "      <td>-0.918210</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.926967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_atomic_rad_pa_(angstroms)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>-6.179710e-03</td>\n",
       "      <td>0.993608</td>\n",
       "      <td>-2.006160</td>\n",
       "      <td>-0.842487</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>1.009197</td>\n",
       "      <td>1.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_affinity_pa_(eV)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>-2.087578e-03</td>\n",
       "      <td>0.995729</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-1.103888</td>\n",
       "      <td>-0.566297</td>\n",
       "      <td>1.290621</td>\n",
       "      <td>1.290621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_ionization_potential_pa_(eV)</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>5.919419e-03</td>\n",
       "      <td>1.002849</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>-1.561358</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.882982</td>\n",
       "      <td>0.882982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_electronegativity_pa</th>\n",
       "      <td>7638.0</td>\n",
       "      <td>1.799746e-03</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-1.177238</td>\n",
       "      <td>-0.193532</td>\n",
       "      <td>0.231429</td>\n",
       "      <td>2.305082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    count          mean       std       min  \\\n",
       "Mafp_Br1_C2_C1                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_C2_C2                     7638.0  1.309243e-05  0.001144  0.000000   \n",
       "Mafp_Br1_C2_C3                     7638.0  2.493797e-06  0.000218  0.000000   \n",
       "Mafp_Br1_C3_Br1                    7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_C3_C1                     7638.0  1.212262e-06  0.000106  0.000000   \n",
       "Mafp_Br1_C3_C2                     7638.0  7.759606e-04  0.005888  0.000000   \n",
       "Mafp_Br1_C3_C3                     7638.0  1.453487e-02  0.053843  0.000000   \n",
       "Mafp_Br1_C3_C4                     7638.0  2.962438e-05  0.000606  0.000000   \n",
       "Mafp_Br1_C3_N1                     7638.0  1.636554e-05  0.000826  0.000000   \n",
       "Mafp_Br1_C3_N2                     7638.0  1.564576e-03  0.011956  0.000000   \n",
       "Mafp_Br1_C3_N3                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_C3_O1                     7638.0  8.886437e-06  0.000549  0.000000   \n",
       "Mafp_Br1_C4_Br1                    7638.0  2.657655e-04  0.003141  0.000000   \n",
       "Mafp_Br1_C4_C2                     7638.0  3.401040e-06  0.000141  0.000000   \n",
       "Mafp_Br1_C4_C3                     7638.0  4.194847e-04  0.005673  0.000000   \n",
       "Mafp_Br1_C4_C4                     7638.0  4.129295e-03  0.032860  0.000000   \n",
       "Mafp_Br1_C4_H1                     7638.0  7.777665e-04  0.008173  0.000000   \n",
       "Mafp_Br1_C4_N2                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_C4_N3                     7638.0  9.944692e-05  0.004105  0.000000   \n",
       "Mafp_Br1_C4_O1                     7638.0  2.257316e-06  0.000197  0.000000   \n",
       "Mafp_Br1_C4_O2                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_N2_C3                     7638.0  1.879310e-04  0.004204  0.000000   \n",
       "Mafp_Br1_N2_N2                     7638.0  2.517775e-06  0.000220  0.000000   \n",
       "Mafp_Br1_N3_C3                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_N3_N2                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_O2_C2                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_Br1_O2_C3                     7638.0  2.746589e-04  0.002538  0.000000   \n",
       "Mafp_Br1_O2_C4                     7638.0  0.000000e+00  0.000000  0.000000   \n",
       "Mafp_C1_C2_C2                      7638.0  1.370976e-03  0.009011  0.000000   \n",
       "Mafp_C1_C2_C3                      7638.0  4.901576e-05  0.001072  0.000000   \n",
       "...                                   ...           ...       ...       ...   \n",
       "Mmfp_MQNs27                        7638.0  1.194960e-01  0.122206  0.000000   \n",
       "Mmfp_MQNs28                        7638.0  1.112712e-01  0.082792  0.000000   \n",
       "Mmfp_MQNs29                        7638.0  3.495908e-02  0.055314  0.000000   \n",
       "Mmfp_MQNs30                        7638.0  1.562436e-02  0.026314  0.000000   \n",
       "Mmfp_MQNs31                        7638.0  2.193217e-01  0.160513  0.000000   \n",
       "Mmfp_MQNs32                        7638.0  3.680896e-02  0.072690  0.000000   \n",
       "Mmfp_MQNs33                        7638.0  5.247439e-05  0.001049  0.000000   \n",
       "Mmfp_MQNs34                        7638.0  1.732539e-02  0.047142  0.000000   \n",
       "Mmfp_MQNs35                        7638.0  1.006872e-05  0.000431  0.000000   \n",
       "Mmfp_MQNs36                        7638.0  4.882455e-02  0.034862  0.000000   \n",
       "Mmfp_MQNs37                        7638.0  1.972792e-04  0.001522  0.000000   \n",
       "Mmfp_MQNs38                        7638.0  1.240361e-05  0.000358  0.000000   \n",
       "Mmfp_MQNs39                        7638.0  1.334601e-07  0.000012  0.000000   \n",
       "Mmfp_MQNs40                        7638.0  7.581307e-06  0.000251  0.000000   \n",
       "Mmfp_MQNs41                        7638.0  6.336283e-02  0.081139  0.000000   \n",
       "Mmfp_MQNs42                        7638.0  6.474942e-02  0.104280  0.000000   \n",
       "Mmfp_NumAliphaticRings             7638.0  2.457951e-02  0.049520  0.000000   \n",
       "Mmfp_NumAromaticRings              7638.0  4.185037e-02  0.032795  0.000000   \n",
       "Mmfp_tpsa                          7638.0  1.025295e+00  0.496262  0.000000   \n",
       "norm_Dom._Pore_(ang.)              7638.0  1.805586e-02  1.010555 -1.898171   \n",
       "norm_Max._Pore_(ang.)              7638.0  2.015886e-02  1.010255 -1.986365   \n",
       "norm_Void_Fraction                 7638.0  1.035890e-02  1.004029 -4.373737   \n",
       "norm_Surf._Area_(m2/g)             7638.0  8.139461e-03  0.996483 -2.065405   \n",
       "norm_Vol._Surf._Area               7638.0 -5.860316e-03  0.996546 -3.545858   \n",
       "norm_Density                       7638.0 -1.946588e-02  0.977798 -1.679026   \n",
       "norm_valence_pa                    7638.0 -1.108253e-02  0.997964 -1.379504   \n",
       "norm_atomic_rad_pa_(angstroms)     7638.0 -6.179710e-03  0.993608 -2.006160   \n",
       "norm_affinity_pa_(eV)              7638.0 -2.087578e-03  0.995729 -1.103888   \n",
       "norm_ionization_potential_pa_(eV)  7638.0  5.919419e-03  1.002849 -1.561358   \n",
       "norm_electronegativity_pa          7638.0  1.799746e-03  0.985079 -1.177238   \n",
       "\n",
       "                                        25%       50%       75%       max  \n",
       "Mafp_Br1_C2_C1                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C2_C2                     0.000000  0.000000  0.000000  0.100000  \n",
       "Mafp_Br1_C2_C3                     0.000000  0.000000  0.000000  0.019048  \n",
       "Mafp_Br1_C3_Br1                    0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C3_C1                     0.000000  0.000000  0.000000  0.009259  \n",
       "Mafp_Br1_C3_C2                     0.000000  0.000000  0.000000  0.130952  \n",
       "Mafp_Br1_C3_C3                     0.000000  0.000000  0.000000  0.466667  \n",
       "Mafp_Br1_C3_C4                     0.000000  0.000000  0.000000  0.022399  \n",
       "Mafp_Br1_C3_N1                     0.000000  0.000000  0.000000  0.041667  \n",
       "Mafp_Br1_C3_N2                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_C3_N3                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C3_O1                     0.000000  0.000000  0.000000  0.035088  \n",
       "Mafp_Br1_C4_Br1                    0.000000  0.000000  0.000000  0.076923  \n",
       "Mafp_Br1_C4_C2                     0.000000  0.000000  0.000000  0.008621  \n",
       "Mafp_Br1_C4_C3                     0.000000  0.000000  0.000000  0.142857  \n",
       "Mafp_Br1_C4_C4                     0.000000  0.000000  0.000000  0.750000  \n",
       "Mafp_Br1_C4_H1                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_C4_N2                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_C4_N3                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_C4_O1                     0.000000  0.000000  0.000000  0.017241  \n",
       "Mafp_Br1_C4_O2                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_N2_C3                     0.000000  0.000000  0.000000  0.250000  \n",
       "Mafp_Br1_N2_N2                     0.000000  0.000000  0.000000  0.019231  \n",
       "Mafp_Br1_N3_C3                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_N3_N2                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_O2_C2                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_Br1_O2_C3                     0.000000  0.000000  0.000000  0.062731  \n",
       "Mafp_Br1_O2_C4                     0.000000  0.000000  0.000000  0.000000  \n",
       "Mafp_C1_C2_C2                      0.000000  0.000000  0.000000  0.200000  \n",
       "Mafp_C1_C2_C3                      0.000000  0.000000  0.000000  0.050000  \n",
       "...                                     ...       ...       ...       ...  \n",
       "Mmfp_MQNs27                        0.016667  0.088889  0.178344  0.800000  \n",
       "Mmfp_MQNs28                        0.050691  0.080952  0.157783  0.444444  \n",
       "Mmfp_MQNs29                        0.000000  0.000000  0.072911  0.228814  \n",
       "Mmfp_MQNs30                        0.000000  0.000000  0.022989  0.230769  \n",
       "Mmfp_MQNs31                        0.086896  0.215531  0.344723  0.602730  \n",
       "Mmfp_MQNs32                        0.000000  0.000000  0.044444  0.428571  \n",
       "Mmfp_MQNs33                        0.000000  0.000000  0.000000  0.041667  \n",
       "Mmfp_MQNs34                        0.000000  0.000000  0.000000  0.428571  \n",
       "Mmfp_MQNs35                        0.000000  0.000000  0.000000  0.029304  \n",
       "Mmfp_MQNs36                        0.022062  0.046786  0.074074  0.180578  \n",
       "Mmfp_MQNs37                        0.000000  0.000000  0.000000  0.031915  \n",
       "Mmfp_MQNs38                        0.000000  0.000000  0.000000  0.020833  \n",
       "Mmfp_MQNs39                        0.000000  0.000000  0.000000  0.001019  \n",
       "Mmfp_MQNs40                        0.000000  0.000000  0.000000  0.015625  \n",
       "Mmfp_MQNs41                        0.000000  0.035385  0.099041  0.571429  \n",
       "Mmfp_MQNs42                        0.000000  0.020833  0.084686  0.857143  \n",
       "Mmfp_NumAliphaticRings             0.000000  0.000000  0.027778  0.428571  \n",
       "Mmfp_NumAromaticRings              0.014706  0.039674  0.065472  0.178897  \n",
       "Mmfp_tpsa                          0.620986  0.954768  1.361704  3.080833  \n",
       "norm_Dom._Pore_(ang.)             -0.762865 -0.081681  0.599503  3.437769  \n",
       "norm_Max._Pore_(ang.)             -0.733613 -0.164180  0.633026  3.138531  \n",
       "norm_Void_Fraction                -0.436701  0.269914  0.745570  1.515700  \n",
       "norm_Surf._Area_(m2/g)            -0.747613  0.061212  0.811476  2.415071  \n",
       "norm_Vol._Surf._Area              -0.385618  0.186248  0.642950  2.653957  \n",
       "norm_Density                      -0.785019 -0.215904  0.542286  6.309613  \n",
       "norm_valence_pa                   -0.918210  0.926967  0.926967  0.926967  \n",
       "norm_atomic_rad_pa_(angstroms)    -0.842487  0.754378  1.009197  1.009197  \n",
       "norm_affinity_pa_(eV)             -1.103888 -0.566297  1.290621  1.290621  \n",
       "norm_ionization_potential_pa_(eV) -1.561358  0.057798  0.882982  0.882982  \n",
       "norm_electronegativity_pa         -1.177238 -0.193532  0.231429  2.305082  \n",
       "\n",
       "[385 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats = test_fp.describe()\n",
    "test_stats = test_stats.transpose()\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(100, activation='relu', input_shape=[len(train_fp.keys())]),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "        optimizer='adam',\n",
    "        metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               38600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,901\n",
      "Trainable params: 58,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54995 samples, validate on 13749 samples\n",
      "Epoch 1/1000\n",
      "54560/54995 [============================>.] - ETA: 0s - loss: 0.1942 - mae: 0.2131 - mse: 0.1942\n",
      "Epoch 00001: val_loss improved from inf to 0.14005, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.1935,  mae:0.2128,  mse:0.1935,  val_loss:0.1400,  val_mae:0.1733,  val_mse:0.1400,  \n",
      "54995/54995 [==============================] - 7s 133us/sample - loss: 0.1935 - mae: 0.2128 - mse: 0.1935 - val_loss: 0.1400 - val_mae: 0.1733 - val_mse: 0.1400\n",
      "Epoch 2/1000\n",
      "54528/54995 [============================>.] - ETA: 0s - loss: 0.1471 - mae: 0.1769 - mse: 0.1471\n",
      "Epoch 00002: val_loss improved from 0.14005 to 0.12258, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 122us/sample - loss: 0.1465 - mae: 0.1767 - mse: 0.1465 - val_loss: 0.1226 - val_mae: 0.1541 - val_mse: 0.1226\n",
      "Epoch 3/1000\n",
      "54912/54995 [============================>.] - ETA: 0s - loss: 0.1316 - mae: 0.1647 - mse: 0.1316\n",
      "Epoch 00003: val_loss improved from 0.12258 to 0.11800, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.1337 - mae: 0.1648 - mse: 0.1337 - val_loss: 0.1180 - val_mae: 0.1508 - val_mse: 0.1180\n",
      "Epoch 4/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.1268 - mae: 0.1597 - mse: 0.1268\n",
      "Epoch 00004: val_loss improved from 0.11800 to 0.11748, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 117us/sample - loss: 0.1290 - mae: 0.1599 - mse: 0.1290 - val_loss: 0.1175 - val_mae: 0.1729 - val_mse: 0.1175\n",
      "Epoch 5/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.1226 - mae: 0.1540 - mse: 0.1226\n",
      "Epoch 00005: val_loss did not improve from 0.11748\n",
      "54995/54995 [==============================] - 7s 122us/sample - loss: 0.1226 - mae: 0.1540 - mse: 0.1226 - val_loss: 0.1193 - val_mae: 0.1458 - val_mse: 0.1193\n",
      "Epoch 6/1000\n",
      "54624/54995 [============================>.] - ETA: 0s - loss: 0.1206 - mae: 0.1511 - mse: 0.1206\n",
      "Epoch 00006: val_loss improved from 0.11748 to 0.11178, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 124us/sample - loss: 0.1203 - mae: 0.1511 - mse: 0.1203 - val_loss: 0.1118 - val_mae: 0.1462 - val_mse: 0.1118\n",
      "Epoch 7/1000\n",
      "54624/54995 [============================>.] - ETA: 0s - loss: 0.1149 - mae: 0.1493 - mse: 0.1149\n",
      "Epoch 00007: val_loss did not improve from 0.11178\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.1169 - mae: 0.1494 - mse: 0.1169 - val_loss: 0.1124 - val_mae: 0.1644 - val_mse: 0.1124\n",
      "Epoch 8/1000\n",
      "54784/54995 [============================>.] - ETA: 0s - loss: 0.1151 - mae: 0.1452 - mse: 0.1151\n",
      "Epoch 00008: val_loss improved from 0.11178 to 0.11123, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.1149 - mae: 0.1452 - mse: 0.1149 - val_loss: 0.1112 - val_mae: 0.1490 - val_mse: 0.1112\n",
      "Epoch 9/1000\n",
      "54624/54995 [============================>.] - ETA: 0s - loss: 0.1127 - mae: 0.1441 - mse: 0.1127\n",
      "Epoch 00009: val_loss improved from 0.11123 to 0.10881, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 121us/sample - loss: 0.1124 - mae: 0.1440 - mse: 0.1124 - val_loss: 0.1088 - val_mae: 0.1409 - val_mse: 0.1088\n",
      "Epoch 10/1000\n",
      "54880/54995 [============================>.] - ETA: 0s - loss: 0.1108 - mae: 0.1431 - mse: 0.1108\n",
      "Epoch 00010: val_loss improved from 0.10881 to 0.10685, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 121us/sample - loss: 0.1108 - mae: 0.1431 - mse: 0.1108 - val_loss: 0.1068 - val_mae: 0.1380 - val_mse: 0.1068\n",
      "Epoch 11/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.1066 - mae: 0.1399 - mse: 0.1066\n",
      "Epoch 00011: val_loss did not improve from 0.10685\n",
      "54995/54995 [==============================] - 6s 114us/sample - loss: 0.1088 - mae: 0.1401 - mse: 0.1088 - val_loss: 0.1164 - val_mae: 0.1688 - val_mse: 0.1164\n",
      "Epoch 12/1000\n",
      "54784/54995 [============================>.] - ETA: 0s - loss: 0.1079 - mae: 0.1397 - mse: 0.1079\n",
      "Epoch 00012: val_loss did not improve from 0.10685\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.1076 - mae: 0.1396 - mse: 0.1076 - val_loss: 0.1087 - val_mae: 0.1381 - val_mse: 0.1087\n",
      "Epoch 13/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.1065 - mae: 0.1380 - mse: 0.1065\n",
      "Epoch 00013: val_loss improved from 0.10685 to 0.10402, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.1064 - mae: 0.1380 - mse: 0.1064 - val_loss: 0.1040 - val_mae: 0.1357 - val_mse: 0.1040\n",
      "Epoch 14/1000\n",
      "54496/54995 [============================>.] - ETA: 0s - loss: 0.1004 - mae: 0.1357 - mse: 0.1004\n",
      "Epoch 00014: val_loss did not improve from 0.10402\n",
      "54995/54995 [==============================] - 6s 116us/sample - loss: 0.1041 - mae: 0.1363 - mse: 0.1041 - val_loss: 0.1184 - val_mae: 0.1769 - val_mse: 0.1184\n",
      "Epoch 15/1000\n",
      "54752/54995 [============================>.] - ETA: 0s - loss: 0.1039 - mae: 0.1351 - mse: 0.1039\n",
      "Epoch 00015: val_loss did not improve from 0.10402\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.1037 - mae: 0.1350 - mse: 0.1037 - val_loss: 0.1060 - val_mae: 0.1388 - val_mse: 0.1060\n",
      "Epoch 16/1000\n",
      "54688/54995 [============================>.] - ETA: 0s - loss: 0.1026 - mae: 0.1348 - mse: 0.1026\n",
      "Epoch 00016: val_loss improved from 0.10402 to 0.10391, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 117us/sample - loss: 0.1023 - mae: 0.1348 - mse: 0.1023 - val_loss: 0.1039 - val_mae: 0.1338 - val_mse: 0.1039\n",
      "Epoch 17/1000\n",
      "54656/54995 [============================>.] - ETA: 0s - loss: 0.1019 - mae: 0.1337 - mse: 0.1019\n",
      "Epoch 00017: val_loss did not improve from 0.10391\n",
      "54995/54995 [==============================] - 6s 117us/sample - loss: 0.1015 - mae: 0.1335 - mse: 0.1015 - val_loss: 0.1069 - val_mae: 0.1417 - val_mse: 0.1069\n",
      "Epoch 18/1000\n",
      "54944/54995 [============================>.] - ETA: 0s - loss: 0.0998 - mae: 0.1314 - mse: 0.0998\n",
      "Epoch 00018: val_loss did not improve from 0.10391\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.0998 - mae: 0.1314 - mse: 0.0998 - val_loss: 0.1062 - val_mae: 0.1406 - val_mse: 0.1062\n",
      "Epoch 19/1000\n",
      "54944/54995 [============================>.] - ETA: 0s - loss: 0.0994 - mae: 0.1309 - mse: 0.0994\n",
      "Epoch 00019: val_loss did not improve from 0.10391\n",
      "54995/54995 [==============================] - 6s 116us/sample - loss: 0.0994 - mae: 0.1309 - mse: 0.0994 - val_loss: 0.1061 - val_mae: 0.1370 - val_mse: 0.1061\n",
      "Epoch 20/1000\n",
      "54592/54995 [============================>.] - ETA: 0s - loss: 0.0983 - mae: 0.1307 - mse: 0.0983\n",
      "Epoch 00020: val_loss improved from 0.10391 to 0.10359, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 118us/sample - loss: 0.0980 - mae: 0.1306 - mse: 0.0980 - val_loss: 0.1036 - val_mae: 0.1333 - val_mse: 0.1036\n",
      "Epoch 21/1000\n",
      "54496/54995 [============================>.] - ETA: 0s - loss: 0.0978 - mae: 0.1296 - mse: 0.0978\n",
      "Epoch 00021: val_loss did not improve from 0.10359\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0973 - mae: 0.1295 - mse: 0.0973 - val_loss: 0.1058 - val_mae: 0.1392 - val_mse: 0.1058\n",
      "Epoch 22/1000\n",
      "54880/54995 [============================>.] - ETA: 0s - loss: 0.0965 - mae: 0.1286 - mse: 0.0965\n",
      "Epoch 00022: val_loss improved from 0.10359 to 0.10325, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.0964 - mae: 0.1286 - mse: 0.0964 - val_loss: 0.1033 - val_mae: 0.1332 - val_mse: 0.1033\n",
      "Epoch 23/1000\n",
      "54752/54995 [============================>.] - ETA: 0s - loss: 0.0963 - mae: 0.1288 - mse: 0.0963\n",
      "Epoch 00023: val_loss improved from 0.10325 to 0.10271, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 114us/sample - loss: 0.0961 - mae: 0.1287 - mse: 0.0961 - val_loss: 0.1027 - val_mae: 0.1323 - val_mse: 0.1027\n",
      "Epoch 24/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.0945 - mae: 0.1275 - mse: 0.0945\n",
      "Epoch 00024: val_loss improved from 0.10271 to 0.10115, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 118us/sample - loss: 0.0944 - mae: 0.1275 - mse: 0.0944 - val_loss: 0.1011 - val_mae: 0.1307 - val_mse: 0.1011\n",
      "Epoch 25/1000\n",
      "54592/54995 [============================>.] - ETA: 0s - loss: 0.0938 - mae: 0.1268 - mse: 0.0938\n",
      "Epoch 00025: val_loss did not improve from 0.10115\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.0935 - mae: 0.1269 - mse: 0.0935 - val_loss: 0.1012 - val_mae: 0.1335 - val_mse: 0.1012\n",
      "Epoch 26/1000\n",
      "54944/54995 [============================>.] - ETA: 0s - loss: 0.0935 - mae: 0.1266 - mse: 0.0935\n",
      "Epoch 00026: val_loss did not improve from 0.10115\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.0935 - mae: 0.1266 - mse: 0.0935 - val_loss: 0.1025 - val_mae: 0.1346 - val_mse: 0.1025\n",
      "Epoch 27/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.0926 - mae: 0.1250 - mse: 0.0926\n",
      "Epoch 00027: val_loss improved from 0.10115 to 0.10071, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 118us/sample - loss: 0.0925 - mae: 0.1250 - mse: 0.0925 - val_loss: 0.1007 - val_mae: 0.1355 - val_mse: 0.1007\n",
      "Epoch 28/1000\n",
      "54816/54995 [============================>.] - ETA: 0s - loss: 0.0921 - mae: 0.1248 - mse: 0.0921\n",
      "Epoch 00028: val_loss did not improve from 0.10071\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.0920 - mae: 0.1247 - mse: 0.0920 - val_loss: 0.1101 - val_mae: 0.1413 - val_mse: 0.1101\n",
      "Epoch 29/1000\n",
      "54720/54995 [============================>.] - ETA: 0s - loss: 0.0920 - mae: 0.1254 - mse: 0.0920\n",
      "Epoch 00029: val_loss improved from 0.10071 to 0.09946, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 7s 120us/sample - loss: 0.0918 - mae: 0.1254 - mse: 0.0918 - val_loss: 0.0995 - val_mae: 0.1325 - val_mse: 0.0995\n",
      "Epoch 30/1000\n",
      "54912/54995 [============================>.] - ETA: 0s - loss: 0.0905 - mae: 0.1231 - mse: 0.0905\n",
      "Epoch 00030: val_loss did not improve from 0.09946\n",
      "54995/54995 [==============================] - 7s 118us/sample - loss: 0.0904 - mae: 0.1232 - mse: 0.0904 - val_loss: 0.1001 - val_mae: 0.1335 - val_mse: 0.1001\n",
      "Epoch 31/1000\n",
      "54656/54995 [============================>.] - ETA: 0s - loss: 0.0902 - mae: 0.1243 - mse: 0.0902\n",
      "Epoch 00031: val_loss did not improve from 0.09946\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0908 - mae: 0.1243 - mse: 0.0908 - val_loss: 0.1016 - val_mae: 0.1313 - val_mse: 0.1016\n",
      "Epoch 32/1000\n",
      "54976/54995 [============================>.] - ETA: 0s - loss: 0.0896 - mae: 0.1230 - mse: 0.0896\n",
      "Epoch 00032: val_loss did not improve from 0.09946\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.0896 - mae: 0.1230 - mse: 0.0896 - val_loss: 0.1011 - val_mae: 0.1553 - val_mse: 0.1011\n",
      "Epoch 33/1000\n",
      "54752/54995 [============================>.] - ETA: 0s - loss: 0.0885 - mae: 0.1215 - mse: 0.0885\n",
      "Epoch 00033: val_loss improved from 0.09946 to 0.09808, saving model to model_checkpoint.h5\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0883 - mae: 0.1214 - mse: 0.0883 - val_loss: 0.0981 - val_mae: 0.1288 - val_mse: 0.0981\n",
      "Epoch 34/1000\n",
      "54624/54995 [============================>.] - ETA: 0s - loss: 0.0866 - mae: 0.1217 - mse: 0.0866\n",
      "Epoch 00034: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 116us/sample - loss: 0.0880 - mae: 0.1218 - mse: 0.0880 - val_loss: 0.1020 - val_mae: 0.1367 - val_mse: 0.1020\n",
      "Epoch 35/1000\n",
      "54880/54995 [============================>.] - ETA: 0s - loss: 0.0875 - mae: 0.1207 - mse: 0.0875\n",
      "Epoch 00035: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 116us/sample - loss: 0.0873 - mae: 0.1207 - mse: 0.0873 - val_loss: 0.1018 - val_mae: 0.1317 - val_mse: 0.1018\n",
      "Epoch 36/1000\n",
      "54912/54995 [============================>.] - ETA: 0s - loss: 0.0886 - mae: 0.1216 - mse: 0.0886\n",
      "Epoch 00036: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 117us/sample - loss: 0.0885 - mae: 0.1215 - mse: 0.0885 - val_loss: 0.0998 - val_mae: 0.1307 - val_mse: 0.0998\n",
      "Epoch 37/1000\n",
      "54848/54995 [============================>.] - ETA: 0s - loss: 0.0861 - mae: 0.1200 - mse: 0.0861\n",
      "Epoch 00037: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 7s 119us/sample - loss: 0.0860 - mae: 0.1199 - mse: 0.0860 - val_loss: 0.1068 - val_mae: 0.1371 - val_mse: 0.1068\n",
      "Epoch 38/1000\n",
      "54688/54995 [============================>.] - ETA: 0s - loss: 0.0860 - mae: 0.1202 - mse: 0.0860\n",
      "Epoch 00038: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 7s 121us/sample - loss: 0.0861 - mae: 0.1202 - mse: 0.0861 - val_loss: 0.1097 - val_mae: 0.1419 - val_mse: 0.1097\n",
      "Epoch 39/1000\n",
      "54880/54995 [============================>.] - ETA: 0s - loss: 0.0861 - mae: 0.1202 - mse: 0.0861\n",
      "Epoch 00039: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 117us/sample - loss: 0.0860 - mae: 0.1202 - mse: 0.0860 - val_loss: 0.0988 - val_mae: 0.1298 - val_mse: 0.0988\n",
      "Epoch 40/1000\n",
      "54688/54995 [============================>.] - ETA: 0s - loss: 0.0822 - mae: 0.1179 - mse: 0.0822\n",
      "Epoch 00040: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0848 - mae: 0.1182 - mse: 0.0848 - val_loss: 0.1076 - val_mae: 0.1556 - val_mse: 0.1076\n",
      "Epoch 41/1000\n",
      "54592/54995 [============================>.] - ETA: 0s - loss: 0.0847 - mae: 0.1188 - mse: 0.0847\n",
      "Epoch 00041: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0843 - mae: 0.1187 - mse: 0.0843 - val_loss: 0.1009 - val_mae: 0.1316 - val_mse: 0.1009\n",
      "Epoch 42/1000\n",
      "54688/54995 [============================>.] - ETA: 0s - loss: 0.0835 - mae: 0.1186 - mse: 0.0835\n",
      "Epoch 00042: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 114us/sample - loss: 0.0834 - mae: 0.1186 - mse: 0.0834 - val_loss: 0.0984 - val_mae: 0.1288 - val_mse: 0.0984\n",
      "Epoch 43/1000\n",
      "54592/54995 [============================>.] - ETA: 0s - loss: 0.0837 - mae: 0.1182 - mse: 0.0837\n",
      "Epoch 00043: val_loss did not improve from 0.09808\n",
      "54995/54995 [==============================] - 6s 115us/sample - loss: 0.0833 - mae: 0.1181 - mse: 0.0833 - val_loss: 0.1007 - val_mae: 0.1333 - val_mse: 0.1007\n"
     ]
    }
   ],
   "source": [
    "# NN model training\n",
    "EPOCHS = 1000\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "checkpoint_callbacks = keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', monitor='val_loss',\\\n",
    "                                                      verbose=1, save_best_only=True, mode='min')\n",
    "# early_history = model.fit(normed_train_data, train_label.to_numpy(), \n",
    "#                     epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop,checkpoint_callbacks])\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_history = model.fit(train_fp.to_numpy(), train_label.to_numpy(), \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=1,\\\n",
    "                          callbacks=[early_stop,checkpoint_callbacks,tfdocs.modeling.EpochDots(),tensorboard_callback])\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MAE')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if run converged\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "#plt.ylim([0, 0.15])\n",
    "plt.ylabel('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7638/7638 - 0s - loss: 0.0628 - mae: 0.1275 - mse: 0.0628\n",
      "Testing set Mean Abs Error:  0.13 bg\n",
      "68744/68744 - 3s - loss: 0.0856 - mae: 0.1190 - mse: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0309 20:41:31.257315 140043953170176 legend.py:1289] No handles with labels found to put in legend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './norm_CH4_v/v_1_bar_test_parity_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e56c0bec0ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./%s_test_parity_%s.png'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, frameon, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    523\u001b[0m                             self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modules/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './norm_CH4_v/v_1_bar_test_parity_1.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation of test error and plotting parity\n",
    "\n",
    "#model = tf.keras.models.load_model('model_checkpoint_bandgap.h5')\n",
    "loss, mae, mse = model.evaluate(test_fp.to_numpy(), test_label.to_numpy(), verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} bg\".format(mae))\n",
    "\n",
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_fp.to_numpy(), train_label.to_numpy(), verbose=2)\n",
    "\n",
    "tr_rmse = math.sqrt(tr_mse)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "test_predictions = model.predict(test_fp.to_numpy()).flatten()\n",
    "\n",
    "\n",
    "train_predictions = model.predict(train_fp.to_numpy()).flatten()\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize = (8,8))\n",
    "ax1.scatter(test_label, test_predictions, c='r',s=30)\n",
    "\n",
    "ax1.scatter(train_label, train_predictions, c='b',s=30)\n",
    "\n",
    "ax1.set_xlabel('True normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_ylabel('Predicted normalized CH4 Uptake @ 1 bar',fontsize=labelfontsize)\n",
    "ax1.set_xlim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.set_ylim(min([min(test_label),min(test_predictions)])-1,max([max(test_label),max(test_predictions)])+1)\n",
    "ax1.legend()\n",
    "plot_x_min, plot_x_max = plt.xlim()\n",
    "plot_y_min, plot_y_max = plt.ylim()\n",
    "\n",
    "ax1.plot(np.linspace(plot_x_min,plot_x_max,100),np.linspace(plot_y_min,plot_y_max,100),c='k',ls='--')\n",
    "text_position_x = plot_x_min + (plot_x_max - plot_x_min) * 0.05\n",
    "text_position_y = plot_y_max - (plot_y_max - plot_y_min) * 0.15\n",
    "\n",
    "ax1.text(text_position_x, text_position_y, \"RMSE test=\" + str(\"%.4f\" % rmse) + '\\n' + \n",
    "         \"RMSE train=\" + str(\"%.4f\" % tr_rmse), ha='left', fontsize=16)\n",
    "\n",
    "# ax1.text(text_position_x, text_position_y, \"MAE=\" + str(\"%.4f\" % mae) + ' \\n' + \n",
    "#          \"MSE=\" + str(\"%.4f\" % mse), ha='left', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./%s_test_parity_%s.png'%(property_used, total_frac),dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions - test_label\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_space(4,9, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = 1\n",
    "defaults = [10, .8, 3, 100, 'relu', 'mse', 'adam', .2]\n",
    "patience = defaults[0]\n",
    "training_pct = defaults[1]\n",
    "n_layer = defaults[2]\n",
    "n_unit = defaults[3]\n",
    "activation = defaults[4]\n",
    "loss = defaults[5]\n",
    "opt = defaults[6]\n",
    "val_pct = defaults[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 100)               38600     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,901\n",
      "Trainable params: 58,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48884 samples, validate on 12222 samples\n",
      "Epoch 1/5000\n",
      "47616/48884 [============================>.] - ETA: 0s - loss: 0.1944 - mae: 0.2153 - mse: 0.1944\n",
      "Epoch 00001: val_loss improved from inf to 0.14806, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.1950,  mae:0.2147,  mse:0.1950,  val_loss:0.1481,  val_mae:0.2079,  val_mse:0.1481,  \n",
      "48884/48884 [==============================] - 2s 42us/sample - loss: 0.1950 - mae: 0.2147 - mse: 0.1950 - val_loss: 0.1481 - val_mae: 0.2079 - val_mse: 0.1481\n",
      "Epoch 2/5000\n",
      "48064/48884 [============================>.] - ETA: 0s - loss: 0.1474 - mae: 0.1765 - mse: 0.1474\n",
      "Epoch 00002: val_loss improved from 0.14806 to 0.13700, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1465 - mae: 0.1763 - mse: 0.1465 - val_loss: 0.1370 - val_mae: 0.1630 - val_mse: 0.1370\n",
      "Epoch 3/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1347 - mae: 0.1659 - mse: 0.1347\n",
      "Epoch 00003: val_loss improved from 0.13700 to 0.12152, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1365 - mae: 0.1657 - mse: 0.1365 - val_loss: 0.1215 - val_mae: 0.1687 - val_mse: 0.1215\n",
      "Epoch 4/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1314 - mae: 0.1618 - mse: 0.1314\n",
      "Epoch 00004: val_loss did not improve from 0.12152\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1311 - mae: 0.1619 - mse: 0.1311 - val_loss: 0.1239 - val_mae: 0.1733 - val_mse: 0.1239\n",
      "Epoch 5/5000\n",
      "48288/48884 [============================>.] - ETA: 0s - loss: 0.1272 - mae: 0.1561 - mse: 0.1272\n",
      "Epoch 00005: val_loss improved from 0.12152 to 0.11592, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1266 - mae: 0.1560 - mse: 0.1266 - val_loss: 0.1159 - val_mae: 0.1458 - val_mse: 0.1159\n",
      "Epoch 6/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.1245 - mae: 0.1544 - mse: 0.1245\n",
      "Epoch 00006: val_loss did not improve from 0.11592\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1241 - mae: 0.1543 - mse: 0.1241 - val_loss: 0.1194 - val_mae: 0.1491 - val_mse: 0.1194\n",
      "Epoch 7/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1209 - mae: 0.1516 - mse: 0.1209\n",
      "Epoch 00007: val_loss did not improve from 0.11592\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.1207 - mae: 0.1517 - mse: 0.1207 - val_loss: 0.1176 - val_mae: 0.1474 - val_mse: 0.1176\n",
      "Epoch 8/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1193 - mae: 0.1483 - mse: 0.1193\n",
      "Epoch 00008: val_loss improved from 0.11592 to 0.11350, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1190 - mae: 0.1483 - mse: 0.1190 - val_loss: 0.1135 - val_mae: 0.1480 - val_mse: 0.1135\n",
      "Epoch 9/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.1169 - mae: 0.1468 - mse: 0.1169\n",
      "Epoch 00009: val_loss did not improve from 0.11350\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1165 - mae: 0.1468 - mse: 0.1165 - val_loss: 0.1148 - val_mae: 0.1476 - val_mse: 0.1148\n",
      "Epoch 10/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1144 - mae: 0.1449 - mse: 0.1144\n",
      "Epoch 00010: val_loss improved from 0.11350 to 0.10856, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1138 - mae: 0.1448 - mse: 0.1138 - val_loss: 0.1086 - val_mae: 0.1374 - val_mse: 0.1086\n",
      "Epoch 11/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1144 - mae: 0.1442 - mse: 0.1144\n",
      "Epoch 00011: val_loss did not improve from 0.10856\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1137 - mae: 0.1442 - mse: 0.1137 - val_loss: 0.1142 - val_mae: 0.1441 - val_mse: 0.1142\n",
      "Epoch 12/5000\n",
      "48608/48884 [============================>.] - ETA: 0s - loss: 0.1124 - mae: 0.1423 - mse: 0.1124\n",
      "Epoch 00012: val_loss did not improve from 0.10856\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1121 - mae: 0.1423 - mse: 0.1121 - val_loss: 0.1114 - val_mae: 0.1400 - val_mse: 0.1114\n",
      "Epoch 13/5000\n",
      "48544/48884 [============================>.] - ETA: 0s - loss: 0.1098 - mae: 0.1399 - mse: 0.1098\n",
      "Epoch 00013: val_loss improved from 0.10856 to 0.10710, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1095 - mae: 0.1399 - mse: 0.1095 - val_loss: 0.1071 - val_mae: 0.1354 - val_mse: 0.1071\n",
      "Epoch 14/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1083 - mae: 0.1386 - mse: 0.1083\n",
      "Epoch 00014: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1076 - mae: 0.1385 - mse: 0.1076 - val_loss: 0.1080 - val_mae: 0.1367 - val_mse: 0.1080\n",
      "Epoch 15/5000\n",
      "48160/48884 [============================>.] - ETA: 0s - loss: 0.1093 - mae: 0.1400 - mse: 0.1093\n",
      "Epoch 00015: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1087 - mae: 0.1398 - mse: 0.1087 - val_loss: 0.1120 - val_mae: 0.1407 - val_mse: 0.1120\n",
      "Epoch 16/5000\n",
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1064 - mae: 0.1367 - mse: 0.1064\n",
      "Epoch 00016: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1058 - mae: 0.1367 - mse: 0.1058 - val_loss: 0.1082 - val_mae: 0.1376 - val_mse: 0.1082\n",
      "Epoch 17/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.1057 - mae: 0.1367 - mse: 0.1057\n",
      "Epoch 00017: val_loss did not improve from 0.10710\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.1052 - mae: 0.1366 - mse: 0.1052 - val_loss: 0.1110 - val_mae: 0.1368 - val_mse: 0.1110\n",
      "Epoch 18/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.1038 - mae: 0.1350 - mse: 0.1038\n",
      "Epoch 00018: val_loss improved from 0.10710 to 0.10580, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1035 - mae: 0.1349 - mse: 0.1035 - val_loss: 0.1058 - val_mae: 0.1342 - val_mse: 0.1058\n",
      "Epoch 19/5000\n",
      "48128/48884 [============================>.] - ETA: 0s - loss: 0.1026 - mae: 0.1333 - mse: 0.1026\n",
      "Epoch 00019: val_loss did not improve from 0.10580\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1017 - mae: 0.1330 - mse: 0.1017 - val_loss: 0.1060 - val_mae: 0.1330 - val_mse: 0.1060\n",
      "Epoch 20/5000\n",
      "48352/48884 [============================>.] - ETA: 0s - loss: 0.1025 - mae: 0.1338 - mse: 0.1025\n",
      "Epoch 00020: val_loss did not improve from 0.10580\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1020 - mae: 0.1338 - mse: 0.1020 - val_loss: 0.1071 - val_mae: 0.1423 - val_mse: 0.1071\n",
      "Epoch 21/5000\n",
      "48384/48884 [============================>.] - ETA: 0s - loss: 0.1018 - mae: 0.1336 - mse: 0.1018\n",
      "Epoch 00021: val_loss improved from 0.10580 to 0.10549, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.1013 - mae: 0.1335 - mse: 0.1013 - val_loss: 0.1055 - val_mae: 0.1331 - val_mse: 0.1055\n",
      "Epoch 22/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48224/48884 [============================>.] - ETA: 0s - loss: 0.1001 - mae: 0.1318 - mse: 0.1001\n",
      "Epoch 00022: val_loss improved from 0.10549 to 0.10472, saving model to model_checkpoint.h5\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0995 - mae: 0.1317 - mse: 0.0995 - val_loss: 0.1047 - val_mae: 0.1333 - val_mse: 0.1047\n",
      "Epoch 23/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.0996 - mae: 0.1306 - mse: 0.0996\n",
      "Epoch 00023: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0994 - mae: 0.1308 - mse: 0.0994 - val_loss: 0.1103 - val_mae: 0.1474 - val_mse: 0.1103\n",
      "Epoch 24/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0959 - mae: 0.1292 - mse: 0.0959\n",
      "Epoch 00024: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0972 - mae: 0.1291 - mse: 0.0972 - val_loss: 0.1101 - val_mae: 0.1421 - val_mse: 0.1101\n",
      "Epoch 25/5000\n",
      "48448/48884 [============================>.] - ETA: 0s - loss: 0.0978 - mae: 0.1294 - mse: 0.0978\n",
      "Epoch 00025: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0975 - mae: 0.1293 - mse: 0.0975 - val_loss: 0.1067 - val_mae: 0.1353 - val_mse: 0.1067\n",
      "Epoch 26/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0971 - mae: 0.1284 - mse: 0.0971\n",
      "Epoch 00026: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0967 - mae: 0.1284 - mse: 0.0967 - val_loss: 0.1073 - val_mae: 0.1342 - val_mse: 0.1073\n",
      "Epoch 27/5000\n",
      "48608/48884 [============================>.] - ETA: 0s - loss: 0.0961 - mae: 0.1279 - mse: 0.0961\n",
      "Epoch 00027: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0959 - mae: 0.1279 - mse: 0.0959 - val_loss: 0.1094 - val_mae: 0.1368 - val_mse: 0.1094\n",
      "Epoch 28/5000\n",
      "48352/48884 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.1266 - mse: 0.0932\n",
      "Epoch 00028: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0943 - mae: 0.1267 - mse: 0.0943 - val_loss: 0.1101 - val_mae: 0.1405 - val_mse: 0.1101\n",
      "Epoch 29/5000\n",
      "48512/48884 [============================>.] - ETA: 0s - loss: 0.0891 - mae: 0.1258 - mse: 0.0891\n",
      "Epoch 00029: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0941 - mae: 0.1262 - mse: 0.0941 - val_loss: 0.1117 - val_mae: 0.1515 - val_mse: 0.1117\n",
      "Epoch 30/5000\n",
      "48416/48884 [============================>.] - ETA: 0s - loss: 0.0939 - mae: 0.1268 - mse: 0.0939\n",
      "Epoch 00030: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0935 - mae: 0.1267 - mse: 0.0935 - val_loss: 0.1076 - val_mae: 0.1338 - val_mse: 0.1076\n",
      "Epoch 31/5000\n",
      "48480/48884 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.1254 - mse: 0.0932\n",
      "Epoch 00031: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 37us/sample - loss: 0.0927 - mae: 0.1252 - mse: 0.0927 - val_loss: 0.1068 - val_mae: 0.1314 - val_mse: 0.1068\n",
      "Epoch 32/5000\n",
      "48544/48884 [============================>.] - ETA: 0s - loss: 0.0923 - mae: 0.1245 - mse: 0.0923\n",
      "Epoch 00032: val_loss did not improve from 0.10472\n",
      "48884/48884 [==============================] - 2s 36us/sample - loss: 0.0920 - mae: 0.1245 - mse: 0.0920 - val_loss: 0.1204 - val_mae: 0.1448 - val_mse: 0.1204\n",
      "15276/15276 - 0s - loss: 0.0947 - mae: 0.1430 - mse: 0.0947\n"
     ]
    }
   ],
   "source": [
    "mse = evaluate_model(ml_data, total_frac, start_str, end_str, patience, training_pct, n_layer, n_unit, activation, \n",
    "                   loss, opt, val_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09473248"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frac = .1\n",
    "defaults = {\"patience\":10, \"training_pct\":.8, \"n_layer\":7, \"n_unit\":20, \"activation\":'relu', \"loss\":'mean_absolute_error', \n",
    "            \"opt\":'rmsprop', \"val_pct\":.2}\n",
    "all_grid = {\"patience\":[10], \"training_pct\":eq_space(.5, .8, 5), \n",
    "             \"n_layer\":eq_space(3, 20, 5, True), \"n_unit\":eq_space(20, 1000, 5, True), \"activation\":['relu', 'tanh', 'sigmoid'],\n",
    "             \"loss\":['huber_loss', 'mse', 'mean_absolute_error', 'logcosh'], \n",
    "            \"opt\":['sgd', 'rmsprop', 'adamax', 'adam', 'adagrad'], \"val_pct\":eq_space(.2, .5, 5)}\n",
    "\n",
    "\n",
    "init_grid = {\"val_pct\":eq_space(.2, .5, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.2\n",
      "385\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_318 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4888 samples, validate on 1222 samples\n",
      "Epoch 1/2000\n",
      "4672/4888 [===========================>..] - ETA: 0s - loss: 0.3754 - mae: 0.3754 - mse: 0.6080\n",
      "Epoch 00001: val_loss improved from inf to 0.31702, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.3715,  mae:0.3715,  mse:0.5963,  val_loss:0.3170,  val_mae:0.3170,  val_mse:0.4920,  \n",
      "4888/4888 [==============================] - 1s 152us/sample - loss: 0.3715 - mae: 0.3715 - mse: 0.5963 - val_loss: 0.3170 - val_mae: 0.3170 - val_mse: 0.4920\n",
      "Epoch 2/2000\n",
      "3488/4888 [====================>.........] - ETA: 0s - loss: 0.2824 - mae: 0.2824 - mse: 0.3752\n",
      "Epoch 00002: val_loss improved from 0.31702 to 0.25170, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 53us/sample - loss: 0.2725 - mae: 0.2725 - mse: 0.3564 - val_loss: 0.2517 - val_mae: 0.2517 - val_mse: 0.3266\n",
      "Epoch 3/2000\n",
      "4704/4888 [===========================>..] - ETA: 0s - loss: 0.2354 - mae: 0.2354 - mse: 0.2533\n",
      "Epoch 00003: val_loss improved from 0.25170 to 0.23193, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 57us/sample - loss: 0.2363 - mae: 0.2363 - mse: 0.2729 - val_loss: 0.2319 - val_mae: 0.2319 - val_mse: 0.2720\n",
      "Epoch 4/2000\n",
      "3616/4888 [=====================>........] - ETA: 0s - loss: 0.2199 - mae: 0.2199 - mse: 0.2613\n",
      "Epoch 00004: val_loss improved from 0.23193 to 0.22040, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 52us/sample - loss: 0.2151 - mae: 0.2151 - mse: 0.2308 - val_loss: 0.2204 - val_mae: 0.2204 - val_mse: 0.2685\n",
      "Epoch 5/2000\n",
      "4768/4888 [============================>.] - ETA: 0s - loss: 0.2039 - mae: 0.2039 - mse: 0.2161\n",
      "Epoch 00005: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.2036 - mae: 0.2036 - mse: 0.2144 - val_loss: 0.2269 - val_mae: 0.2269 - val_mse: 0.2931\n",
      "Epoch 6/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1925 - mae: 0.1925 - mse: 0.1681\n",
      "Epoch 00006: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1991 - mae: 0.1991 - mse: 0.2090 - val_loss: 0.2592 - val_mae: 0.2592 - val_mse: 0.2942\n",
      "Epoch 7/2000\n",
      "3776/4888 [======================>.......] - ETA: 0s - loss: 0.1893 - mae: 0.1893 - mse: 0.2150\n",
      "Epoch 00007: val_loss did not improve from 0.22040\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1907 - mae: 0.1907 - mse: 0.2011 - val_loss: 0.2314 - val_mae: 0.2314 - val_mse: 0.2927\n",
      "Epoch 8/2000\n",
      "3712/4888 [=====================>........] - ETA: 0s - loss: 0.1879 - mae: 0.1879 - mse: 0.1919\n",
      "Epoch 00008: val_loss improved from 0.22040 to 0.19471, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1859 - mae: 0.1859 - mse: 0.1947 - val_loss: 0.1947 - val_mae: 0.1947 - val_mse: 0.2358\n",
      "Epoch 9/2000\n",
      "3616/4888 [=====================>........] - ETA: 0s - loss: 0.1833 - mae: 0.1833 - mse: 0.1807\n",
      "Epoch 00009: val_loss did not improve from 0.19471\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1838 - mae: 0.1838 - mse: 0.1894 - val_loss: 0.2062 - val_mae: 0.2062 - val_mse: 0.2489\n",
      "Epoch 10/2000\n",
      "3552/4888 [====================>.........] - ETA: 0s - loss: 0.1775 - mae: 0.1775 - mse: 0.2074\n",
      "Epoch 00010: val_loss did not improve from 0.19471\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1803 - mae: 0.1803 - mse: 0.1868 - val_loss: 0.1954 - val_mae: 0.1954 - val_mse: 0.2356\n",
      "Epoch 11/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1803 - mae: 0.1803 - mse: 0.1748\n",
      "Epoch 00011: val_loss improved from 0.19471 to 0.19371, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 49us/sample - loss: 0.1800 - mae: 0.1800 - mse: 0.1864 - val_loss: 0.1937 - val_mae: 0.1937 - val_mse: 0.2288\n",
      "Epoch 12/2000\n",
      "3744/4888 [=====================>........] - ETA: 0s - loss: 0.1765 - mae: 0.1765 - mse: 0.1700\n",
      "Epoch 00012: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1744 - mae: 0.1744 - mse: 0.1787 - val_loss: 0.2072 - val_mae: 0.2072 - val_mse: 0.2398\n",
      "Epoch 13/2000\n",
      "3776/4888 [======================>.......] - ETA: 0s - loss: 0.1775 - mae: 0.1775 - mse: 0.1775\n",
      "Epoch 00013: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1735 - mae: 0.1735 - mse: 0.1812 - val_loss: 0.1965 - val_mae: 0.1965 - val_mse: 0.2347\n",
      "Epoch 14/2000\n",
      "3808/4888 [======================>.......] - ETA: 0s - loss: 0.1688 - mae: 0.1688 - mse: 0.1882\n",
      "Epoch 00014: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1702 - mae: 0.1702 - mse: 0.1746 - val_loss: 0.2289 - val_mae: 0.2289 - val_mse: 0.2735\n",
      "Epoch 15/2000\n",
      "4864/4888 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.1685 - mse: 0.1738\n",
      "Epoch 00015: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 50us/sample - loss: 0.1680 - mae: 0.1680 - mse: 0.1731 - val_loss: 0.1949 - val_mae: 0.1949 - val_mse: 0.2310\n",
      "Epoch 16/2000\n",
      "4096/4888 [========================>.....] - ETA: 0s - loss: 0.1697 - mae: 0.1697 - mse: 0.1609\n",
      "Epoch 00016: val_loss did not improve from 0.19371\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1678 - mae: 0.1678 - mse: 0.1728 - val_loss: 0.2035 - val_mae: 0.2035 - val_mse: 0.2447\n",
      "Epoch 17/2000\n",
      "3552/4888 [====================>.........] - ETA: 0s - loss: 0.1642 - mae: 0.1642 - mse: 0.1888\n",
      "Epoch 00017: val_loss improved from 0.19371 to 0.18904, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1655 - mae: 0.1655 - mse: 0.1696 - val_loss: 0.1890 - val_mae: 0.1890 - val_mse: 0.2237\n",
      "Epoch 18/2000\n",
      "4864/4888 [============================>.] - ETA: 0s - loss: 0.1654 - mae: 0.1654 - mse: 0.1706\n",
      "Epoch 00018: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 51us/sample - loss: 0.1650 - mae: 0.1650 - mse: 0.1699 - val_loss: 0.2417 - val_mae: 0.2417 - val_mse: 0.2997\n",
      "Epoch 19/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1563 - mae: 0.1563 - mse: 0.1228\n",
      "Epoch 00019: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 47us/sample - loss: 0.1616 - mae: 0.1616 - mse: 0.1660 - val_loss: 0.1988 - val_mae: 0.1988 - val_mse: 0.2276\n",
      "Epoch 20/2000\n",
      "3520/4888 [====================>.........] - ETA: 0s - loss: 0.1645 - mae: 0.1645 - mse: 0.1907\n",
      "Epoch 00020: val_loss did not improve from 0.18904\n",
      "4888/4888 [==============================] - 0s 48us/sample - loss: 0.1604 - mae: 0.1604 - mse: 0.1638 - val_loss: 0.1998 - val_mae: 0.1998 - val_mse: 0.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/2000\n",
      "4416/4888 [==========================>...] - ETA: 0s - loss: 0.1564 - mae: 0.1564 - mse: 0.1463\n",
      "Epoch 00021: val_loss improved from 0.18904 to 0.18687, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1666 - val_loss: 0.1869 - val_mae: 0.1869 - val_mse: 0.2211\n",
      "Epoch 22/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1544 - mae: 0.1544 - mse: 0.1489\n",
      "Epoch 00022: val_loss did not improve from 0.18687\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1562 - mae: 0.1562 - mse: 0.1604 - val_loss: 0.1928 - val_mae: 0.1928 - val_mse: 0.2369\n",
      "Epoch 23/2000\n",
      "4224/4888 [========================>.....] - ETA: 0s - loss: 0.1560 - mae: 0.1560 - mse: 0.1701\n",
      "Epoch 00023: val_loss did not improve from 0.18687\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1541 - mae: 0.1541 - mse: 0.1568 - val_loss: 0.1910 - val_mae: 0.1910 - val_mse: 0.2350\n",
      "Epoch 24/2000\n",
      "4448/4888 [==========================>...] - ETA: 0s - loss: 0.1541 - mae: 0.1541 - mse: 0.1656\n",
      "Epoch 00024: val_loss improved from 0.18687 to 0.18336, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1550 - mae: 0.1550 - mse: 0.1602 - val_loss: 0.1834 - val_mae: 0.1834 - val_mse: 0.2262\n",
      "Epoch 25/2000\n",
      "3744/4888 [=====================>........] - ETA: 0s - loss: 0.1524 - mae: 0.1524 - mse: 0.1471\n",
      "Epoch 00025: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1540 - mae: 0.1540 - mse: 0.1596 - val_loss: 0.2260 - val_mae: 0.2260 - val_mse: 0.2687\n",
      "Epoch 26/2000\n",
      "4128/4888 [========================>.....] - ETA: 0s - loss: 0.1510 - mae: 0.1510 - mse: 0.1418\n",
      "Epoch 00026: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1543 - mae: 0.1543 - mse: 0.1599 - val_loss: 0.2104 - val_mae: 0.2104 - val_mse: 0.2458\n",
      "Epoch 27/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1531 - mae: 0.1531 - mse: 0.1650\n",
      "Epoch 00027: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1514 - mae: 0.1514 - mse: 0.1550 - val_loss: 0.2011 - val_mae: 0.2011 - val_mse: 0.2425\n",
      "Epoch 28/2000\n",
      "4480/4888 [==========================>...] - ETA: 0s - loss: 0.1504 - mae: 0.1504 - mse: 0.1582\n",
      "Epoch 00028: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1495 - mae: 0.1495 - mse: 0.1524 - val_loss: 0.1899 - val_mae: 0.1899 - val_mse: 0.2270\n",
      "Epoch 29/2000\n",
      "4160/4888 [========================>.....] - ETA: 0s - loss: 0.1478 - mae: 0.1478 - mse: 0.1620\n",
      "Epoch 00029: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1507 - mae: 0.1507 - mse: 0.1556 - val_loss: 0.1998 - val_mae: 0.1998 - val_mse: 0.2426\n",
      "Epoch 30/2000\n",
      "4480/4888 [==========================>...] - ETA: 0s - loss: 0.1493 - mae: 0.1493 - mse: 0.1585\n",
      "Epoch 00030: val_loss did not improve from 0.18336\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1491 - mae: 0.1491 - mse: 0.1527 - val_loss: 0.1930 - val_mae: 0.1930 - val_mse: 0.2249\n",
      "Epoch 31/2000\n",
      "4192/4888 [========================>.....] - ETA: 0s - loss: 0.1489 - mae: 0.1489 - mse: 0.1610\n",
      "Epoch 00031: val_loss improved from 0.18336 to 0.18114, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1474 - mae: 0.1474 - mse: 0.1501 - val_loss: 0.1811 - val_mae: 0.1811 - val_mse: 0.2184\n",
      "Epoch 32/2000\n",
      "3904/4888 [======================>.......] - ETA: 0s - loss: 0.1482 - mae: 0.1482 - mse: 0.1718\n",
      "Epoch 00032: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1467 - mae: 0.1467 - mse: 0.1520 - val_loss: 0.1818 - val_mae: 0.1818 - val_mse: 0.2133\n",
      "Epoch 33/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1477 - mae: 0.1477 - mse: 0.1586\n",
      "Epoch 00033: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1468 - mae: 0.1468 - mse: 0.1509 - val_loss: 0.2196 - val_mae: 0.2196 - val_mse: 0.2616\n",
      "Epoch 34/2000\n",
      "4288/4888 [=========================>....] - ETA: 0s - loss: 0.1448 - mae: 0.1448 - mse: 0.1575\n",
      "Epoch 00034: val_loss did not improve from 0.18114\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1441 - mae: 0.1441 - mse: 0.1489 - val_loss: 0.1868 - val_mae: 0.1868 - val_mse: 0.2229\n",
      "Epoch 35/2000\n",
      "4512/4888 [==========================>...] - ETA: 0s - loss: 0.1452 - mae: 0.1452 - mse: 0.1544\n",
      "Epoch 00035: val_loss improved from 0.18114 to 0.17427, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1434 - mae: 0.1434 - mse: 0.1474 - val_loss: 0.1743 - val_mae: 0.1743 - val_mse: 0.2114\n",
      "Epoch 36/2000\n",
      "3872/4888 [======================>.......] - ETA: 0s - loss: 0.1452 - mae: 0.1452 - mse: 0.1649\n",
      "Epoch 00036: val_loss did not improve from 0.17427\n",
      "4888/4888 [==============================] - 0s 45us/sample - loss: 0.1444 - mae: 0.1444 - mse: 0.1500 - val_loss: 0.1852 - val_mae: 0.1852 - val_mse: 0.2253\n",
      "Epoch 37/2000\n",
      "4256/4888 [=========================>....] - ETA: 0s - loss: 0.1393 - mae: 0.1393 - mse: 0.1294\n",
      "Epoch 00037: val_loss improved from 0.17427 to 0.16954, saving model to model_checkpoint.h5\n",
      "4888/4888 [==============================] - 0s 46us/sample - loss: 0.1405 - mae: 0.1405 - mse: 0.1442 - val_loss: 0.1695 - val_mae: 0.1695 - val_mse: 0.2086\n",
      "Epoch 38/2000\n",
      "4032/4888 [=======================>......] - ETA: 0s - loss: 0.1443 - mae: 0.1443 - mse: 0.1607\n",
      "Epoch 00038: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1427 - mae: 0.1427 - mse: 0.1452 - val_loss: 0.1786 - val_mae: 0.1786 - val_mse: 0.2139\n",
      "Epoch 39/2000\n",
      "4192/4888 [========================>.....] - ETA: 0s - loss: 0.1435 - mae: 0.1435 - mse: 0.1591\n",
      "Epoch 00039: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 43us/sample - loss: 0.1422 - mae: 0.1422 - mse: 0.1474 - val_loss: 0.1895 - val_mae: 0.1895 - val_mse: 0.2216\n",
      "Epoch 40/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1384 - mae: 0.1384 - mse: 0.1275\n",
      "Epoch 00040: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1409 - mae: 0.1409 - mse: 0.1451 - val_loss: 0.1764 - val_mae: 0.1764 - val_mse: 0.2085\n",
      "Epoch 41/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1391 - mae: 0.1391 - mse: 0.1265\n",
      "Epoch 00041: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1398 - mae: 0.1398 - mse: 0.1426 - val_loss: 0.1698 - val_mae: 0.1698 - val_mse: 0.2068\n",
      "Epoch 42/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1398 - mae: 0.1398 - mse: 0.1519\n",
      "Epoch 00042: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1386 - mae: 0.1386 - mse: 0.1425 - val_loss: 0.1731 - val_mae: 0.1731 - val_mse: 0.2070\n",
      "Epoch 43/2000\n",
      "4064/4888 [=======================>......] - ETA: 0s - loss: 0.1315 - mae: 0.1315 - mse: 0.1227\n",
      "Epoch 00043: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 44us/sample - loss: 0.1364 - mae: 0.1364 - mse: 0.1405 - val_loss: 0.1778 - val_mae: 0.1778 - val_mse: 0.2128\n",
      "Epoch 44/2000\n",
      "4320/4888 [=========================>....] - ETA: 0s - loss: 0.1380 - mae: 0.1380 - mse: 0.1498\n",
      "Epoch 00044: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1385 - mae: 0.1385 - mse: 0.1444 - val_loss: 0.1915 - val_mae: 0.1915 - val_mse: 0.2318\n",
      "Epoch 45/2000\n",
      "4288/4888 [=========================>....] - ETA: 0s - loss: 0.1390 - mae: 0.1390 - mse: 0.1497\n",
      "Epoch 00045: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 42us/sample - loss: 0.1378 - mae: 0.1378 - mse: 0.1410 - val_loss: 0.1778 - val_mae: 0.1778 - val_mse: 0.2049\n",
      "Epoch 46/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/4888 [==========================>...] - ETA: 0s - loss: 0.1374 - mae: 0.1374 - mse: 0.1448\n",
      "Epoch 00046: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 41us/sample - loss: 0.1364 - mae: 0.1364 - mse: 0.1380 - val_loss: 0.1732 - val_mae: 0.1732 - val_mse: 0.2062\n",
      "Epoch 47/2000\n",
      "4544/4888 [==========================>...] - ETA: 0s - loss: 0.1384 - mae: 0.1384 - mse: 0.1468\n",
      "Epoch 00047: val_loss did not improve from 0.16954\n",
      "4888/4888 [==============================] - 0s 41us/sample - loss: 0.1360 - mae: 0.1360 - mse: 0.1394 - val_loss: 0.1772 - val_mae: 0.1772 - val_mse: 0.2112\n",
      "1528/1528 - 0s - loss: 0.1615 - mae: 0.1615 - mse: 0.1068\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.275\n",
      "385\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_326 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4429 samples, validate on 1681 samples\n",
      "Epoch 1/2000\n",
      "4416/4429 [============================>.] - ETA: 0s - loss: 0.4281 - mae: 0.4281 - mse: 0.8085\n",
      "Epoch 00001: val_loss improved from inf to 0.35309, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4272,  mae:0.4272,  mse:0.8063,  val_loss:0.3531,  val_mae:0.3531,  val_mse:0.6535,  \n",
      "4429/4429 [==============================] - 1s 159us/sample - loss: 0.4272 - mae: 0.4272 - mse: 0.8063 - val_loss: 0.3531 - val_mae: 0.3531 - val_mse: 0.6535\n",
      "Epoch 2/2000\n",
      "3936/4429 [=========================>....] - ETA: 0s - loss: 0.2987 - mae: 0.2987 - mse: 0.4605\n",
      "Epoch 00002: val_loss improved from 0.35309 to 0.24095, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 52us/sample - loss: 0.2904 - mae: 0.2904 - mse: 0.4287 - val_loss: 0.2409 - val_mae: 0.2409 - val_mse: 0.3043\n",
      "Epoch 3/2000\n",
      "4288/4429 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.2218 - mse: 0.2073\n",
      "Epoch 00003: val_loss improved from 0.24095 to 0.21329, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 50us/sample - loss: 0.2276 - mae: 0.2276 - mse: 0.2580 - val_loss: 0.2133 - val_mae: 0.2133 - val_mse: 0.2370\n",
      "Epoch 4/2000\n",
      "3648/4429 [=======================>......] - ETA: 0s - loss: 0.2095 - mae: 0.2095 - mse: 0.2373\n",
      "Epoch 00004: val_loss did not improve from 0.21329\n",
      "4429/4429 [==============================] - 0s 51us/sample - loss: 0.2084 - mae: 0.2084 - mse: 0.2223 - val_loss: 0.2465 - val_mae: 0.2465 - val_mse: 0.2743\n",
      "Epoch 5/2000\n",
      "4224/4429 [===========================>..] - ETA: 0s - loss: 0.1996 - mae: 0.1996 - mse: 0.2124\n",
      "Epoch 00005: val_loss did not improve from 0.21329\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.2001 - mae: 0.2001 - mse: 0.2090 - val_loss: 0.2324 - val_mae: 0.2324 - val_mse: 0.2673\n",
      "Epoch 6/2000\n",
      "4128/4429 [==========================>...] - ETA: 0s - loss: 0.1981 - mae: 0.1981 - mse: 0.2157\n",
      "Epoch 00006: val_loss improved from 0.21329 to 0.19794, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 51us/sample - loss: 0.1968 - mae: 0.1968 - mse: 0.2093 - val_loss: 0.1979 - val_mae: 0.1979 - val_mse: 0.2190\n",
      "Epoch 7/2000\n",
      "3776/4429 [========================>.....] - ETA: 0s - loss: 0.1965 - mae: 0.1965 - mse: 0.2242\n",
      "Epoch 00007: val_loss improved from 0.19794 to 0.19042, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 53us/sample - loss: 0.1945 - mae: 0.1945 - mse: 0.2091 - val_loss: 0.1904 - val_mae: 0.1904 - val_mse: 0.2118\n",
      "Epoch 8/2000\n",
      "4192/4429 [===========================>..] - ETA: 0s - loss: 0.1887 - mae: 0.1887 - mse: 0.2035\n",
      "Epoch 00008: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1908 - mae: 0.1908 - mse: 0.2016 - val_loss: 0.1963 - val_mae: 0.1963 - val_mse: 0.2125\n",
      "Epoch 9/2000\n",
      "4128/4429 [==========================>...] - ETA: 0s - loss: 0.1891 - mae: 0.1891 - mse: 0.2062\n",
      "Epoch 00009: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1860 - mae: 0.1860 - mse: 0.1974 - val_loss: 0.2073 - val_mae: 0.2073 - val_mse: 0.2504\n",
      "Epoch 10/2000\n",
      "4352/4429 [============================>.] - ETA: 0s - loss: 0.1823 - mae: 0.1823 - mse: 0.1959\n",
      "Epoch 00010: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1825 - mae: 0.1825 - mse: 0.1944 - val_loss: 0.1982 - val_mae: 0.1982 - val_mse: 0.2114\n",
      "Epoch 11/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1813 - mae: 0.1813 - mse: 0.1930\n",
      "Epoch 00011: val_loss did not improve from 0.19042\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1824 - mae: 0.1824 - mse: 0.1938 - val_loss: 0.2079 - val_mae: 0.2079 - val_mse: 0.2147\n",
      "Epoch 12/2000\n",
      "2976/4429 [===================>..........] - ETA: 0s - loss: 0.1814 - mae: 0.1814 - mse: 0.2013\n",
      "Epoch 00012: val_loss improved from 0.19042 to 0.17982, saving model to model_checkpoint.h5\n",
      "4429/4429 [==============================] - 0s 49us/sample - loss: 0.1764 - mae: 0.1764 - mse: 0.1888 - val_loss: 0.1798 - val_mae: 0.1798 - val_mse: 0.1900\n",
      "Epoch 13/2000\n",
      "3904/4429 [=========================>....] - ETA: 0s - loss: 0.1776 - mae: 0.1776 - mse: 0.1927\n",
      "Epoch 00013: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 48us/sample - loss: 0.1772 - mae: 0.1772 - mse: 0.1850 - val_loss: 0.1915 - val_mae: 0.1915 - val_mse: 0.2217\n",
      "Epoch 14/2000\n",
      "4032/4429 [==========================>...] - ETA: 0s - loss: 0.1750 - mae: 0.1750 - mse: 0.1691\n",
      "Epoch 00014: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 48us/sample - loss: 0.1745 - mae: 0.1745 - mse: 0.1853 - val_loss: 0.2057 - val_mae: 0.2057 - val_mse: 0.2210\n",
      "Epoch 15/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.1714 - mse: 0.1862\n",
      "Epoch 00015: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1711 - mae: 0.1711 - mse: 0.1841 - val_loss: 0.1921 - val_mae: 0.1921 - val_mse: 0.2105\n",
      "Epoch 16/2000\n",
      "4352/4429 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.1676 - mse: 0.1780\n",
      "Epoch 00016: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 45us/sample - loss: 0.1685 - mae: 0.1685 - mse: 0.1792 - val_loss: 0.2682 - val_mae: 0.2682 - val_mse: 0.3046\n",
      "Epoch 17/2000\n",
      "4000/4429 [==========================>...] - ETA: 0s - loss: 0.1692 - mae: 0.1692 - mse: 0.1908\n",
      "Epoch 00017: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 47us/sample - loss: 0.1686 - mae: 0.1686 - mse: 0.1820 - val_loss: 0.1847 - val_mae: 0.1847 - val_mse: 0.1992\n",
      "Epoch 18/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1674 - mae: 0.1674 - mse: 0.1799\n",
      "Epoch 00018: val_loss did not improve from 0.17982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1665 - mae: 0.1665 - mse: 0.1767 - val_loss: 0.1888 - val_mae: 0.1888 - val_mse: 0.2113\n",
      "Epoch 19/2000\n",
      "3392/4429 [=====================>........] - ETA: 0s - loss: 0.1657 - mae: 0.1657 - mse: 0.1410\n",
      "Epoch 00019: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 55us/sample - loss: 0.1681 - mae: 0.1681 - mse: 0.1812 - val_loss: 0.1996 - val_mae: 0.1996 - val_mse: 0.2086\n",
      "Epoch 20/2000\n",
      "4064/4429 [==========================>...] - ETA: 0s - loss: 0.1652 - mae: 0.1652 - mse: 0.1792\n",
      "Epoch 00020: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 59us/sample - loss: 0.1655 - mae: 0.1655 - mse: 0.1724 - val_loss: 0.2297 - val_mae: 0.2297 - val_mse: 0.2568\n",
      "Epoch 21/2000\n",
      "4320/4429 [============================>.] - ETA: 0s - loss: 0.1631 - mae: 0.1631 - mse: 0.1775\n",
      "Epoch 00021: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 46us/sample - loss: 0.1632 - mae: 0.1632 - mse: 0.1766 - val_loss: 0.1812 - val_mae: 0.1812 - val_mse: 0.1902\n",
      "Epoch 22/2000\n",
      "4160/4429 [===========================>..] - ETA: 0s - loss: 0.1629 - mae: 0.1629 - mse: 0.1805\n",
      "Epoch 00022: val_loss did not improve from 0.17982\n",
      "4429/4429 [==============================] - 0s 50us/sample - loss: 0.1624 - mae: 0.1624 - mse: 0.1758 - val_loss: 0.1893 - val_mae: 0.1893 - val_mse: 0.2136\n",
      "1528/1528 - 0s - loss: 0.1855 - mae: 0.1855 - mse: 0.1430\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.35\n",
      "385\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_334 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3971 samples, validate on 2139 samples\n",
      "Epoch 1/2000\n",
      "3552/3971 [=========================>....] - ETA: 0s - loss: 0.4320 - mae: 0.4320 - mse: 0.7491\n",
      "Epoch 00001: val_loss improved from inf to 0.33272, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4160,  mae:0.4160,  mse:0.7015,  val_loss:0.3327,  val_mae:0.3327,  val_mse:0.4836,  \n",
      "3971/3971 [==============================] - 1s 250us/sample - loss: 0.4160 - mae: 0.4160 - mse: 0.7015 - val_loss: 0.3327 - val_mae: 0.3327 - val_mse: 0.4836\n",
      "Epoch 2/2000\n",
      "2496/3971 [=================>............] - ETA: 0s - loss: 0.2879 - mae: 0.2879 - mse: 0.4316\n",
      "Epoch 00002: val_loss improved from 0.33272 to 0.26711, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.2670 - mae: 0.2670 - mse: 0.3439 - val_loss: 0.2671 - val_mae: 0.2671 - val_mse: 0.3334\n",
      "Epoch 3/2000\n",
      "3744/3971 [===========================>..] - ETA: 0s - loss: 0.2224 - mae: 0.2224 - mse: 0.2618\n",
      "Epoch 00003: val_loss improved from 0.26711 to 0.25610, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 59us/sample - loss: 0.2235 - mae: 0.2235 - mse: 0.2598 - val_loss: 0.2561 - val_mae: 0.2561 - val_mse: 0.2599\n",
      "Epoch 4/2000\n",
      "3968/3971 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.2038 - mse: 0.2295\n",
      "Epoch 00004: val_loss improved from 0.25610 to 0.23298, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.2038 - mae: 0.2038 - mse: 0.2293 - val_loss: 0.2330 - val_mae: 0.2330 - val_mse: 0.2296\n",
      "Epoch 5/2000\n",
      "2400/3971 [=================>............] - ETA: 0s - loss: 0.1919 - mae: 0.1919 - mse: 0.2660\n",
      "Epoch 00005: val_loss did not improve from 0.23298\n",
      "3971/3971 [==============================] - 0s 53us/sample - loss: 0.1906 - mae: 0.1906 - mse: 0.2129 - val_loss: 0.2430 - val_mae: 0.2430 - val_mse: 0.3033\n",
      "Epoch 6/2000\n",
      "2624/3971 [==================>...........] - ETA: 0s - loss: 0.1907 - mae: 0.1907 - mse: 0.2487\n",
      "Epoch 00006: val_loss improved from 0.23298 to 0.21105, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1880 - mae: 0.1880 - mse: 0.2109 - val_loss: 0.2111 - val_mae: 0.2111 - val_mse: 0.2259\n",
      "Epoch 7/2000\n",
      "2752/3971 [===================>..........] - ETA: 0s - loss: 0.1867 - mae: 0.1867 - mse: 0.2052\n",
      "Epoch 00007: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1815 - mae: 0.1815 - mse: 0.1989 - val_loss: 0.2206 - val_mae: 0.2206 - val_mse: 0.2155\n",
      "Epoch 8/2000\n",
      "2368/3971 [================>.............] - ETA: 0s - loss: 0.1809 - mae: 0.1809 - mse: 0.2146\n",
      "Epoch 00008: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1785 - mae: 0.1785 - mse: 0.1976 - val_loss: 0.2249 - val_mae: 0.2249 - val_mse: 0.2221\n",
      "Epoch 9/2000\n",
      "3712/3971 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.1737 - mse: 0.1926\n",
      "Epoch 00009: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1752 - mae: 0.1752 - mse: 0.1902 - val_loss: 0.2682 - val_mae: 0.2682 - val_mse: 0.2631\n",
      "Epoch 10/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1717 - mae: 0.1717 - mse: 0.1859\n",
      "Epoch 00010: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1705 - mae: 0.1705 - mse: 0.1906 - val_loss: 0.2582 - val_mae: 0.2582 - val_mse: 0.2842\n",
      "Epoch 11/2000\n",
      "3936/3971 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.1714 - mse: 0.1907\n",
      "Epoch 00011: val_loss did not improve from 0.21105\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1714 - mae: 0.1714 - mse: 0.1899 - val_loss: 0.2610 - val_mae: 0.2610 - val_mse: 0.2255\n",
      "Epoch 12/2000\n",
      "2880/3971 [====================>.........] - ETA: 0s - loss: 0.1671 - mae: 0.1671 - mse: 0.1764\n",
      "Epoch 00012: val_loss improved from 0.21105 to 0.20880, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1694 - mae: 0.1694 - mse: 0.1837 - val_loss: 0.2088 - val_mae: 0.2088 - val_mse: 0.2155\n",
      "Epoch 13/2000\n",
      "2464/3971 [=================>............] - ETA: 0s - loss: 0.1660 - mae: 0.1660 - mse: 0.1870\n",
      "Epoch 00013: val_loss improved from 0.20880 to 0.19382, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1683 - mae: 0.1683 - mse: 0.1838 - val_loss: 0.1938 - val_mae: 0.1938 - val_mse: 0.1975\n",
      "Epoch 14/2000\n",
      "3776/3971 [===========================>..] - ETA: 0s - loss: 0.1626 - mae: 0.1626 - mse: 0.1800\n",
      "Epoch 00014: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1624 - mae: 0.1624 - mse: 0.1754 - val_loss: 0.2003 - val_mae: 0.2003 - val_mse: 0.1999\n",
      "Epoch 15/2000\n",
      "3840/3971 [============================>.] - ETA: 0s - loss: 0.1627 - mae: 0.1627 - mse: 0.1794\n",
      "Epoch 00015: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 53us/sample - loss: 0.1616 - mae: 0.1616 - mse: 0.1752 - val_loss: 0.2115 - val_mae: 0.2115 - val_mse: 0.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/2000\n",
      "2624/3971 [==================>...........] - ETA: 0s - loss: 0.1592 - mae: 0.1592 - mse: 0.1330\n",
      "Epoch 00016: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1630 - mae: 0.1630 - mse: 0.1786 - val_loss: 0.2224 - val_mae: 0.2224 - val_mse: 0.2115\n",
      "Epoch 17/2000\n",
      "2336/3971 [================>.............] - ETA: 0s - loss: 0.1524 - mae: 0.1524 - mse: 0.2161\n",
      "Epoch 00017: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 52us/sample - loss: 0.1586 - mae: 0.1586 - mse: 0.1732 - val_loss: 0.2274 - val_mae: 0.2274 - val_mse: 0.2038\n",
      "Epoch 18/2000\n",
      "3840/3971 [============================>.] - ETA: 0s - loss: 0.1576 - mae: 0.1576 - mse: 0.1752\n",
      "Epoch 00018: val_loss did not improve from 0.19382\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1739 - val_loss: 0.3448 - val_mae: 0.3448 - val_mse: 0.3433\n",
      "Epoch 19/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1599 - mae: 0.1599 - mse: 0.1707\n",
      "Epoch 00019: val_loss improved from 0.19382 to 0.19274, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1588 - mae: 0.1588 - mse: 0.1752 - val_loss: 0.1927 - val_mae: 0.1927 - val_mse: 0.2017\n",
      "Epoch 20/2000\n",
      "3904/3971 [============================>.] - ETA: 0s - loss: 0.1506 - mae: 0.1506 - mse: 0.1402\n",
      "Epoch 00020: val_loss improved from 0.19274 to 0.18348, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 57us/sample - loss: 0.1529 - mae: 0.1529 - mse: 0.1685 - val_loss: 0.1835 - val_mae: 0.1835 - val_mse: 0.1827\n",
      "Epoch 21/2000\n",
      "3648/3971 [==========================>...] - ETA: 0s - loss: 0.1525 - mae: 0.1525 - mse: 0.1484\n",
      "Epoch 00021: val_loss did not improve from 0.18348\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1541 - mae: 0.1541 - mse: 0.1719 - val_loss: 0.2174 - val_mae: 0.2174 - val_mse: 0.2038\n",
      "Epoch 22/2000\n",
      "2560/3971 [==================>...........] - ETA: 0s - loss: 0.1500 - mae: 0.1500 - mse: 0.2123\n",
      "Epoch 00022: val_loss improved from 0.18348 to 0.18239, saving model to model_checkpoint.h5\n",
      "3971/3971 [==============================] - 0s 56us/sample - loss: 0.1530 - mae: 0.1530 - mse: 0.1722 - val_loss: 0.1824 - val_mae: 0.1824 - val_mse: 0.1874\n",
      "Epoch 23/2000\n",
      "3776/3971 [===========================>..] - ETA: 0s - loss: 0.1504 - mae: 0.1504 - mse: 0.1446\n",
      "Epoch 00023: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 54us/sample - loss: 0.1517 - mae: 0.1517 - mse: 0.1685 - val_loss: 0.2322 - val_mae: 0.2322 - val_mse: 0.2635\n",
      "Epoch 24/2000\n",
      "2496/3971 [=================>............] - ETA: 0s - loss: 0.1572 - mae: 0.1572 - mse: 0.2194\n",
      "Epoch 00024: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1532 - mae: 0.1532 - mse: 0.1689 - val_loss: 0.2030 - val_mae: 0.2030 - val_mse: 0.1952\n",
      "Epoch 25/2000\n",
      "2688/3971 [===================>..........] - ETA: 0s - loss: 0.1469 - mae: 0.1469 - mse: 0.1577\n",
      "Epoch 00025: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1489 - mae: 0.1489 - mse: 0.1643 - val_loss: 0.2167 - val_mae: 0.2167 - val_mse: 0.2544\n",
      "Epoch 26/2000\n",
      "2592/3971 [==================>...........] - ETA: 0s - loss: 0.1457 - mae: 0.1457 - mse: 0.1275\n",
      "Epoch 00026: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1493 - mae: 0.1493 - mse: 0.1662 - val_loss: 0.1866 - val_mae: 0.1866 - val_mse: 0.1955\n",
      "Epoch 27/2000\n",
      "2592/3971 [==================>...........] - ETA: 0s - loss: 0.1434 - mae: 0.1434 - mse: 0.1245\n",
      "Epoch 00027: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 51us/sample - loss: 0.1485 - mae: 0.1485 - mse: 0.1654 - val_loss: 0.2202 - val_mae: 0.2202 - val_mse: 0.2389\n",
      "Epoch 28/2000\n",
      "2688/3971 [===================>..........] - ETA: 0s - loss: 0.1375 - mae: 0.1375 - mse: 0.1180\n",
      "Epoch 00028: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1485 - mae: 0.1485 - mse: 0.1672 - val_loss: 0.2082 - val_mae: 0.2082 - val_mse: 0.2156\n",
      "Epoch 29/2000\n",
      "2784/3971 [====================>.........] - ETA: 0s - loss: 0.1469 - mae: 0.1469 - mse: 0.1591\n",
      "Epoch 00029: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1462 - mae: 0.1462 - mse: 0.1647 - val_loss: 0.2113 - val_mae: 0.2113 - val_mse: 0.2017\n",
      "Epoch 30/2000\n",
      "2816/3971 [====================>.........] - ETA: 0s - loss: 0.1483 - mae: 0.1483 - mse: 0.2016\n",
      "Epoch 00030: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 50us/sample - loss: 0.1465 - mae: 0.1465 - mse: 0.1659 - val_loss: 0.2127 - val_mae: 0.2127 - val_mse: 0.1948\n",
      "Epoch 31/2000\n",
      "3712/3971 [===========================>..] - ETA: 0s - loss: 0.1495 - mae: 0.1495 - mse: 0.1712\n",
      "Epoch 00031: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 55us/sample - loss: 0.1463 - mae: 0.1463 - mse: 0.1621 - val_loss: 0.1843 - val_mae: 0.1843 - val_mse: 0.1843\n",
      "Epoch 32/2000\n",
      "2656/3971 [===================>..........] - ETA: 0s - loss: 0.1446 - mae: 0.1446 - mse: 0.1226\n",
      "Epoch 00032: val_loss did not improve from 0.18239\n",
      "3971/3971 [==============================] - 0s 49us/sample - loss: 0.1463 - mae: 0.1463 - mse: 0.1620 - val_loss: 0.2203 - val_mae: 0.2203 - val_mse: 0.2097\n",
      "1528/1528 - 0s - loss: 0.2050 - mae: 0.2050 - mse: 0.1352\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.425\n",
      "385\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_342 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3513 samples, validate on 2597 samples\n",
      "Epoch 1/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.4148 - mae: 0.4148 - mse: 0.6017\n",
      "Epoch 00001: val_loss improved from inf to 0.30770, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.3882,  mae:0.3882,  mse:0.5935,  val_loss:0.3077,  val_mae:0.3077,  val_mse:0.4319,  \n",
      "3513/3513 [==============================] - 1s 202us/sample - loss: 0.3882 - mae: 0.3882 - mse: 0.5935 - val_loss: 0.3077 - val_mae: 0.3077 - val_mse: 0.4319\n",
      "Epoch 2/2000\n",
      "2176/3513 [=================>............] - ETA: 0s - loss: 0.2968 - mae: 0.2968 - mse: 0.4197\n",
      "Epoch 00002: val_loss improved from 0.30770 to 0.27396, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 66us/sample - loss: 0.2765 - mae: 0.2765 - mse: 0.3758 - val_loss: 0.2740 - val_mae: 0.2740 - val_mse: 0.3754\n",
      "Epoch 3/2000\n",
      "2240/3513 [==================>...........] - ETA: 0s - loss: 0.2435 - mae: 0.2435 - mse: 0.3555\n",
      "Epoch 00003: val_loss improved from 0.27396 to 0.22949, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 65us/sample - loss: 0.2372 - mae: 0.2372 - mse: 0.2902 - val_loss: 0.2295 - val_mae: 0.2295 - val_mse: 0.2534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/2000\n",
      "2432/3513 [===================>..........] - ETA: 0s - loss: 0.2120 - mae: 0.2120 - mse: 0.2422\n",
      "Epoch 00004: val_loss improved from 0.22949 to 0.21983, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 64us/sample - loss: 0.2162 - mae: 0.2162 - mse: 0.2532 - val_loss: 0.2198 - val_mae: 0.2198 - val_mse: 0.2476\n",
      "Epoch 5/2000\n",
      "2336/3513 [==================>...........] - ETA: 0s - loss: 0.2069 - mae: 0.2069 - mse: 0.2829\n",
      "Epoch 00005: val_loss improved from 0.21983 to 0.20917, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 64us/sample - loss: 0.2053 - mae: 0.2053 - mse: 0.2406 - val_loss: 0.2092 - val_mae: 0.2092 - val_mse: 0.2179\n",
      "Epoch 6/2000\n",
      "2304/3513 [==================>...........] - ETA: 0s - loss: 0.2048 - mae: 0.2048 - mse: 0.2467\n",
      "Epoch 00006: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1999 - mae: 0.1999 - mse: 0.2336 - val_loss: 0.2130 - val_mae: 0.2130 - val_mse: 0.2109\n",
      "Epoch 7/2000\n",
      "2144/3513 [=================>............] - ETA: 0s - loss: 0.1975 - mae: 0.1975 - mse: 0.2368\n",
      "Epoch 00007: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 61us/sample - loss: 0.1957 - mae: 0.1957 - mse: 0.2285 - val_loss: 0.2360 - val_mae: 0.2360 - val_mse: 0.2275\n",
      "Epoch 8/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1936 - mae: 0.1936 - mse: 0.1805\n",
      "Epoch 00008: val_loss did not improve from 0.20917\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1909 - mae: 0.1909 - mse: 0.2184 - val_loss: 0.2548 - val_mae: 0.2548 - val_mse: 0.2968\n",
      "Epoch 9/2000\n",
      "3488/3513 [============================>.] - ETA: 0s - loss: 0.1866 - mae: 0.1866 - mse: 0.2168\n",
      "Epoch 00009: val_loss improved from 0.20917 to 0.19422, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 68us/sample - loss: 0.1869 - mae: 0.1869 - mse: 0.2163 - val_loss: 0.1942 - val_mae: 0.1942 - val_mse: 0.1871\n",
      "Epoch 10/2000\n",
      "2144/3513 [=================>............] - ETA: 0s - loss: 0.1841 - mae: 0.1841 - mse: 0.2199\n",
      "Epoch 00010: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 63us/sample - loss: 0.1836 - mae: 0.1836 - mse: 0.2162 - val_loss: 0.2094 - val_mae: 0.2094 - val_mse: 0.1834\n",
      "Epoch 11/2000\n",
      "2784/3513 [======================>.......] - ETA: 0s - loss: 0.1847 - mae: 0.1847 - mse: 0.2356\n",
      "Epoch 00011: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1774 - mae: 0.1774 - mse: 0.2055 - val_loss: 0.2675 - val_mae: 0.2675 - val_mse: 0.2884\n",
      "Epoch 12/2000\n",
      "2656/3513 [=====================>........] - ETA: 0s - loss: 0.1831 - mae: 0.1831 - mse: 0.2363\n",
      "Epoch 00012: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 56us/sample - loss: 0.1800 - mae: 0.1800 - mse: 0.2061 - val_loss: 0.2002 - val_mae: 0.2002 - val_mse: 0.2034\n",
      "Epoch 13/2000\n",
      "1920/3513 [===============>..............] - ETA: 0s - loss: 0.1731 - mae: 0.1731 - mse: 0.2227\n",
      "Epoch 00013: val_loss did not improve from 0.19422\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1753 - mae: 0.1753 - mse: 0.2033 - val_loss: 0.2137 - val_mae: 0.2137 - val_mse: 0.2090\n",
      "Epoch 14/2000\n",
      "2720/3513 [======================>.......] - ETA: 0s - loss: 0.1725 - mae: 0.1725 - mse: 0.1909\n",
      "Epoch 00014: val_loss improved from 0.19422 to 0.18293, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 62us/sample - loss: 0.1747 - mae: 0.1747 - mse: 0.2008 - val_loss: 0.1829 - val_mae: 0.1829 - val_mse: 0.1824\n",
      "Epoch 15/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1696 - mae: 0.1696 - mse: 0.1127\n",
      "Epoch 00015: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1763 - mae: 0.1763 - mse: 0.2041 - val_loss: 0.2334 - val_mae: 0.2334 - val_mse: 0.2186\n",
      "Epoch 16/2000\n",
      "2432/3513 [===================>..........] - ETA: 0s - loss: 0.1699 - mae: 0.1699 - mse: 0.1547\n",
      "Epoch 00016: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1721 - mae: 0.1721 - mse: 0.2000 - val_loss: 0.1864 - val_mae: 0.1864 - val_mse: 0.1782\n",
      "Epoch 17/2000\n",
      "2752/3513 [======================>.......] - ETA: 0s - loss: 0.1730 - mae: 0.1730 - mse: 0.2250\n",
      "Epoch 00017: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 54us/sample - loss: 0.1692 - mae: 0.1692 - mse: 0.1945 - val_loss: 0.2193 - val_mae: 0.2193 - val_mse: 0.2096\n",
      "Epoch 18/2000\n",
      "2624/3513 [=====================>........] - ETA: 0s - loss: 0.1781 - mae: 0.1781 - mse: 0.2396\n",
      "Epoch 00018: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1700 - mae: 0.1700 - mse: 0.1987 - val_loss: 0.1835 - val_mae: 0.1835 - val_mse: 0.1785\n",
      "Epoch 19/2000\n",
      "2816/3513 [=======================>......] - ETA: 0s - loss: 0.1707 - mae: 0.1707 - mse: 0.2130\n",
      "Epoch 00019: val_loss did not improve from 0.18293\n",
      "3513/3513 [==============================] - 0s 55us/sample - loss: 0.1672 - mae: 0.1672 - mse: 0.1911 - val_loss: 0.1984 - val_mae: 0.1984 - val_mse: 0.1844\n",
      "Epoch 20/2000\n",
      "2752/3513 [======================>.......] - ETA: 0s - loss: 0.1646 - mae: 0.1646 - mse: 0.1814\n",
      "Epoch 00020: val_loss improved from 0.18293 to 0.17534, saving model to model_checkpoint.h5\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1670 - mae: 0.1670 - mse: 0.1951 - val_loss: 0.1753 - val_mae: 0.1753 - val_mse: 0.1664\n",
      "Epoch 21/2000\n",
      "2208/3513 [=================>............] - ETA: 0s - loss: 0.1619 - mae: 0.1619 - mse: 0.1484\n",
      "Epoch 00021: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 60us/sample - loss: 0.1649 - mae: 0.1649 - mse: 0.1901 - val_loss: 0.1795 - val_mae: 0.1795 - val_mse: 0.1678\n",
      "Epoch 22/2000\n",
      "2496/3513 [====================>.........] - ETA: 0s - loss: 0.1626 - mae: 0.1626 - mse: 0.2238\n",
      "Epoch 00022: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1610 - mae: 0.1610 - mse: 0.1890 - val_loss: 0.1860 - val_mae: 0.1860 - val_mse: 0.1743\n",
      "Epoch 23/2000\n",
      "2496/3513 [====================>.........] - ETA: 0s - loss: 0.1633 - mae: 0.1633 - mse: 0.2213\n",
      "Epoch 00023: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 58us/sample - loss: 0.1618 - mae: 0.1618 - mse: 0.1849 - val_loss: 0.1762 - val_mae: 0.1762 - val_mse: 0.1736\n",
      "Epoch 24/2000\n",
      "2400/3513 [===================>..........] - ETA: 0s - loss: 0.1585 - mae: 0.1585 - mse: 0.1811\n",
      "Epoch 00024: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1622 - mae: 0.1622 - mse: 0.1873 - val_loss: 0.1816 - val_mae: 0.1816 - val_mse: 0.1710\n",
      "Epoch 25/2000\n",
      "2400/3513 [===================>..........] - ETA: 0s - loss: 0.1618 - mae: 0.1618 - mse: 0.2308\n",
      "Epoch 00025: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1619 - mae: 0.1619 - mse: 0.1866 - val_loss: 0.1925 - val_mae: 0.1925 - val_mse: 0.1866\n",
      "Epoch 26/2000\n",
      "2336/3513 [==================>...........] - ETA: 0s - loss: 0.1571 - mae: 0.1571 - mse: 0.1795\n",
      "Epoch 00026: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1569 - mae: 0.1569 - mse: 0.1809 - val_loss: 0.1816 - val_mae: 0.1816 - val_mse: 0.1749\n",
      "Epoch 27/2000\n",
      "2592/3513 [=====================>........] - ETA: 0s - loss: 0.1634 - mae: 0.1634 - mse: 0.2145\n",
      "Epoch 00027: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 56us/sample - loss: 0.1597 - mae: 0.1597 - mse: 0.1839 - val_loss: 0.1768 - val_mae: 0.1768 - val_mse: 0.1709\n",
      "Epoch 28/2000\n",
      "2368/3513 [===================>..........] - ETA: 0s - loss: 0.1564 - mae: 0.1564 - mse: 0.1739\n",
      "Epoch 00028: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 59us/sample - loss: 0.1577 - mae: 0.1577 - mse: 0.1808 - val_loss: 0.1836 - val_mae: 0.1836 - val_mse: 0.1851\n",
      "Epoch 29/2000\n",
      "2464/3513 [====================>.........] - ETA: 0s - loss: 0.1550 - mae: 0.1550 - mse: 0.2184\n",
      "Epoch 00029: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 57us/sample - loss: 0.1540 - mae: 0.1540 - mse: 0.1804 - val_loss: 0.1842 - val_mae: 0.1842 - val_mse: 0.1832\n",
      "Epoch 30/2000\n",
      "2784/3513 [======================>.......] - ETA: 0s - loss: 0.1572 - mae: 0.1572 - mse: 0.1723\n",
      "Epoch 00030: val_loss did not improve from 0.17534\n",
      "3513/3513 [==============================] - 0s 54us/sample - loss: 0.1547 - mae: 0.1547 - mse: 0.1812 - val_loss: 0.1770 - val_mae: 0.1770 - val_mse: 0.1812\n",
      "1528/1528 - 0s - loss: 0.1756 - mae: 0.1756 - mse: 0.1300\n",
      "Patience:  10\n",
      "training_pct:  0.8\n",
      "n_layer:  7\n",
      "n_unit:  20\n",
      "activation:  relu\n",
      "loss:  mean_absolute_error\n",
      "opt:  rmsprop\n",
      "val_pct:  0.5\n",
      "385\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_350 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3055 samples, validate on 3055 samples\n",
      "Epoch 1/2000\n",
      "2848/3055 [==========================>...] - ETA: 0s - loss: 0.4273 - mae: 0.4273 - mse: 0.6682\n",
      "Epoch 00001: val_loss improved from inf to 0.33730, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.4159,  mae:0.4159,  mse:0.6402,  val_loss:0.3373,  val_mae:0.3373,  val_mse:0.5570,  \n",
      "3055/3055 [==============================] - 1s 224us/sample - loss: 0.4159 - mae: 0.4159 - mse: 0.6402 - val_loss: 0.3373 - val_mae: 0.3373 - val_mse: 0.5570\n",
      "Epoch 2/2000\n",
      "1984/3055 [==================>...........] - ETA: 0s - loss: 0.2919 - mae: 0.2919 - mse: 0.3765\n",
      "Epoch 00002: val_loss improved from 0.33730 to 0.27766, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.2771 - mae: 0.2771 - mse: 0.3589 - val_loss: 0.2777 - val_mae: 0.2777 - val_mse: 0.4036\n",
      "Epoch 3/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.2372 - mae: 0.2372 - mse: 0.2966\n",
      "Epoch 00003: val_loss did not improve from 0.27766\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.2372 - mae: 0.2372 - mse: 0.2754 - val_loss: 0.2832 - val_mae: 0.2832 - val_mse: 0.4028\n",
      "Epoch 4/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.2012 - mae: 0.2012 - mse: 0.1474\n",
      "Epoch 00004: val_loss improved from 0.27766 to 0.24788, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 74us/sample - loss: 0.2141 - mae: 0.2141 - mse: 0.2258 - val_loss: 0.2479 - val_mae: 0.2479 - val_mse: 0.3209\n",
      "Epoch 5/2000\n",
      "2336/3055 [=====================>........] - ETA: 0s - loss: 0.2055 - mae: 0.2055 - mse: 0.2269\n",
      "Epoch 00005: val_loss improved from 0.24788 to 0.22283, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.2012 - mae: 0.2012 - mse: 0.2089 - val_loss: 0.2228 - val_mae: 0.2228 - val_mse: 0.2749\n",
      "Epoch 6/2000\n",
      "2368/3055 [======================>.......] - ETA: 0s - loss: 0.1968 - mae: 0.1968 - mse: 0.2199\n",
      "Epoch 00006: val_loss did not improve from 0.22283\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1938 - mae: 0.1938 - mse: 0.1977 - val_loss: 0.2484 - val_mae: 0.2484 - val_mse: 0.2729\n",
      "Epoch 7/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.1908 - mae: 0.1908 - mse: 0.2077\n",
      "Epoch 00007: val_loss did not improve from 0.22283\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1915 - mae: 0.1915 - mse: 0.1948 - val_loss: 0.2359 - val_mae: 0.2359 - val_mse: 0.2721\n",
      "Epoch 8/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1876 - mae: 0.1876 - mse: 0.1633\n",
      "Epoch 00008: val_loss improved from 0.22283 to 0.20990, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 68us/sample - loss: 0.1867 - mae: 0.1867 - mse: 0.1886 - val_loss: 0.2099 - val_mae: 0.2099 - val_mse: 0.2282\n",
      "Epoch 9/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1855 - mae: 0.1855 - mse: 0.1997\n",
      "Epoch 00009: val_loss did not improve from 0.20990\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1824 - mae: 0.1824 - mse: 0.1846 - val_loss: 0.2110 - val_mae: 0.2110 - val_mse: 0.2568\n",
      "Epoch 10/2000\n",
      "2272/3055 [=====================>........] - ETA: 0s - loss: 0.1727 - mae: 0.1727 - mse: 0.1500\n",
      "Epoch 00010: val_loss improved from 0.20990 to 0.19393, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 72us/sample - loss: 0.1777 - mae: 0.1777 - mse: 0.1825 - val_loss: 0.1939 - val_mae: 0.1939 - val_mse: 0.2131\n",
      "Epoch 11/2000\n",
      "2304/3055 [=====================>........] - ETA: 0s - loss: 0.1857 - mae: 0.1857 - mse: 0.2110\n",
      "Epoch 00011: val_loss did not improve from 0.19393\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1776 - mae: 0.1776 - mse: 0.1789 - val_loss: 0.1990 - val_mae: 0.1990 - val_mse: 0.2172\n",
      "Epoch 12/2000\n",
      "2176/3055 [====================>.........] - ETA: 0s - loss: 0.1758 - mae: 0.1758 - mse: 0.1959\n",
      "Epoch 00012: val_loss improved from 0.19393 to 0.19333, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 75us/sample - loss: 0.1773 - mae: 0.1773 - mse: 0.1761 - val_loss: 0.1933 - val_mae: 0.1933 - val_mse: 0.2148\n",
      "Epoch 13/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1700 - mae: 0.1700 - mse: 0.1116\n",
      "Epoch 00013: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 67us/sample - loss: 0.1748 - mae: 0.1748 - mse: 0.1790 - val_loss: 0.2030 - val_mae: 0.2030 - val_mse: 0.2317\n",
      "Epoch 14/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1753 - mae: 0.1753 - mse: 0.1981\n",
      "Epoch 00014: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1697 - mae: 0.1697 - mse: 0.1725 - val_loss: 0.1956 - val_mae: 0.1956 - val_mse: 0.2191\n",
      "Epoch 15/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1667 - mae: 0.1667 - mse: 0.1454\n",
      "Epoch 00015: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1686 - mae: 0.1686 - mse: 0.1695 - val_loss: 0.2227 - val_mae: 0.2227 - val_mse: 0.2741\n",
      "Epoch 16/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1727 - mae: 0.1727 - mse: 0.1979\n",
      "Epoch 00016: val_loss did not improve from 0.19333\n",
      "3055/3055 [==============================] - 0s 67us/sample - loss: 0.1677 - mae: 0.1677 - mse: 0.1691 - val_loss: 0.1951 - val_mae: 0.1951 - val_mse: 0.2332\n",
      "Epoch 17/2000\n",
      "2208/3055 [====================>.........] - ETA: 0s - loss: 0.1665 - mae: 0.1665 - mse: 0.1964\n",
      "Epoch 00017: val_loss improved from 0.19333 to 0.18515, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 72us/sample - loss: 0.1641 - mae: 0.1641 - mse: 0.1654 - val_loss: 0.1852 - val_mae: 0.1852 - val_mse: 0.2031\n",
      "Epoch 18/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/3055 [=====================>........] - ETA: 0s - loss: 0.1640 - mae: 0.1640 - mse: 0.1863\n",
      "Epoch 00018: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1639 - mae: 0.1639 - mse: 0.1671 - val_loss: 0.2228 - val_mae: 0.2228 - val_mse: 0.2724\n",
      "Epoch 19/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1656 - mae: 0.1656 - mse: 0.1758\n",
      "Epoch 00019: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1650 - mae: 0.1650 - mse: 0.1660 - val_loss: 0.2010 - val_mae: 0.2010 - val_mse: 0.2104\n",
      "Epoch 20/2000\n",
      "2656/3055 [=========================>....] - ETA: 0s - loss: 0.1590 - mae: 0.1590 - mse: 0.1334\n",
      "Epoch 00020: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1632 - mae: 0.1632 - mse: 0.1652 - val_loss: 0.2712 - val_mae: 0.2712 - val_mse: 0.3200\n",
      "Epoch 21/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1570 - mae: 0.1570 - mse: 0.1335\n",
      "Epoch 00021: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1581 - mae: 0.1581 - mse: 0.1619 - val_loss: 0.2015 - val_mae: 0.2015 - val_mse: 0.2066\n",
      "Epoch 22/2000\n",
      "2752/3055 [==========================>...] - ETA: 0s - loss: 0.1561 - mae: 0.1561 - mse: 0.1277\n",
      "Epoch 00022: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1573 - mae: 0.1573 - mse: 0.1575 - val_loss: 0.1976 - val_mae: 0.1976 - val_mse: 0.2077\n",
      "Epoch 23/2000\n",
      "2688/3055 [=========================>....] - ETA: 0s - loss: 0.1600 - mae: 0.1600 - mse: 0.1728\n",
      "Epoch 00023: val_loss did not improve from 0.18515\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1582 - mae: 0.1582 - mse: 0.1615 - val_loss: 0.2351 - val_mae: 0.2351 - val_mse: 0.2278\n",
      "Epoch 24/2000\n",
      "2400/3055 [======================>.......] - ETA: 0s - loss: 0.1568 - mae: 0.1568 - mse: 0.1407\n",
      "Epoch 00024: val_loss improved from 0.18515 to 0.18482, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 71us/sample - loss: 0.1558 - mae: 0.1558 - mse: 0.1615 - val_loss: 0.1848 - val_mae: 0.1848 - val_mse: 0.1948\n",
      "Epoch 25/2000\n",
      "2464/3055 [=======================>......] - ETA: 0s - loss: 0.1614 - mae: 0.1614 - mse: 0.1810\n",
      "Epoch 00025: val_loss did not improve from 0.18482\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1600 - mae: 0.1600 - mse: 0.1641 - val_loss: 0.1965 - val_mae: 0.1965 - val_mse: 0.2021\n",
      "Epoch 26/2000\n",
      "2528/3055 [=======================>......] - ETA: 0s - loss: 0.1527 - mae: 0.1527 - mse: 0.1652\n",
      "Epoch 00026: val_loss improved from 0.18482 to 0.18444, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 70us/sample - loss: 0.1536 - mae: 0.1536 - mse: 0.1526 - val_loss: 0.1844 - val_mae: 0.1844 - val_mse: 0.2016\n",
      "Epoch 27/2000\n",
      "2240/3055 [====================>.........] - ETA: 0s - loss: 0.1499 - mae: 0.1499 - mse: 0.1250\n",
      "Epoch 00027: val_loss did not improve from 0.18444\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1549 - mae: 0.1549 - mse: 0.1559 - val_loss: 0.1994 - val_mae: 0.1994 - val_mse: 0.2367\n",
      "Epoch 28/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1480 - mae: 0.1480 - mse: 0.1247\n",
      "Epoch 00028: val_loss did not improve from 0.18444\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1538 - mae: 0.1538 - mse: 0.1594 - val_loss: 0.1884 - val_mae: 0.1884 - val_mse: 0.2100\n",
      "Epoch 29/2000\n",
      "2912/3055 [===========================>..] - ETA: 0s - loss: 0.1506 - mae: 0.1506 - mse: 0.1573\n",
      "Epoch 00029: val_loss improved from 0.18444 to 0.17581, saving model to model_checkpoint.h5\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1493 - mae: 0.1493 - mse: 0.1524 - val_loss: 0.1758 - val_mae: 0.1758 - val_mse: 0.1948\n",
      "Epoch 30/2000\n",
      "2112/3055 [===================>..........] - ETA: 0s - loss: 0.1569 - mae: 0.1569 - mse: 0.1885\n",
      "Epoch 00030: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 68us/sample - loss: 0.1530 - mae: 0.1530 - mse: 0.1551 - val_loss: 0.1766 - val_mae: 0.1766 - val_mse: 0.2017\n",
      "Epoch 31/2000\n",
      "2112/3055 [===================>..........] - ETA: 0s - loss: 0.1445 - mae: 0.1445 - mse: 0.0826\n",
      "Epoch 00031: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 66us/sample - loss: 0.1466 - mae: 0.1466 - mse: 0.1490 - val_loss: 0.2106 - val_mae: 0.2106 - val_mse: 0.2580\n",
      "Epoch 32/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1422 - mae: 0.1422 - mse: 0.1191\n",
      "Epoch 00032: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1489 - mae: 0.1489 - mse: 0.1502 - val_loss: 0.2200 - val_mae: 0.2200 - val_mse: 0.2167\n",
      "Epoch 33/2000\n",
      "2720/3055 [=========================>....] - ETA: 0s - loss: 0.1484 - mae: 0.1484 - mse: 0.1572\n",
      "Epoch 00033: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 62us/sample - loss: 0.1483 - mae: 0.1483 - mse: 0.1498 - val_loss: 0.2273 - val_mae: 0.2273 - val_mse: 0.2766\n",
      "Epoch 34/2000\n",
      "2752/3055 [==========================>...] - ETA: 0s - loss: 0.1493 - mae: 0.1493 - mse: 0.1595\n",
      "Epoch 00034: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 61us/sample - loss: 0.1482 - mae: 0.1482 - mse: 0.1507 - val_loss: 0.1807 - val_mae: 0.1807 - val_mse: 0.2066\n",
      "Epoch 35/2000\n",
      "2848/3055 [==========================>...] - ETA: 0s - loss: 0.1494 - mae: 0.1494 - mse: 0.1559\n",
      "Epoch 00035: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 60us/sample - loss: 0.1473 - mae: 0.1473 - mse: 0.1485 - val_loss: 0.2192 - val_mae: 0.2192 - val_mse: 0.2659\n",
      "Epoch 36/2000\n",
      "2528/3055 [=======================>......] - ETA: 0s - loss: 0.1497 - mae: 0.1497 - mse: 0.1661\n",
      "Epoch 00036: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 64us/sample - loss: 0.1465 - mae: 0.1465 - mse: 0.1491 - val_loss: 0.1900 - val_mae: 0.1900 - val_mse: 0.1981\n",
      "Epoch 37/2000\n",
      "2592/3055 [========================>.....] - ETA: 0s - loss: 0.1467 - mae: 0.1467 - mse: 0.1638\n",
      "Epoch 00037: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 63us/sample - loss: 0.1452 - mae: 0.1452 - mse: 0.1490 - val_loss: 0.1777 - val_mae: 0.1777 - val_mse: 0.1892\n",
      "Epoch 38/2000\n",
      "2432/3055 [======================>.......] - ETA: 0s - loss: 0.1490 - mae: 0.1490 - mse: 0.1677\n",
      "Epoch 00038: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1459 - mae: 0.1459 - mse: 0.1480 - val_loss: 0.2146 - val_mae: 0.2146 - val_mse: 0.2284\n",
      "Epoch 39/2000\n",
      "2496/3055 [=======================>......] - ETA: 0s - loss: 0.1500 - mae: 0.1500 - mse: 0.1669\n",
      "Epoch 00039: val_loss did not improve from 0.17581\n",
      "3055/3055 [==============================] - 0s 65us/sample - loss: 0.1453 - mae: 0.1453 - mse: 0.1472 - val_loss: 0.1829 - val_mae: 0.1829 - val_mse: 0.2019\n",
      "1528/1528 - 0s - loss: 0.1805 - mae: 0.1805 - mse: 0.1341\n"
     ]
    }
   ],
   "source": [
    "#patience_d, training_pct_d, n_layer_d, n_unit_d, activation_d, loss_d, opt_d, val_pct_d \n",
    "r = varyParams(ml_data, defaults, init_grid, total_frac, start_str, end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.2: 0.106797,\n",
       " 0.275: 0.14296195,\n",
       " 0.35: 0.13522059,\n",
       " 0.425: 0.1300026,\n",
       " 0.5: 0.13410667}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience:  10\n",
      "training_pct:  0.9\n",
      "n_layer:  3\n",
      "n_unit:  200\n",
      "activation:  relu\n",
      "loss:  mse\n",
      "opt:  adam\n",
      "val_pct:  0.2\n",
      "385\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 200)               77200     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 157,801\n",
      "Trainable params: 157,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 14 steps, validate for 14 steps\n",
      "Epoch 1/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.6858 - mae: 0.5077 - mse: 0.6858\n",
      "Epoch 00001: val_loss improved from inf to 0.41624, saving model to model_checkpoint.h5\n",
      "\n",
      "Epoch: 0, loss:0.6135,  mae:0.4638,  mse:0.6135,  val_loss:0.4162,  val_mae:0.3655,  val_mse:0.4167,  \n",
      "14/14 [==============================] - 1s 71ms/step - loss: 0.6101 - mae: 0.4638 - mse: 0.6135 - val_loss: 0.4162 - val_mae: 0.3655 - val_mse: 0.4167\n",
      "Epoch 2/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.3337 - mae: 0.3154 - mse: 0.3337\n",
      "Epoch 00002: val_loss improved from 0.41624 to 0.22106, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.3053 - mae: 0.3005 - mse: 0.3072 - val_loss: 0.2211 - val_mae: 0.2463 - val_mse: 0.2218\n",
      "Epoch 3/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1958 - mae: 0.2267 - mse: 0.1958\n",
      "Epoch 00003: val_loss improved from 0.22106 to 0.16638, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.1907 - mae: 0.2255 - mse: 0.1918 - val_loss: 0.1664 - val_mae: 0.2050 - val_mse: 0.1672\n",
      "Epoch 4/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1657 - mae: 0.1944 - mse: 0.1657\n",
      "Epoch 00004: val_loss improved from 0.16638 to 0.15256, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.1598 - mae: 0.1924 - mse: 0.1607 - val_loss: 0.1526 - val_mae: 0.1865 - val_mse: 0.1535\n",
      "Epoch 5/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1524 - mae: 0.1815 - mse: 0.1524\n",
      "Epoch 00005: val_loss improved from 0.15256 to 0.14432, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.1493 - mae: 0.1816 - mse: 0.1502 - val_loss: 0.1443 - val_mae: 0.1760 - val_mse: 0.1452\n",
      "Epoch 6/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1456 - mae: 0.1739 - mse: 0.1456\n",
      "Epoch 00006: val_loss improved from 0.14432 to 0.13830, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.1426 - mae: 0.1744 - mse: 0.1435 - val_loss: 0.1383 - val_mae: 0.1697 - val_mse: 0.1392\n",
      "Epoch 7/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1402 - mae: 0.1681 - mse: 0.1402\n",
      "Epoch 00007: val_loss improved from 0.13830 to 0.13433, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1373 - mae: 0.1684 - mse: 0.1382 - val_loss: 0.1343 - val_mae: 0.1650 - val_mse: 0.1352\n",
      "Epoch 8/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.1371 - mae: 0.1645 - mse: 0.1371\n",
      "Epoch 00008: val_loss improved from 0.13433 to 0.13139, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1330 - mae: 0.1637 - mse: 0.1339 - val_loss: 0.1314 - val_mae: 0.1617 - val_mse: 0.1323\n",
      "Epoch 9/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1341 - mae: 0.1598 - mse: 0.1341\n",
      "Epoch 00009: val_loss improved from 0.13139 to 0.12746, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1292 - mae: 0.1599 - mse: 0.1301 - val_loss: 0.1275 - val_mae: 0.1568 - val_mse: 0.1284\n",
      "Epoch 10/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.1284 - mae: 0.1560 - mse: 0.1284\n",
      "Epoch 00010: val_loss improved from 0.12746 to 0.12408, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1259 - mae: 0.1563 - mse: 0.1267 - val_loss: 0.1241 - val_mae: 0.1531 - val_mse: 0.1250\n",
      "Epoch 11/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.1253 - mae: 0.1528 - mse: 0.1253\n",
      "Epoch 00011: val_loss improved from 0.12408 to 0.12069, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1231 - mae: 0.1532 - mse: 0.1239 - val_loss: 0.1207 - val_mae: 0.1498 - val_mse: 0.1216\n",
      "Epoch 12/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1252 - mae: 0.1509 - mse: 0.1252\n",
      "Epoch 00012: val_loss improved from 0.12069 to 0.11801, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.1206 - mae: 0.1504 - mse: 0.1215 - val_loss: 0.1180 - val_mae: 0.1484 - val_mse: 0.1189\n",
      "Epoch 13/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.1202 - mae: 0.1478 - mse: 0.1202\n",
      "Epoch 00013: val_loss improved from 0.11801 to 0.11558, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.1184 - mae: 0.1481 - mse: 0.1193 - val_loss: 0.1156 - val_mae: 0.1461 - val_mse: 0.1165\n",
      "Epoch 14/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1202 - mae: 0.1466 - mse: 0.1202\n",
      "Epoch 00014: val_loss improved from 0.11558 to 0.11349, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.1164 - mae: 0.1458 - mse: 0.1173 - val_loss: 0.1135 - val_mae: 0.1436 - val_mse: 0.1144\n",
      "Epoch 15/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1185 - mae: 0.1438 - mse: 0.1185\n",
      "Epoch 00015: val_loss improved from 0.11349 to 0.11203, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.1145 - mae: 0.1439 - mse: 0.1153 - val_loss: 0.1120 - val_mae: 0.1437 - val_mse: 0.1129\n",
      "Epoch 16/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1165 - mae: 0.1432 - mse: 0.1165\n",
      "Epoch 00016: val_loss improved from 0.11203 to 0.11054, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.1127 - mae: 0.1424 - mse: 0.1136 - val_loss: 0.1105 - val_mae: 0.1424 - val_mse: 0.1114\n",
      "Epoch 17/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1153 - mae: 0.1411 - mse: 0.1153\n",
      "Epoch 00017: val_loss improved from 0.11054 to 0.10902, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.1114 - mae: 0.1413 - mse: 0.1123 - val_loss: 0.1090 - val_mae: 0.1409 - val_mse: 0.1099\n",
      "Epoch 18/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1122 - mae: 0.1400 - mse: 0.1122\n",
      "Epoch 00018: val_loss improved from 0.10902 to 0.10781, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.1097 - mae: 0.1396 - mse: 0.1105 - val_loss: 0.1078 - val_mae: 0.1402 - val_mse: 0.1087\n",
      "Epoch 19/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1109 - mae: 0.1390 - mse: 0.1109\n",
      "Epoch 00019: val_loss improved from 0.10781 to 0.10631, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.1087 - mae: 0.1391 - mse: 0.1096 - val_loss: 0.1063 - val_mae: 0.1384 - val_mse: 0.1072\n",
      "Epoch 20/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1109 - mae: 0.1380 - mse: 0.1109\n",
      "Epoch 00020: val_loss improved from 0.10631 to 0.10544, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.1071 - mae: 0.1373 - mse: 0.1079 - val_loss: 0.1054 - val_mae: 0.1387 - val_mse: 0.1063\n",
      "Epoch 21/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1101 - mae: 0.1365 - mse: 0.1101\n",
      "Epoch 00021: val_loss improved from 0.10544 to 0.10430, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.1065 - mae: 0.1372 - mse: 0.1074 - val_loss: 0.1043 - val_mae: 0.1374 - val_mse: 0.1052\n",
      "Epoch 22/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.1087 - mae: 0.1361 - mse: 0.1087\n",
      "Epoch 00022: val_loss improved from 0.10430 to 0.10320, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.1049 - mae: 0.1355 - mse: 0.1058 - val_loss: 0.1032 - val_mae: 0.1360 - val_mse: 0.1041\n",
      "Epoch 23/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1078 - mae: 0.1343 - mse: 0.1078\n",
      "Epoch 00023: val_loss did not improve from 0.10320\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.1044 - mae: 0.1351 - mse: 0.1052 - val_loss: 0.1036 - val_mae: 0.1396 - val_mse: 0.1045\n",
      "Epoch 24/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1072 - mae: 0.1353 - mse: 0.1072\n",
      "Epoch 00024: val_loss improved from 0.10320 to 0.10099, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1033 - mae: 0.1345 - mse: 0.1042 - val_loss: 0.1010 - val_mae: 0.1323 - val_mse: 0.1019\n",
      "Epoch 25/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.1043 - mae: 0.1327 - mse: 0.1043\n",
      "Epoch 00025: val_loss did not improve from 0.10099\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.1021 - mae: 0.1327 - mse: 0.1030 - val_loss: 0.1017 - val_mae: 0.1374 - val_mse: 0.1026\n",
      "Epoch 26/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.1053 - mae: 0.1333 - mse: 0.1053\n",
      "Epoch 00026: val_loss improved from 0.10099 to 0.10011, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1019 - mae: 0.1333 - mse: 0.1028 - val_loss: 0.1001 - val_mae: 0.1329 - val_mse: 0.1010\n",
      "Epoch 27/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1053 - mae: 0.1330 - mse: 0.1053\n",
      "Epoch 00027: val_loss improved from 0.10011 to 0.09865, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.1008 - mae: 0.1320 - mse: 0.1017 - val_loss: 0.0986 - val_mae: 0.1299 - val_mse: 0.0995\n",
      "Epoch 28/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1033 - mae: 0.1309 - mse: 0.1033\n",
      "Epoch 00028: val_loss did not improve from 0.09865\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0996 - mae: 0.1302 - mse: 0.1004 - val_loss: 0.0995 - val_mae: 0.1347 - val_mse: 0.1004\n",
      "Epoch 29/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1033 - mae: 0.1316 - mse: 0.1033\n",
      "Epoch 00029: val_loss did not improve from 0.09865\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0997 - mae: 0.1311 - mse: 0.1005 - val_loss: 0.1012 - val_mae: 0.1395 - val_mse: 0.1021\n",
      "Epoch 30/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.1032 - mae: 0.1320 - mse: 0.1032\n",
      "Epoch 00030: val_loss improved from 0.09865 to 0.09699, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 0.0994 - mae: 0.1314 - mse: 0.1002 - val_loss: 0.0970 - val_mae: 0.1268 - val_mse: 0.0979\n",
      "Epoch 31/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1021 - mae: 0.1302 - mse: 0.1021\n",
      "Epoch 00031: val_loss improved from 0.09699 to 0.09642, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0982 - mae: 0.1294 - mse: 0.0991 - val_loss: 0.0964 - val_mae: 0.1297 - val_mse: 0.0973\n",
      "Epoch 32/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.1004 - mae: 0.1279 - mse: 0.1004\n",
      "Epoch 00032: val_loss did not improve from 0.09642\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.0969 - mae: 0.1276 - mse: 0.0977 - val_loss: 0.0984 - val_mae: 0.1355 - val_mse: 0.0993\n",
      "Epoch 33/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0991 - mae: 0.1285 - mse: 0.0991\n",
      "Epoch 00033: val_loss did not improve from 0.09642\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 0.0975 - mae: 0.1293 - mse: 0.0983 - val_loss: 0.0974 - val_mae: 0.1333 - val_mse: 0.0983\n",
      "Epoch 34/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.1015 - mae: 0.1303 - mse: 0.1015\n",
      "Epoch 00034: val_loss improved from 0.09642 to 0.09461, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0970 - mae: 0.1292 - mse: 0.0979 - val_loss: 0.0946 - val_mae: 0.1248 - val_mse: 0.0955\n",
      "Epoch 35/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0982 - mae: 0.1281 - mse: 0.0982\n",
      "Epoch 00035: val_loss improved from 0.09461 to 0.09447, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0960 - mae: 0.1274 - mse: 0.0968 - val_loss: 0.0945 - val_mae: 0.1284 - val_mse: 0.0954\n",
      "Epoch 36/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0987 - mae: 0.1264 - mse: 0.0987\n",
      "Epoch 00036: val_loss did not improve from 0.09447\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0947 - mae: 0.1257 - mse: 0.0955 - val_loss: 0.0970 - val_mae: 0.1354 - val_mse: 0.0979\n",
      "Epoch 37/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0985 - mae: 0.1266 - mse: 0.0985\n",
      "Epoch 00037: val_loss did not improve from 0.09447\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0950 - mae: 0.1268 - mse: 0.0958 - val_loss: 0.0967 - val_mae: 0.1349 - val_mse: 0.0976\n",
      "Epoch 38/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0988 - mae: 0.1276 - mse: 0.0988\n",
      "Epoch 00038: val_loss improved from 0.09447 to 0.09278, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0950 - mae: 0.1274 - mse: 0.0958 - val_loss: 0.0928 - val_mae: 0.1239 - val_mse: 0.0937\n",
      "Epoch 39/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0993 - mae: 0.1286 - mse: 0.0993\n",
      "Epoch 00039: val_loss improved from 0.09278 to 0.09204, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0949 - mae: 0.1274 - mse: 0.0957 - val_loss: 0.0920 - val_mae: 0.1241 - val_mse: 0.0929\n",
      "Epoch 40/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0967 - mae: 0.1248 - mse: 0.0967\n",
      "Epoch 00040: val_loss did not improve from 0.09204\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0930 - mae: 0.1242 - mse: 0.0939 - val_loss: 0.0945 - val_mae: 0.1325 - val_mse: 0.0954\n",
      "Epoch 41/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0963 - mae: 0.1247 - mse: 0.0963\n",
      "Epoch 00041: val_loss did not improve from 0.09204\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0926 - mae: 0.1242 - mse: 0.0934 - val_loss: 0.0944 - val_mae: 0.1326 - val_mse: 0.0953\n",
      "Epoch 42/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0966 - mae: 0.1259 - mse: 0.0966\n",
      "Epoch 00042: val_loss improved from 0.09204 to 0.09191, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.0928 - mae: 0.1252 - mse: 0.0937 - val_loss: 0.0919 - val_mae: 0.1259 - val_mse: 0.0928\n",
      "Epoch 43/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0972 - mae: 0.1267 - mse: 0.0972\n",
      "Epoch 00043: val_loss improved from 0.09191 to 0.09059, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0927 - mae: 0.1253 - mse: 0.0936 - val_loss: 0.0906 - val_mae: 0.1214 - val_mse: 0.0915\n",
      "Epoch 44/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0963 - mae: 0.1258 - mse: 0.0963\n",
      "Epoch 00044: val_loss did not improve from 0.09059\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0926 - mae: 0.1251 - mse: 0.0935 - val_loss: 0.0912 - val_mae: 0.1261 - val_mse: 0.0921\n",
      "Epoch 45/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0932 - mae: 0.1226 - mse: 0.0932\n",
      "Epoch 00045: val_loss did not improve from 0.09059\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0909 - mae: 0.1224 - mse: 0.0918 - val_loss: 0.0937 - val_mae: 0.1334 - val_mse: 0.0946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0923 - mae: 0.1230 - mse: 0.0923\n",
      "Epoch 00046: val_loss did not improve from 0.09059\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0906 - mae: 0.1227 - mse: 0.0914 - val_loss: 0.0923 - val_mae: 0.1301 - val_mse: 0.0932\n",
      "Epoch 47/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0941 - mae: 0.1231 - mse: 0.0941\n",
      "Epoch 00047: val_loss improved from 0.09059 to 0.09047, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 0.0906 - mae: 0.1229 - mse: 0.0915 - val_loss: 0.0905 - val_mae: 0.1258 - val_mse: 0.0914\n",
      "Epoch 48/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0929 - mae: 0.1246 - mse: 0.0929\n",
      "Epoch 00048: val_loss improved from 0.09047 to 0.08873, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0907 - mae: 0.1236 - mse: 0.0916 - val_loss: 0.0887 - val_mae: 0.1201 - val_mse: 0.0896\n",
      "Epoch 49/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0934 - mae: 0.1253 - mse: 0.0934\n",
      "Epoch 00049: val_loss improved from 0.08873 to 0.08869, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0913 - mae: 0.1247 - mse: 0.0922 - val_loss: 0.0887 - val_mae: 0.1226 - val_mse: 0.0896\n",
      "Epoch 50/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0913 - mae: 0.1215 - mse: 0.0913\n",
      "Epoch 00050: val_loss did not improve from 0.08869\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0898 - mae: 0.1220 - mse: 0.0906 - val_loss: 0.0941 - val_mae: 0.1358 - val_mse: 0.0950\n",
      "Epoch 51/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0929 - mae: 0.1226 - mse: 0.0929\n",
      "Epoch 00051: val_loss did not improve from 0.08869\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0889 - mae: 0.1214 - mse: 0.0897 - val_loss: 0.0901 - val_mae: 0.1289 - val_mse: 0.0910\n",
      "Epoch 52/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0904 - mae: 0.1220 - mse: 0.0904\n",
      "Epoch 00052: val_loss improved from 0.08869 to 0.08828, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0884 - mae: 0.1210 - mse: 0.0893 - val_loss: 0.0883 - val_mae: 0.1236 - val_mse: 0.0892\n",
      "Epoch 53/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0917 - mae: 0.1211 - mse: 0.0917\n",
      "Epoch 00053: val_loss improved from 0.08828 to 0.08748, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0881 - mae: 0.1205 - mse: 0.0889 - val_loss: 0.0875 - val_mae: 0.1220 - val_mse: 0.0884\n",
      "Epoch 54/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0903 - mae: 0.1209 - mse: 0.0903\n",
      "Epoch 00054: val_loss improved from 0.08748 to 0.08679, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0880 - mae: 0.1208 - mse: 0.0889 - val_loss: 0.0868 - val_mae: 0.1202 - val_mse: 0.0877\n",
      "Epoch 55/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0922 - mae: 0.1223 - mse: 0.0922\n",
      "Epoch 00055: val_loss improved from 0.08679 to 0.08673, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0885 - mae: 0.1219 - mse: 0.0894 - val_loss: 0.0867 - val_mae: 0.1206 - val_mse: 0.0876\n",
      "Epoch 56/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0928 - mae: 0.1235 - mse: 0.0928\n",
      "Epoch 00056: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0889 - mae: 0.1226 - mse: 0.0898 - val_loss: 0.0925 - val_mae: 0.1339 - val_mse: 0.0934\n",
      "Epoch 57/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0912 - mae: 0.1213 - mse: 0.0912\n",
      "Epoch 00057: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0881 - mae: 0.1211 - mse: 0.0888 - val_loss: 0.0959 - val_mae: 0.1445 - val_mse: 0.0968\n",
      "Epoch 58/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0926 - mae: 0.1249 - mse: 0.0926\n",
      "Epoch 00058: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0883 - mae: 0.1229 - mse: 0.0891 - val_loss: 0.0889 - val_mae: 0.1269 - val_mse: 0.0898\n",
      "Epoch 59/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0902 - mae: 0.1226 - mse: 0.0902\n",
      "Epoch 00059: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0880 - mae: 0.1224 - mse: 0.0888 - val_loss: 0.0917 - val_mae: 0.1352 - val_mse: 0.0926\n",
      "Epoch 60/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0924 - mae: 0.1253 - mse: 0.0924\n",
      "Epoch 00060: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0902 - mae: 0.1261 - mse: 0.0908 - val_loss: 0.0894 - val_mae: 0.1318 - val_mse: 0.0903\n",
      "Epoch 61/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1054 - mae: 0.1445 - mse: 0.1054\n",
      "Epoch 00061: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.1018 - mae: 0.1441 - mse: 0.1026 - val_loss: 0.0874 - val_mae: 0.1220 - val_mse: 0.0883\n",
      "Epoch 62/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0922 - mae: 0.1250 - mse: 0.0922\n",
      "Epoch 00062: val_loss did not improve from 0.08673\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0893 - mae: 0.1239 - mse: 0.0902 - val_loss: 0.0876 - val_mae: 0.1210 - val_mse: 0.0885\n",
      "Epoch 63/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0919 - mae: 0.1229 - mse: 0.0919\n",
      "Epoch 00063: val_loss improved from 0.08673 to 0.08668, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0877 - mae: 0.1226 - mse: 0.0885 - val_loss: 0.0867 - val_mae: 0.1209 - val_mse: 0.0876\n",
      "Epoch 64/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0917 - mae: 0.1233 - mse: 0.0917\n",
      "Epoch 00064: val_loss improved from 0.08668 to 0.08608, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0874 - mae: 0.1225 - mse: 0.0882 - val_loss: 0.0861 - val_mae: 0.1212 - val_mse: 0.0870\n",
      "Epoch 65/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0903 - mae: 0.1220 - mse: 0.0903\n",
      "Epoch 00065: val_loss improved from 0.08608 to 0.08488, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0862 - mae: 0.1212 - mse: 0.0870 - val_loss: 0.0849 - val_mae: 0.1189 - val_mse: 0.0858\n",
      "Epoch 66/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0893 - mae: 0.1210 - mse: 0.0893\n",
      "Epoch 00066: val_loss improved from 0.08488 to 0.08417, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0855 - mae: 0.1202 - mse: 0.0864 - val_loss: 0.0842 - val_mae: 0.1183 - val_mse: 0.0851\n",
      "Epoch 67/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0889 - mae: 0.1207 - mse: 0.0889\n",
      "Epoch 00067: val_loss improved from 0.08417 to 0.08364, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0851 - mae: 0.1199 - mse: 0.0860 - val_loss: 0.0836 - val_mae: 0.1177 - val_mse: 0.0845\n",
      "Epoch 68/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0871 - mae: 0.1198 - mse: 0.0871\n",
      "Epoch 00068: val_loss improved from 0.08364 to 0.08317, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0846 - mae: 0.1190 - mse: 0.0854 - val_loss: 0.0832 - val_mae: 0.1172 - val_mse: 0.0841\n",
      "Epoch 69/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0868 - mae: 0.1196 - mse: 0.0868\n",
      "Epoch 00069: val_loss improved from 0.08317 to 0.08283, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0843 - mae: 0.1187 - mse: 0.0851 - val_loss: 0.0828 - val_mae: 0.1166 - val_mse: 0.0837\n",
      "Epoch 70/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0879 - mae: 0.1191 - mse: 0.0879\n",
      "Epoch 00070: val_loss improved from 0.08283 to 0.08229, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0840 - mae: 0.1182 - mse: 0.0848 - val_loss: 0.0823 - val_mae: 0.1161 - val_mse: 0.0832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0851 - mae: 0.1179 - mse: 0.0851\n",
      "Epoch 00071: val_loss improved from 0.08229 to 0.08199, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0836 - mae: 0.1177 - mse: 0.0844 - val_loss: 0.0820 - val_mae: 0.1156 - val_mse: 0.0829\n",
      "Epoch 72/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0869 - mae: 0.1181 - mse: 0.0869\n",
      "Epoch 00072: val_loss improved from 0.08199 to 0.08171, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0832 - mae: 0.1173 - mse: 0.0840 - val_loss: 0.0817 - val_mae: 0.1156 - val_mse: 0.0826\n",
      "Epoch 73/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0869 - mae: 0.1179 - mse: 0.0869\n",
      "Epoch 00073: val_loss improved from 0.08171 to 0.08130, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0830 - mae: 0.1171 - mse: 0.0839 - val_loss: 0.0813 - val_mae: 0.1149 - val_mse: 0.0822\n",
      "Epoch 74/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0866 - mae: 0.1179 - mse: 0.0866\n",
      "Epoch 00074: val_loss improved from 0.08130 to 0.08102, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0827 - mae: 0.1169 - mse: 0.0836 - val_loss: 0.0810 - val_mae: 0.1146 - val_mse: 0.0819\n",
      "Epoch 75/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0861 - mae: 0.1174 - mse: 0.0861\n",
      "Epoch 00075: val_loss improved from 0.08102 to 0.08077, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0824 - mae: 0.1166 - mse: 0.0832 - val_loss: 0.0808 - val_mae: 0.1145 - val_mse: 0.0817\n",
      "Epoch 76/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0859 - mae: 0.1173 - mse: 0.0859\n",
      "Epoch 00076: val_loss improved from 0.08077 to 0.08049, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0821 - mae: 0.1163 - mse: 0.0830 - val_loss: 0.0805 - val_mae: 0.1144 - val_mse: 0.0814\n",
      "Epoch 77/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0844 - mae: 0.1170 - mse: 0.0844\n",
      "Epoch 00077: val_loss improved from 0.08049 to 0.08028, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0819 - mae: 0.1162 - mse: 0.0827 - val_loss: 0.0803 - val_mae: 0.1142 - val_mse: 0.0812\n",
      "Epoch 78/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0843 - mae: 0.1171 - mse: 0.0843\n",
      "Epoch 00078: val_loss improved from 0.08028 to 0.07999, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0819 - mae: 0.1162 - mse: 0.0827 - val_loss: 0.0800 - val_mae: 0.1133 - val_mse: 0.0809\n",
      "Epoch 79/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0854 - mae: 0.1171 - mse: 0.0854\n",
      "Epoch 00079: val_loss improved from 0.07999 to 0.07994, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0817 - mae: 0.1161 - mse: 0.0825 - val_loss: 0.0799 - val_mae: 0.1134 - val_mse: 0.0808\n",
      "Epoch 80/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0841 - mae: 0.1170 - mse: 0.0841\n",
      "Epoch 00080: val_loss improved from 0.07994 to 0.07981, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 0.0816 - mae: 0.1161 - mse: 0.0824 - val_loss: 0.0798 - val_mae: 0.1132 - val_mse: 0.0807\n",
      "Epoch 81/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0850 - mae: 0.1165 - mse: 0.0850\n",
      "Epoch 00081: val_loss improved from 0.07981 to 0.07948, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0813 - mae: 0.1157 - mse: 0.0821 - val_loss: 0.0795 - val_mae: 0.1131 - val_mse: 0.0803\n",
      "Epoch 82/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0846 - mae: 0.1161 - mse: 0.0846\n",
      "Epoch 00082: val_loss improved from 0.07948 to 0.07913, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.0808 - mae: 0.1153 - mse: 0.0817 - val_loss: 0.0791 - val_mae: 0.1132 - val_mse: 0.0800\n",
      "Epoch 83/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0841 - mae: 0.1158 - mse: 0.0841\n",
      "Epoch 00083: val_loss improved from 0.07913 to 0.07900, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0804 - mae: 0.1149 - mse: 0.0812 - val_loss: 0.0790 - val_mae: 0.1134 - val_mse: 0.0799\n",
      "Epoch 84/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0837 - mae: 0.1153 - mse: 0.0837\n",
      "Epoch 00084: val_loss improved from 0.07900 to 0.07892, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0800 - mae: 0.1144 - mse: 0.0808 - val_loss: 0.0789 - val_mae: 0.1141 - val_mse: 0.0798\n",
      "Epoch 85/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0809 - mae: 0.1145 - mse: 0.0809\n",
      "Epoch 00085: val_loss improved from 0.07892 to 0.07857, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0796 - mae: 0.1140 - mse: 0.0804 - val_loss: 0.0786 - val_mae: 0.1148 - val_mse: 0.0794\n",
      "Epoch 86/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0826 - mae: 0.1148 - mse: 0.0826\n",
      "Epoch 00086: val_loss improved from 0.07857 to 0.07814, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0791 - mae: 0.1138 - mse: 0.0799 - val_loss: 0.0781 - val_mae: 0.1144 - val_mse: 0.0790\n",
      "Epoch 87/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0823 - mae: 0.1146 - mse: 0.0823\n",
      "Epoch 00087: val_loss did not improve from 0.07814\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0787 - mae: 0.1134 - mse: 0.0795 - val_loss: 0.0785 - val_mae: 0.1160 - val_mse: 0.0794\n",
      "Epoch 88/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0794 - mae: 0.1134 - mse: 0.0794\n",
      "Epoch 00088: val_loss did not improve from 0.07814\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0783 - mae: 0.1128 - mse: 0.0791 - val_loss: 0.0785 - val_mae: 0.1157 - val_mse: 0.0793\n",
      "Epoch 89/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0816 - mae: 0.1135 - mse: 0.0816\n",
      "Epoch 00089: val_loss improved from 0.07814 to 0.07794, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0781 - mae: 0.1124 - mse: 0.0789 - val_loss: 0.0779 - val_mae: 0.1152 - val_mse: 0.0788\n",
      "Epoch 90/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0814 - mae: 0.1137 - mse: 0.0814\n",
      "Epoch 00090: val_loss improved from 0.07794 to 0.07756, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0778 - mae: 0.1126 - mse: 0.0786 - val_loss: 0.0776 - val_mae: 0.1152 - val_mse: 0.0784\n",
      "Epoch 91/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0812 - mae: 0.1132 - mse: 0.0812\n",
      "Epoch 00091: val_loss improved from 0.07756 to 0.07713, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0776 - mae: 0.1125 - mse: 0.0784 - val_loss: 0.0771 - val_mae: 0.1140 - val_mse: 0.0780\n",
      "Epoch 92/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0783 - mae: 0.1122 - mse: 0.0783\n",
      "Epoch 00092: val_loss did not improve from 0.07713\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0772 - mae: 0.1116 - mse: 0.0780 - val_loss: 0.0778 - val_mae: 0.1165 - val_mse: 0.0787\n",
      "Epoch 93/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0783 - mae: 0.1124 - mse: 0.0783\n",
      "Epoch 00093: val_loss did not improve from 0.07713\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0771 - mae: 0.1117 - mse: 0.0779 - val_loss: 0.0773 - val_mae: 0.1153 - val_mse: 0.0781\n",
      "Epoch 94/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0802 - mae: 0.1123 - mse: 0.0802\n",
      "Epoch 00094: val_loss did not improve from 0.07713\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0767 - mae: 0.1112 - mse: 0.0775 - val_loss: 0.0784 - val_mae: 0.1178 - val_mse: 0.0793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0803 - mae: 0.1122 - mse: 0.0803\n",
      "Epoch 00095: val_loss improved from 0.07713 to 0.07664, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 50ms/step - loss: 0.0768 - mae: 0.1115 - mse: 0.0776 - val_loss: 0.0766 - val_mae: 0.1150 - val_mse: 0.0775\n",
      "Epoch 96/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0793 - mae: 0.1147 - mse: 0.0793\n",
      "Epoch 00096: val_loss did not improve from 0.07664\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0789 - mae: 0.1151 - mse: 0.0795 - val_loss: 0.0888 - val_mae: 0.1374 - val_mse: 0.0897\n",
      "Epoch 97/2000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0836 - mae: 0.1183 - mse: 0.0836\n",
      "Epoch 00097: val_loss did not improve from 0.07664\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0799 - mae: 0.1173 - mse: 0.0807 - val_loss: 0.0779 - val_mae: 0.1123 - val_mse: 0.0788\n",
      "Epoch 98/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0845 - mae: 0.1198 - mse: 0.0845\n",
      "Epoch 00098: val_loss did not improve from 0.07664\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0803 - mae: 0.1175 - mse: 0.0811 - val_loss: 0.0787 - val_mae: 0.1195 - val_mse: 0.0795\n",
      "Epoch 99/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0784 - mae: 0.1124 - mse: 0.0784\n",
      "Epoch 00099: val_loss improved from 0.07664 to 0.07634, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0763 - mae: 0.1119 - mse: 0.0771 - val_loss: 0.0763 - val_mae: 0.1150 - val_mse: 0.0772\n",
      "Epoch 100/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0801 - mae: 0.1140 - mse: 0.0801\n",
      "Epoch 00100: val_loss improved from 0.07634 to 0.07618, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0765 - mae: 0.1125 - mse: 0.0773 - val_loss: 0.0762 - val_mae: 0.1142 - val_mse: 0.0770\n",
      "Epoch 101/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0792 - mae: 0.1130 - mse: 0.0792\n",
      "Epoch 00101: val_loss did not improve from 0.07618\n",
      "\n",
      "Epoch: 100, loss:0.0765,  mae:0.1117,  mse:0.0765,  val_loss:0.0771,  val_mae:0.1185,  val_mse:0.0779,  \n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0757 - mae: 0.1117 - mse: 0.0765 - val_loss: 0.0771 - val_mae: 0.1185 - val_mse: 0.0779\n",
      "Epoch 102/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0784 - mae: 0.1122 - mse: 0.0784\n",
      "Epoch 00102: val_loss did not improve from 0.07618\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0752 - mae: 0.1112 - mse: 0.0760 - val_loss: 0.0766 - val_mae: 0.1168 - val_mse: 0.0774\n",
      "Epoch 103/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0783 - mae: 0.1120 - mse: 0.0783\n",
      "Epoch 00103: val_loss improved from 0.07618 to 0.07592, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 0.0753 - mae: 0.1113 - mse: 0.0761 - val_loss: 0.0759 - val_mae: 0.1158 - val_mse: 0.0767\n",
      "Epoch 104/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0789 - mae: 0.1124 - mse: 0.0789\n",
      "Epoch 00104: val_loss improved from 0.07592 to 0.07439, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 50ms/step - loss: 0.0751 - mae: 0.1115 - mse: 0.0759 - val_loss: 0.0744 - val_mae: 0.1101 - val_mse: 0.0752\n",
      "Epoch 105/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0803 - mae: 0.1152 - mse: 0.0803\n",
      "Epoch 00105: val_loss improved from 0.07439 to 0.07398, saving model to model_checkpoint.h5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0767 - mae: 0.1140 - mse: 0.0775 - val_loss: 0.0740 - val_mae: 0.1109 - val_mse: 0.0748\n",
      "Epoch 106/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0814 - mae: 0.1181 - mse: 0.0814\n",
      "Epoch 00106: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 0.0795 - mae: 0.1178 - mse: 0.0801 - val_loss: 0.0784 - val_mae: 0.1237 - val_mse: 0.0792\n",
      "Epoch 107/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0803 - mae: 0.1181 - mse: 0.0803\n",
      "Epoch 00107: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0782 - mae: 0.1171 - mse: 0.0790 - val_loss: 0.0750 - val_mae: 0.1106 - val_mse: 0.0759\n",
      "Epoch 108/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0915 - mae: 0.1309 - mse: 0.0915\n",
      "Epoch 00108: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0898 - mae: 0.1329 - mse: 0.0905 - val_loss: 0.0772 - val_mae: 0.1142 - val_mse: 0.0780\n",
      "Epoch 109/2000\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.0787 - mae: 0.1159 - mse: 0.0787\n",
      "Epoch 00109: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0777 - mae: 0.1167 - mse: 0.0785 - val_loss: 0.0760 - val_mae: 0.1129 - val_mse: 0.0768\n",
      "Epoch 110/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0824 - mae: 0.1199 - mse: 0.0824\n",
      "Epoch 00110: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0791 - mae: 0.1206 - mse: 0.0798 - val_loss: 0.0804 - val_mae: 0.1192 - val_mse: 0.0812\n",
      "Epoch 111/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0815 - mae: 0.1186 - mse: 0.0815\n",
      "Epoch 00111: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0781 - mae: 0.1190 - mse: 0.0788 - val_loss: 0.0783 - val_mae: 0.1169 - val_mse: 0.0792\n",
      "Epoch 112/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0821 - mae: 0.1193 - mse: 0.0821\n",
      "Epoch 00112: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0788 - mae: 0.1202 - mse: 0.0795 - val_loss: 0.0779 - val_mae: 0.1183 - val_mse: 0.0787\n",
      "Epoch 113/2000\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.0836 - mae: 0.1226 - mse: 0.0836\n",
      "Epoch 00113: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.0817 - mae: 0.1244 - mse: 0.0824 - val_loss: 0.0779 - val_mae: 0.1215 - val_mse: 0.0787\n",
      "Epoch 114/2000\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.0925 - mae: 0.1343 - mse: 0.0925\n",
      "Epoch 00114: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0904 - mae: 0.1355 - mse: 0.0914 - val_loss: 0.1007 - val_mae: 0.1570 - val_mse: 0.1015\n",
      "Epoch 115/2000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.0943 - mae: 0.1366 - mse: 0.0943\n",
      "Epoch 00115: val_loss did not improve from 0.07398\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0891 - mae: 0.1341 - mse: 0.0901 - val_loss: 0.0787 - val_mae: 0.1170 - val_mse: 0.0795\n",
      "2/2 - 0s - loss: 0.0584 - mae: 0.1287 - mse: 0.0603\n",
      "Time elapsed:  73.71201467514038\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "training_pct = .9\n",
    "total_frac = 1\n",
    "mse, model = evaluate_model(ml_data, total_frac, start_str, end_str, 10, training_pct, 3, 200, 'relu', \n",
    "                   'mse', 'adam', .2, batch_size=5000, norm=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060335603"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s - loss: 0.0791 - mae: 0.1170 - mse: 0.0795\n"
     ]
    }
   ],
   "source": [
    "tr_loss, tr_mae, tr_mse = model.evaluate(train_data, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.079521105"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24563306497966297"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7638"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7638"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7638"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f08bcfa7a20>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(preds, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
